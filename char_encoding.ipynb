{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "=\n"
     ]
    }
   ],
   "source": [
    "char = '='\n",
    "print(ord(char))\n",
    "print(chr(ord(char)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "VOCAB_SIZE = 128 + 2\n",
    "SOS, EOS, PAD = 128, 129, 0\n",
    "\n",
    "df = pd.read_csv(\"augmented_data/Dataset_aug_complex_10496_original.csv\",sep=',')\n",
    "\n",
    "def preprocess(df) :\n",
    "    texts = df['text']\n",
    "    max_len = max([len(t) for t in texts])\n",
    "    out = []\n",
    "    for text in texts :\n",
    "        encoded = [SOS] + [ord(c) for c in text] + [EOS] + [PAD] * (max_len - len(text))\n",
    "        out.append(encoded)\n",
    "    return out, max_len\n",
    "\n",
    "out, max_len = preprocess(df)\n",
    "df['preprocessed'] = out\n",
    "df['valid_len'] = [len(t) for t in df['text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, string\n",
    "from pandas import isnull\n",
    "\n",
    "dataset_path = \"datasets/\"\n",
    "re_punkt = re.compile(\"[\" + string.punctuation + \"0-9\" + \"\\s\" + \"]+\")\n",
    "\n",
    "labels = ['text', 'canonical_form_1', 'canonical_form_2', 'canonical_form_3']\n",
    "profanity_df = pd.read_csv(\"datasets/profanity_en.csv\")\n",
    "profanity_list = []\n",
    "for _, row in profanity_df.iterrows() :\n",
    "    words = [i for i in list(row[labels]) if (not isnull(i)) and (not re.findall(re_punkt, i))]\n",
    "    profanity_list += words\n",
    "profanity_list = list(set(profanity_list))\n",
    "\n",
    "profanity_list_df = pd.DataFrame(profanity_list, columns=['word'])\n",
    "profanity_list_df.to_csv(dataset_path + \"profanity_en_list.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      word\n",
      "0                shitdicks\n",
      "1              breasticles\n",
      "2     shitfuckmotherfucker\n",
      "3                 bitchers\n",
      "4                  cocksuk\n",
      "...                    ...\n",
      "1047          mudderfukker\n",
      "1048             bastinado\n",
      "1049           cuntbollock\n",
      "1050             octopussy\n",
      "1051                 niguh\n",
      "\n",
      "[1052 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_path + \"profanity_en_list.csv\", sep=',')\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
