{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk pandas nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cwwojin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "import pandas as pd\n",
    "from pandas import isnull\n",
    "from collections import defaultdict\n",
    "import nltk, os, re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Profanity-spelling-error-dictionary (txt-file)\n",
    "data_path = \"datasets/\"\n",
    "spelling_path = \"spelling/\"\n",
    "\n",
    "train_df = pd.read_csv(data_path + \"2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.1.csv\", sep=',')[['text', 'label']]\n",
    "profanity_df = pd.read_csv(data_path + \"profanity_en.csv\", sep=',')\n",
    "labels = ['canonical_form_1', 'canonical_form_2', 'canonical_form_3']\n",
    "spelling_dict = defaultdict(list)\n",
    "fname_profanity = spelling_path + \"spelling_en_profanity.txt\"\n",
    "\n",
    "for idx, row in profanity_df.iterrows() :\n",
    "    text = row['text']\n",
    "    canons = [i for i in list(row[labels]) if not isnull(i)]\n",
    "    for word in canons :\n",
    "        spelling_dict[word].append(text)\n",
    "\n",
    "if os.path.exists(fname_profanity):\n",
    "    os.remove(fname_profanity)\n",
    "\n",
    "with open(fname_profanity, \"w+\", encoding=\"UTF-8\") as spelling_dict_file :\n",
    "    for k, v in spelling_dict.items() :\n",
    "        #print(\" \".join([k] + v))\n",
    "        spelling_dict_file.write(\" \".join([k] + v))\n",
    "        spelling_dict_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_prof(text, profanities) :\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    return (not not set(tokenized_text).intersection(set(profanities)))\n",
    "\n",
    "#Modifying Original Dataset\n",
    "profanities = list(spelling_dict.keys())\n",
    "new_texts = []\n",
    "new_labels = []\n",
    "\n",
    "for idx, row in train_df.iterrows() :\n",
    "    text = row['text']\n",
    "    label = row['label']\n",
    "    if contains_prof(text, profanities) and len(word_tokenize(text)) <= 100 :\n",
    "        new_texts.append(text)\n",
    "        new_labels.append(label)\n",
    "\n",
    "train_df_prof = pd.DataFrame(list(zip(new_texts, new_labels)), columns=['text', 'label'])\n",
    "train_df_prof.to_csv(data_path + \"2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.1-ProfanityOnly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_aug(n, N, train_df, aug, output_fname, path='') :\n",
    "    train_df_sample = train_df.sample(n=N, ignore_index=True)\n",
    "    #save sample as file\n",
    "    train_df_sample.to_csv(path + output_fname[:-4] + \"original.csv\".format(N=N), sep=\",\")\n",
    "\n",
    "    #aug.augment(data, n)\n",
    "    augmented_data_sentences = []\n",
    "    augmented_data_labels = []\n",
    "\n",
    "    for idx, row in train_df_sample.iterrows() :\n",
    "        sentence = row['text']\n",
    "        label = row['label']\n",
    "        augmented = aug.augment(sentence, n=n) if n > 1 else [aug.augment(sentence, n=n)]\n",
    "\n",
    "        #label-preserving\n",
    "        augmented_data_sentences += ([sentence] + augmented)\n",
    "        augmented_data_labels += [label for i in range(n+1)]\n",
    "    \n",
    "    #save new dataset to csv-file\n",
    "    output_df = pd.DataFrame(list(zip(augmented_data_sentences, augmented_data_labels)), columns=['text', 'label'])\n",
    "    output_df.to_csv(path + output_fname, sep=\",\")\n",
    "\n",
    "    return path + output_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_aug_multiple(n, N, train_df, augs, R, output_fname, path='') :  #Using Multiple Augmenters\n",
    "    train_df_sample = train_df.sample(n=N, ignore_index=True)\n",
    "    #save sample as file\n",
    "    train_df_sample.to_csv(path + output_fname[:-4] + \"original.csv\".format(N=N), sep=\",\")\n",
    "\n",
    "    #aug.augment(data, n)\n",
    "    augmented_data_sentences = []\n",
    "    augmented_data_labels = []\n",
    "\n",
    "    assert(sum(R)==n)\n",
    "    assert(len(R)==len(augs))\n",
    "\n",
    "    for idx, row in train_df_sample.iterrows() :\n",
    "        sentence = row['text']\n",
    "        label = row['label']\n",
    "        augmented = []\n",
    "\n",
    "        for r, aug in zip(R, augs) :\n",
    "            sents = aug.augment(sentence, n=r) #if r > 1 else [aug.augment(sentence, n=r)]\n",
    "            augmented.append(sents)\n",
    "\n",
    "        #label-preserving\n",
    "        augmented_data_sentences += ([sentence] + augmented)\n",
    "        augmented_data_labels += [label for i in range(n+1)]\n",
    "    \n",
    "    #save new dataset to csv-file\n",
    "    output_df = pd.DataFrame(list(zip(augmented_data_sentences, augmented_data_labels)), columns=['text', 'label'])\n",
    "    output_df.to_csv(path + output_fname, sep=\",\")\n",
    "\n",
    "    return path + output_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data augmentation : Single\n",
    "# stop_words = list(set(stopwords.words('english')))\n",
    "# aug_spelling_prof = naw.SpellingAug(dict_path=fname_profanity, aug_max=None, aug_p=0.5, stopwords=stop_words)\n",
    "\n",
    "# fnames = [(\"Dataset_aug_profanity_{i}_.csv\").format(i=i) for i in [500, 1000, 5000, len(train_df_prof)]]\n",
    "# output_path = \"augmented_data/\"\n",
    "\n",
    "# for fname in fnames :\n",
    "#     tokens = fname.split(\"_\")\n",
    "#     N = int(tokens[3])\n",
    "#     print(\"Augmenting - N : {N}\".format(N=N))\n",
    "#     run_aug(5, N, train_df_prof, aug_spelling_prof, fname, path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting (Multiple-Augmenters) - N : 500\n",
      "Augmenting (Multiple-Augmenters) - N : 1000\n",
      "Augmenting (Multiple-Augmenters) - N : 5000\n",
      "Augmenting (Multiple-Augmenters) - N : 6513\n"
     ]
    }
   ],
   "source": [
    "#Data augmentation : Multiple Augmenters\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "#5 different augmenters\n",
    "aug_spelling_prof = naw.SpellingAug(dict_path=fname_profanity, aug_max=None, aug_p=0.5, stopwords=stop_words)\n",
    "aug_spelling_base = naw.SpellingAug(stopwords=stop_words)\n",
    "aug_char_keyboard = nac.KeyboardAug(stopwords=stop_words)\n",
    "aug_char_ocr = nac.OcrAug(stopwords=stop_words)\n",
    "aug_char_random = nac.RandomCharAug(stopwords=stop_words)\n",
    "\n",
    "augs = [aug_spelling_prof, aug_spelling_base, aug_char_keyboard, aug_char_ocr, aug_char_random]\n",
    "R = [1 for i in augs]\n",
    "\n",
    "fnames = [(\"Dataset_aug_complex_{i}_.csv\").format(i=i) for i in [500, 1000, 5000, len(train_df_prof)]]\n",
    "output_path = \"augmented_data/\"\n",
    "\n",
    "for fname in fnames :\n",
    "    tokens = fname.split(\"_\")\n",
    "    N = int(tokens[3])\n",
    "    print(\"Augmenting (Multiple-Augmenters) - N : {N}\".format(N=N))\n",
    "    run_aug_multiple(5, N, train_df_prof, augs, R, fname, path=output_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
