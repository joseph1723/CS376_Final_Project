{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk pandas nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cwwojin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas import isnull\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Profanity-spelling-error-dictionary (txt-file)\n",
    "#2 files : profanity-only, combined\n",
    "data_path = \"datasets/\"\n",
    "spelling_path = \"spelling/\"\n",
    "\n",
    "train_df = pd.read_csv(data_path + \"2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.1.csv\", sep=',')[['text', 'label']]\n",
    "profanity_df = pd.read_csv(data_path + \"profanity_en.csv\", sep=',')\n",
    "labels = ['canonical_form_1', 'canonical_form_2', 'canonical_form_3']\n",
    "spelling_dict = defaultdict(list)\n",
    "fname_profanity = spelling_path + \"spelling_en_profanity.txt\"\n",
    "#fname_combined = spelling_path + \"spelling_en_combined.txt\"\n",
    "\n",
    "for idx, row in profanity_df.iterrows() :\n",
    "    text = row['text']\n",
    "    canons = [i for i in list(row[labels]) if not isnull(i)]\n",
    "    for word in canons :\n",
    "        spelling_dict[word].append(text)\n",
    "\n",
    "if os.path.exists(fname_profanity):\n",
    "    os.remove(fname_profanity)\n",
    "\n",
    "with open(fname_profanity, \"w+\", encoding=\"UTF-8\") as spelling_dict_file :\n",
    "    for k, v in spelling_dict.items() :\n",
    "        #print(\" \".join([k] + v))\n",
    "        spelling_dict_file.write(\" \".join([k] + v))\n",
    "        spelling_dict_file.write(\"\\n\")\n",
    "\n",
    "# with open(fname_combined, \"a+\", encoding=\"UTF-8\") as spelling_dict_file :\n",
    "#     for k, v in spelling_dict.items() :\n",
    "#         #print(\" \".join([k] + v))\n",
    "#         spelling_dict_file.write(\" \".join([k] + v))\n",
    "#         spelling_dict_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifying Original Dataset\n",
    "profanities = list(spelling_dict.keys())\n",
    "for idx, row in train_df.iterrows() :\n",
    "    text = row['text']\n",
    "    label = row['label']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_aug(n, N, train_df, aug, output_fname, path='') :\n",
    "    train_df_sample = train_df.sample(n=N, ignore_index=True)\n",
    "    #save sample as file\n",
    "    train_df_sample.to_csv(path + output_fname[:-4] + \"sample_{N}_.csv\".format(N=N), sep=\",\")\n",
    "\n",
    "    #aug.augment(data, n)\n",
    "    augmented_data_sentences = []\n",
    "    augmented_data_labels = []\n",
    "\n",
    "    for idx, row in train_df_sample.iterrows() :\n",
    "        sentence = row['text']\n",
    "        label = row['label']\n",
    "        augmented = aug.augment(sentence, n=n) if n > 1 else [aug.augment(sentence, n=n)]\n",
    "\n",
    "        #label-preserving\n",
    "        augmented_data_sentences += ([sentence] + augmented)\n",
    "        augmented_data_labels += [label for i in range(n+1)]\n",
    "    \n",
    "    #save new dataset to csv-file\n",
    "    output_df = pd.DataFrame(list(zip(augmented_data_sentences, augmented_data_labels)), columns=['sentence', 'label'])\n",
    "    output_df.to_csv(path + output_fname, sep=\",\")\n",
    "\n",
    "    return path + output_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting - N : 1000\n",
      "Augmenting - N : 5000\n",
      "Augmenting - N : 10000\n"
     ]
    }
   ],
   "source": [
    "#Data augmentation\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "aug_spelling_prof = naw.SpellingAug(dict_path=fname_profanity, stopwords=stop_words)\n",
    "\n",
    "fnames = [(\"Dataset_aug_profanity_{i}_.csv\").format(i=i) for i in [1000, 5000, 10000]]\n",
    "output_path = \"augmented_data/\"\n",
    "\n",
    "for fname in fnames :\n",
    "    tokens = fname.split(\"_\")\n",
    "    N = int(tokens[3])\n",
    "    print(\"Augmenting - N : {N}\".format(N=N))\n",
    "    run_aug(5, N, train_df, aug_spelling_prof, fname, path=output_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
