{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk pandas nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cwwojin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "import pandas as pd\n",
    "from pandas import isnull\n",
    "from collections import defaultdict\n",
    "import nltk, os, re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Profanity-spelling-error-dictionary (txt-file)\n",
    "data_path = \"datasets/\"\n",
    "spelling_path = \"spelling/\"\n",
    "\n",
    "train_df = pd.read_csv(data_path + \"2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.1.csv\", sep=',')[['text', 'label']]\n",
    "profanity_df = pd.read_csv(data_path + \"profanity_en.csv\", sep=',')\n",
    "labels = ['canonical_form_1', 'canonical_form_2', 'canonical_form_3']\n",
    "spelling_dict = defaultdict(list)\n",
    "fname_profanity = spelling_path + \"spelling_en_profanity.txt\"\n",
    "\n",
    "for idx, row in profanity_df.iterrows() :\n",
    "    text = row['text']\n",
    "    canons = [i for i in list(row[labels]) if not isnull(i)]\n",
    "    for word in canons :\n",
    "        spelling_dict[word].append(text)\n",
    "\n",
    "if os.path.exists(fname_profanity):\n",
    "    os.remove(fname_profanity)\n",
    "\n",
    "with open(fname_profanity, \"w+\", encoding=\"UTF-8\") as spelling_dict_file :\n",
    "    for k, v in spelling_dict.items() :\n",
    "        #print(\" \".join([k] + v))\n",
    "        spelling_dict_file.write(\" \".join([k] + v))\n",
    "        spelling_dict_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_range(text) :\n",
    "    for c in text :\n",
    "        if ord(c) not in range(128) : return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1052\n",
      "10424\n"
     ]
    }
   ],
   "source": [
    "def contains_prof(text, profanities) :\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    return (not not set(tokenized_text).intersection(set(profanities)))\n",
    "\n",
    "#Modifying Original Dataset\n",
    "profanities = pd.read_csv(\"datasets/profanity_en_list.csv\", sep=',')    #list(spelling_dict.keys())\n",
    "profanities = list(profanities['word'])\n",
    "print(len(profanities))\n",
    "new_texts = []\n",
    "new_labels = []\n",
    "\n",
    "for idx, row in train_df.iterrows() :\n",
    "    text = row['text']\n",
    "    label = row['label']\n",
    "    if contains_prof(text, profanities) and len(text) <= 256 and ascii_range(text):\n",
    "        new_texts.append(text)\n",
    "        new_labels.append(label)\n",
    "\n",
    "train_df_prof = pd.DataFrame(list(zip(new_texts, new_labels)), columns=['text', 'label'])\n",
    "train_df_prof.to_csv(data_path + \"2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.1-ProfanityOnly.csv\")\n",
    "print(len(train_df_prof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sent(sentence, aug, n, profanities) :\n",
    "    augmented_sents = []\n",
    "    sent_tokenized = word_tokenize(sentence)\n",
    "    for i in range(n) :\n",
    "        #augmented = \" \".join([aug.augment(word, n=1) if word in set(profanities) else word for word in sent_tokenized])\n",
    "        augmented = TreebankWordDetokenizer().detokenize([aug.augment(word, n=1) if word in set(profanities) else word for word in sent_tokenized])\n",
    "        augmented_sents.append(augmented)\n",
    "    return augmented_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def run_aug_multiple(n, N, train_df, augs, R, output_fname, profanities, path='', add_original=True, random=False) :  #Using Multiple Augmenters\n",
    "    train_df_sample = train_df.sample(n=N, ignore_index=True)\n",
    "    #save sample as file\n",
    "    train_df_sample.to_csv(path + output_fname[:-4] + \"original.csv\".format(N=N), sep=\",\")\n",
    "\n",
    "    #aug.augment(data, n)\n",
    "    augmented_data_sentences = []\n",
    "    augmented_data_labels = []\n",
    "\n",
    "    assert(sum(R)==n)\n",
    "    assert(len(R)==len(augs))\n",
    "\n",
    "    for idx, row in train_df_sample.iterrows() :\n",
    "        sentence = row['text']\n",
    "        label = row['label']\n",
    "        augmented = []\n",
    "\n",
    "        if random :\n",
    "            random_aug = sample(augs, 1)[0]\n",
    "            sents = augment_sent(sentence, random_aug, n=1, profanities=profanities)\n",
    "            augmented += sents\n",
    "        else :\n",
    "            for r, aug in zip(R, augs) :\n",
    "                #sents = aug.augment(sentence, n=r) #if r > 1 else [aug.augment(sentence, n=r)]\n",
    "                sents = augment_sent(sentence, aug, n=r, profanities=profanities)\n",
    "                #augmented.append(sents)\n",
    "                augmented += sents\n",
    "\n",
    "        #label-preserving\n",
    "        if add_original :\n",
    "            augmented_data_sentences += ([sentence] + augmented)\n",
    "            augmented_data_labels += [label for i in range(n+1)]\n",
    "        else :\n",
    "            augmented_data_sentences += augmented\n",
    "            augmented_data_labels += [label for i in range(n)]\n",
    "    \n",
    "    #save new dataset to csv-file\n",
    "    output_df = pd.DataFrame(list(zip(augmented_data_sentences, augmented_data_labels)), columns=['text', 'label'])\n",
    "    output_df.to_csv(path + output_fname, sep=\",\")\n",
    "\n",
    "    return path + output_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting (Multiple-Augmenters) - N : 10424\n"
     ]
    }
   ],
   "source": [
    "#Data augmentation : Multiple Augmenters\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "#5 different augmenters\n",
    "aug_spelling_prof = naw.SpellingAug(dict_path=fname_profanity, aug_max=None, aug_p=0.5, stopwords=stop_words)\n",
    "aug_spelling_base = naw.SpellingAug(stopwords=stop_words)\n",
    "aug_char_keyboard = nac.KeyboardAug(stopwords=stop_words)\n",
    "aug_char_ocr = nac.OcrAug(stopwords=stop_words)\n",
    "aug_char_random = nac.RandomCharAug(stopwords=stop_words)\n",
    "\n",
    "augs = [aug_spelling_prof, aug_spelling_base, aug_char_keyboard, aug_char_ocr, aug_char_random]\n",
    "R = [1 for i in augs]\n",
    "\n",
    "fnames = [(\"Dataset_aug_complex_{i}_.csv\").format(i=i) for i in [len(train_df_prof)]]\n",
    "output_path = \"augmented_data/\"\n",
    "\n",
    "for fname in fnames :\n",
    "    tokens = fname.split(\"_\")\n",
    "    N = int(tokens[3])\n",
    "    print(\"Augmenting (Multiple-Augmenters) - N : {N}\".format(N=N))\n",
    "    #run_aug_multiple(len(augs[1:]), N, train_df_prof, augs[1:], R[1:], fname, profanities, path=output_path, add_original=False)\n",
    "    run_aug_multiple(len(augs[1:]), N, train_df_prof, augs[1:], R[1:], fname, profanities, path=output_path, add_original=False, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10424 10424\n"
     ]
    }
   ],
   "source": [
    "sample_aug_df = pd.read_csv('augmented_data/Dataset_aug_complex_{i}_.csv'.format(i=len(train_df_prof)),sep=',')\n",
    "sample_original_df = pd.read_csv('augmented_data/Dataset_aug_complex_{i}_original.csv'.format(i=len(train_df_prof)),sep=',')\n",
    "print(len(sample_original_df), len(sample_aug_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_char_keyboard = nac.KeyboardAug(aug_char_max=3, aug_char_p=0.2)\n",
    "aug_char_ocr = nac.OcrAug(aug_char_max=3, aug_char_p=0.2)\n",
    "#‘insert’, ‘substitute’, ‘swap’ and ‘delete’.\n",
    "aug_char_random_in = nac.RandomCharAug(aug_char_max=3, aug_char_p=0.2, action='insert')\n",
    "aug_char_random_sub = nac.RandomCharAug(aug_char_max=3, aug_char_p=0.2, action='substitute')\n",
    "aug_char_random_swap = nac.RandomCharAug(aug_char_max=3, aug_char_p=0.2, action='swap')\n",
    "aug_char_random_del = nac.RandomCharAug(aug_char_max=3, aug_char_p=0.2, action='delete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      text              original\n",
      "0                  sidicks             shitdicks\n",
      "1              baresitcles           breasticles\n",
      "2     shiafucumo0herfucker  shitfuckmotherfucker\n",
      "3                 6itcheks              bitchers\n",
      "4                  cokcsku               cocksuk\n",
      "...                    ...                   ...\n",
      "1046          muddekforker          mudderfukker\n",
      "1047               bstindo             bastinado\n",
      "1048           cuntboAlonW           cuntbollock\n",
      "1049             octoupsys             octopussy\n",
      "1050                  nigu                 niguh\n",
      "\n",
      "[1051 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "augs = [aug_char_keyboard, aug_char_ocr, aug_char_random_in, aug_char_random_sub, aug_char_random_swap, aug_char_random_del]\n",
    "\n",
    "#profanities -> augmented profanities\n",
    "\n",
    "profanities = [p for p in profanities if ascii_range(p)]\n",
    "new_prof = []\n",
    "prof_augs = []\n",
    "for p in profanities :\n",
    "    aug = sample(augs, 1)[0]\n",
    "    p_aug = aug.augment(p, n=1)\n",
    "    for c in p_aug :\n",
    "        if not ascii_range(c) :\n",
    "            print(p_aug)\n",
    "            p_aug = p_aug.replace(c,\"\")\n",
    "            print(p_aug)\n",
    "    prof_augs.append(p_aug)\n",
    "\n",
    "\n",
    "Char_Aug_df = pd.DataFrame(zip(prof_augs, profanities), columns=['text','original'])\n",
    "\n",
    "print(Char_Aug_df)\n",
    "Char_Aug_df.to_csv(\"augmented_data/Dataset_aug_char_{i}.csv\".format(i=len(Char_Aug_df)),sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              text   original\n",
      "0        sbitdifks  shitdicks\n",
      "1        shitdKckQ  shitdicks\n",
      "2        shitdicr8  shitdicks\n",
      "3        8hitdicrs  shitdicks\n",
      "4      shCiztdicks  shitdicks\n",
      "...            ...        ...\n",
      "12607        jiguh      niguh\n",
      "12608        ngiuh      niguh\n",
      "12609        niugh      niguh\n",
      "12610         niuh      niguh\n",
      "12611         nigu      niguh\n",
      "\n",
      "[12612 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "profanities = [p for p in profanities if ascii_range(p)]\n",
    "N=2\n",
    "\n",
    "new_prof = []\n",
    "prof_augs = []\n",
    "for p in profanities :\n",
    "    p_aug = [aug.augment(p, n=N) for aug in augs]\n",
    "    p_aug = [p for l in p_aug for p in l]\n",
    "    prof_augs += p_aug\n",
    "    new_prof += [p for i in range(len(p_aug))]\n",
    "\n",
    "Char_Aug_df = pd.DataFrame(zip(prof_augs, new_prof), columns=['text','original'])\n",
    "print(Char_Aug_df)\n",
    "\n",
    "Char_Aug_df.to_csv(\"augmented_data/Dataset_aug_char_{i}.csv\".format(i=len(Char_Aug_df)),sep=',')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
