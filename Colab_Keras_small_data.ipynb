{"cells":[{"cell_type":"markdown","metadata":{"id":"KIR1zP0dvAOa"},"source":["# Hate-speech Detection model (w/ Small, Fine-selected data)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJ4koSJ1xbLr","outputId":"ec6eca86-fc5d-4152-92c5-2cdb1d0b2fda","executionInfo":{"status":"ok","timestamp":1655469934985,"user_tz":-540,"elapsed":601,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'CS376_Final_Project' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/joseph1723/CS376_Final_Project.git"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"YHWRrOYpuyPV","executionInfo":{"status":"ok","timestamp":1655469934987,"user_tz":-540,"elapsed":83,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import Adam\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import math\n","from keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"CjInRHDwyuUU","executionInfo":{"status":"ok","timestamp":1655469934988,"user_tz":-540,"elapsed":81,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[],"source":["class TestDataset(Dataset) :\n","  #Dataset - English/typo-added/labeled\n","  def __init__(self, df) :\n","    self.df = df\n","  \n","  def __len__(self) :\n","    return len(self.df)\n","  \n","  def __getitem__(self, idx):\n","    text = self.df.iloc[idx, 0]\n","    item = self.df.iloc[idx, 1]\n","    return text, item"]},{"cell_type":"markdown","metadata":{"id":"0stfCVXzA2yC"},"source":["Hyperparameters"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"3AaP5n2IMsMg","executionInfo":{"status":"ok","timestamp":1655469934989,"user_tz":-540,"elapsed":80,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[],"source":["train_rate, test_rate = 0.9, 0.05\n","total_df = pd.read_csv('/content/CS376_Final_Project/augmented_data/Dataset_new_856.csv', sep=',')\n","total_df.dropna(inplace=True)\n","total_df = total_df[[\"text\", \"label\"]]\n","total_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in total_df[\"label\"]]"]},{"cell_type":"markdown","metadata":{"id":"7DXbBmkz3XJ9"},"source":["LSTM 데이타 전처리"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"L6Xrr5bD4w85","executionInfo":{"status":"ok","timestamp":1655469934989,"user_tz":-540,"elapsed":76,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[],"source":["total_dataset = TestDataset(total_df)\n","total_loader = DataLoader(total_dataset, batch_size=1, shuffle=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-Z5G2AhlqzT","outputId":"80170717-385e-4cad-8ebf-e1233f4e4c87","executionInfo":{"status":"ok","timestamp":1655469934990,"user_tz":-540,"elapsed":74,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'textClassifier' already exists and is not an empty directory.\n"]}],"source":["# clone the repo\n","!git clone https://github.com/richliao/textClassifier.git\n","# install Dependent library"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFoIN0a3mICT","outputId":"f917c6b2-cc75-4e9f-f46a-0e76700592c9","executionInfo":{"status":"ok","timestamp":1655469984486,"user_tz":-540,"elapsed":49550,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting backports.weakref==1.0.post1\n","  Using cached backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n","Requirement already satisfied: beautifulsoup4==4.6.3 in /usr/local/lib/python3.7/dist-packages (from -r ./textClassifier/req.txt (line 2)) (4.6.3)\n","Collecting bleach==1.5.0\n","  Using cached bleach-1.5.0-py2.py3-none-any.whl (17 kB)\n","Requirement already satisfied: bs4==0.0.1 in /usr/local/lib/python3.7/dist-packages (from -r ./textClassifier/req.txt (line 4)) (0.0.1)\n","Collecting Cython==0.27.3\n","  Using cached Cython-0.27.3.tar.gz (1.8 MB)\n","Collecting enum34==1.1.6\n","  Using cached enum34-1.1.6-py3-none-any.whl (12 kB)\n","Collecting funcsigs==1.0.2\n","  Using cached funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n","Collecting h5py==2.7.1\n","  Using cached h5py-2.7.1.tar.gz (264 kB)\n","Collecting html5lib==0.9999999\n","  Using cached html5lib-0.9999999.tar.gz (889 kB)\n","Collecting Keras==2.0.8\n","  Using cached Keras-2.0.8-py2.py3-none-any.whl (276 kB)\n","Collecting laspy==1.5.0\n","  Using cached laspy-1.5.0-py3-none-any.whl (489 kB)\n","Collecting lda==1.0.5\n","  Using cached lda-1.0.5.tar.gz (303 kB)\n","Collecting Markdown==2.6.9\n","  Using cached Markdown-2.6.9.tar.gz (271 kB)\n","Collecting mock==2.0.0\n","  Using cached mock-2.0.0-py2.py3-none-any.whl (56 kB)\n","Collecting nltk==3.3\n","  Using cached nltk-3.3.0.zip (1.4 MB)\n","Collecting numpy==1.13.3\n","  Using cached numpy-1.13.3.zip (5.0 MB)\n","Collecting olefile==0.44\n","  Using cached olefile-0.44.zip (74 kB)\n","Collecting pandas==0.20.3\n","  Using cached pandas-0.20.3.tar.gz (10.4 MB)\n","Collecting pbr==3.1.1\n","  Using cached pbr-3.1.1-py2.py3-none-any.whl (99 kB)\n","Collecting Pillow==4.3.0\n","  Using cached Pillow-4.3.0.tar.gz (13.9 MB)\n","Collecting protobuf==3.4.0\n","  Using cached protobuf-3.4.0-py2.py3-none-any.whl (375 kB)\n","Collecting pypinyin==0.29.0\n","  Using cached pypinyin-0.29.0-py2.py3-none-any.whl (987 kB)\n","Collecting python-dateutil==2.6.1\n","  Using cached python_dateutil-2.6.1-py2.py3-none-any.whl (194 kB)\n","Collecting pytz==2017.2\n","  Using cached pytz-2017.2-py2.py3-none-any.whl (484 kB)\n","Collecting PyYAML==3.12\n","  Using cached PyYAML-3.12.zip (375 kB)\n","Collecting scikit-learn==0.19.1\n","  Using cached scikit-learn-0.19.1.tar.gz (9.5 MB)\n","Collecting scipy==0.19.1\n","  Using cached scipy-0.19.1.tar.gz (14.1 MB)\n","Collecting six==1.11.0\n","  Using cached six-1.11.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r ./textClassifier/req.txt (line 29)) (0.0)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.3.0 (from versions: 1.13.1, 1.13.2, 1.14.0, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.0+zzzcolab20220506153740, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.4+zzzcolab20220516125453, 2.6.5, 2.6.5+zzzcolab20220523104206, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.0+zzzcolab20220506150900, 2.7.1, 2.7.2, 2.7.2+zzzcolab20220516114640, 2.7.3, 2.7.3+zzzcolab20220523111007, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.0+zzzcolab20220506162203, 2.8.1, 2.8.1+zzzcolab20220516111314, 2.8.1+zzzcolab20220518083849, 2.8.2, 2.8.2+zzzcolab20220523105045, 2.8.2+zzzcolab20220527125636, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for tensorflow==1.3.0\u001b[0m\n"]}],"source":["# !cd textClassifier\n","# !ls\n","# !pip install -r req.txt\n","!pip install -r ./textClassifier/req.txt"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6GbH06wH27U3","outputId":"f028b1e4-e9ba-4fab-f871-0220a33b6ec4","executionInfo":{"status":"ok","timestamp":1655469987256,"user_tz":-540,"elapsed":2802,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220527125636)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n","Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.46.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.7)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"]}],"source":["!pip install tensorflow"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SHzEjw4moKBn","outputId":"5ebb668f-9d9a-4ce8-fd17-6a9f803865bc","executionInfo":{"status":"ok","timestamp":1655469896504,"user_tz":-540,"elapsed":160435,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-06-17 12:42:15--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2022-06-17 12:42:15--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2022-06-17 12:42:15--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 40s  \n","\n","2022-06-17 12:44:55 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n"]}],"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQLAP7ANoN1E","outputId":"0d049859-3ac1-4c52-bafd-337bf3590893","executionInfo":{"status":"ok","timestamp":1655469928859,"user_tz":-540,"elapsed":32390,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"]}],"source":["!unzip glove.6B.zip"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0XAVNanDpBzX","outputId":"8ab4528d-7ff9-4ab2-f255-565e8132c95c","executionInfo":{"status":"ok","timestamp":1655469929369,"user_tz":-540,"elapsed":543,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-06-17 12:45:28--  https://www.kaggle.com/c/word2vec-nlp-tutorial/download/labeledTrainData.tsv\n","Resolving www.kaggle.com (www.kaggle.com)... 35.244.233.98\n","Connecting to www.kaggle.com (www.kaggle.com)|35.244.233.98|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /account/login?returnUrl=%2Fcompetitions%2Fword2vec-nlp-tutorial [following]\n","--2022-06-17 12:45:28--  https://www.kaggle.com/account/login?returnUrl=%2Fcompetitions%2Fword2vec-nlp-tutorial\n","Reusing existing connection to www.kaggle.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘labeledTrainData.tsv’\n","\n","labeledTrainData.ts     [ <=>                ]   6.56K  --.-KB/s    in 0.008s  \n","\n","2022-06-17 12:45:28 (789 KB/s) - ‘labeledTrainData.tsv’ saved [6719]\n","\n"]}],"source":["!wget https://www.kaggle.com/c/word2vec-nlp-tutorial/download/labeledTrainData.tsv"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lw_PaR6wodXY","outputId":"6b448402-e130-4a88-e2a2-c8da409d363a","executionInfo":{"status":"ok","timestamp":1655469932950,"user_tz":-540,"elapsed":3587,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.30)\n"]}],"source":["!pip install --upgrade cython"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fxOeDJSAnHfa","outputId":"6c70cff7-62de-4bd8-fa44-6b6a26ab864a","executionInfo":{"status":"ok","timestamp":1655469934392,"user_tz":-540,"elapsed":1450,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.3.0 (from versions: 1.13.1, 1.13.2, 1.14.0, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.0+zzzcolab20220506153740, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.4+zzzcolab20220516125453, 2.6.5, 2.6.5+zzzcolab20220523104206, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.0+zzzcolab20220506150900, 2.7.1, 2.7.2, 2.7.2+zzzcolab20220516114640, 2.7.3, 2.7.3+zzzcolab20220523111007, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.0+zzzcolab20220506162203, 2.8.1, 2.8.1+zzzcolab20220516111314, 2.8.1+zzzcolab20220518083849, 2.8.2, 2.8.2+zzzcolab20220523105045, 2.8.2+zzzcolab20220527125636, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for tensorflow==1.3.0\u001b[0m\n"]}],"source":["!pip install --upgrade tensorflow==1.3.0"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mbFHFHwKiqRB","outputId":"a2ea1f02-0f33-4f00-fdff-a3873aed828a","executionInfo":{"status":"ok","timestamp":1655470046477,"user_tz":-540,"elapsed":58349,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras_preprocessing/text.py:180: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n","  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"]},{"output_type":"stream","name":"stdout","text":["Total 400000 word vectors.\n","model fitting - attention Bi-LSTM network\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 54)]         0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, 54, 100)      376800      ['input_1[0][0]']                \n","                                                                                                  \n"," bidirectional (Bidirectional)  (None, 54, 200)      160800      ['embedding[0][0]']              \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," attention_layer (AttentionLaye  ((None, 200),       0           ['bidirectional[0][0]',          \n"," r)                              (None, 3),                       'input_2[0][0]']                \n","                                 (None, 54))                                                      \n","                                                                                                  \n"," dense (Dense)                  (None, 2)            402         ['attention_layer[0][0]']        \n","                                                                                                  \n","==================================================================================================\n","Total params: 538,002\n","Trainable params: 538,002\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/500\n","14/14 [==============================] - 10s 80ms/step - loss: 0.6534 - acc: 0.5912 - val_loss: 0.5001 - val_acc: 0.8246\n","Epoch 2/500\n","14/14 [==============================] - 0s 15ms/step - loss: 0.4162 - acc: 0.8423 - val_loss: 0.3034 - val_acc: 0.8713\n","Epoch 3/500\n","14/14 [==============================] - 0s 14ms/step - loss: 0.2799 - acc: 0.8891 - val_loss: 0.2358 - val_acc: 0.9181\n","Epoch 4/500\n","14/14 [==============================] - 0s 14ms/step - loss: 0.1966 - acc: 0.9285 - val_loss: 0.1927 - val_acc: 0.9415\n","Epoch 5/500\n","14/14 [==============================] - 0s 14ms/step - loss: 0.1327 - acc: 0.9606 - val_loss: 0.1732 - val_acc: 0.9474\n","Epoch 6/500\n","14/14 [==============================] - 0s 14ms/step - loss: 0.0691 - acc: 0.9839 - val_loss: 0.1595 - val_acc: 0.9649\n","Epoch 7/500\n","14/14 [==============================] - 0s 15ms/step - loss: 0.0403 - acc: 0.9898 - val_loss: 0.1385 - val_acc: 0.9591\n","Epoch 8/500\n","14/14 [==============================] - 0s 14ms/step - loss: 0.0256 - acc: 0.9971 - val_loss: 0.1480 - val_acc: 0.9649\n","Epoch 9/500\n","14/14 [==============================] - 0s 14ms/step - loss: 0.0261 - acc: 0.9942 - val_loss: 0.1694 - val_acc: 0.9357\n","Epoch 10/500\n","14/14 [==============================] - 0s 15ms/step - loss: 0.0233 - acc: 0.9971 - val_loss: 0.1352 - val_acc: 0.9532\n","Epoch 11/500\n","14/14 [==============================] - 0s 14ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.1231 - val_acc: 0.9591\n","Epoch 12/500\n","14/14 [==============================] - 0s 15ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.1405 - val_acc: 0.9649\n","Epoch 13/500\n","14/14 [==============================] - 0s 14ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.1436 - val_acc: 0.9708\n","Epoch 14/500\n","14/14 [==============================] - 0s 14ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.1455 - val_acc: 0.9591\n","Epoch 15/500\n","14/14 [==============================] - 0s 15ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.1422 - val_acc: 0.9708\n","Epoch 16/500\n","14/14 [==============================] - 0s 15ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.1410 - val_acc: 0.9649\n","Epoch 17/500\n","14/14 [==============================] - 0s 15ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.1409 - val_acc: 0.9532\n","Epoch 18/500\n","14/14 [==============================] - 0s 15ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.1415 - val_acc: 0.9591\n","Epoch 19/500\n","14/14 [==============================] - 0s 14ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.1400 - val_acc: 0.9766\n","Epoch 20/500\n","14/14 [==============================] - 0s 15ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.1459 - val_acc: 0.9532\n","Epoch 21/500\n","14/14 [==============================] - 0s 15ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.1393 - val_acc: 0.9766\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f6fbe078910>"]},"metadata":{},"execution_count":22}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras import backend as K\n","import numpy as np\n","import pandas as pd\n","import sys, os, importlib, re, tensorflow.python.keras.engine\n","from collections import defaultdict\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils.np_utils import to_categorical\n","from keras.layers import Dense, Input, Embedding, Dropout, LSTM, Bidirectional, Embedding, Layer\n","from keras.models import Model\n","from keras import initializers\n","\n","os.environ['KERAS_BACKEND']='theano'\n","\n","#Parameters\n","MAX_SEQUENCE_LENGTH = 54\n","MAX_NB_WORDS = 20000\n","EMBEDDING_DIM = 100\n","VALIDATION_SPLIT = 0.2\n","\n","\n","texts = []\n","labels = []\n","valid_lengths = []\n","split_input = []\n","\n","for text, label in total_loader:\n","  texts.append(text[0])\n","  labels.append(label[0])\n","  valid_lengths.append(len(text[0].split(' ')))\n","  split_input.append(text[0].split(' '))\n","    \n","\n","tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","word_index = tokenizer.word_index\n","\n","textseq = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n","split = pad_sequences(split_input, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post', dtype = object, value = '_PAD_')\n","\n","labels = to_categorical(np.asarray(labels))\n","\n","indices = np.arange(textseq.shape[0])\n","np.random.shuffle(indices)\n","\n","\n","valid_lengths = np.array(valid_lengths)\n","split = np.array(split)\n","textseq = textseq[indices]\n","labels = labels[indices]\n","valid_lens = valid_lengths[indices]\n","split = split[indices]\n","nb_validation_samples = int(VALIDATION_SPLIT * textseq.shape[0])\n","\n","\n","texts_for_print = np.array(texts)\n","texts_for_print = texts_for_print[indices]\n","\n","\n","x_train = textseq[:-nb_validation_samples]\n","y_train = labels[:-nb_validation_samples]\n","x_val = textseq[-nb_validation_samples:]\n","y_val = labels[-nb_validation_samples:]\n","vallen_train = valid_lens[:-nb_validation_samples]\n","vallen_val = valid_lens[-nb_validation_samples:]\n","\n","class AttentionLayer(Layer):\n","    def __init__(self, **kwargs):\n","        self.init = initializers.get('normal')\n","        super(AttentionLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape)==3\n","        self.W = self.init((input_shape[-1],))\n","        self.sm = tf.keras.layers.Softmax(axis = -1)\n","        super(AttentionLayer, self).build(input_shape)\n","\n","    def call(self, x, mask=None, splited_input = None, answer = None, batch_size = None):\n","        eij = K.tanh(K.squeeze(K.dot(x, K.expand_dims(self.W)), axis=-1))\n","        masking = np.array([range(x.shape[1])])<mask\n","        ai = self.sm(eij, masking)\n","        ai_result = tf.argsort(ai, direction = 'DESCENDING',)\n","        weights = ai/tf.expand_dims(K.sum(ai, axis = 1), 1)\n","        weighted_input = x*tf.expand_dims(weights,2)\n","        return K.sum(weighted_input, axis = 1), ai_result[:, :3], ai\n","\n","\n","GLOVE_DIR = \"./\"\n","embeddings_index = {}\n","f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Total %s word vectors.' % len(embeddings_index))\n","\n","\n","\n","embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","        \n","#Input Declartion\n","sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n","valid_len_input = Input(shape = (1, ), dtype = 'int32')\n","\n","#Model structure\n","embedded_sequences = Embedding(len(word_index) + 1,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=MAX_SEQUENCE_LENGTH,\n","                            trainable=True)(sequence_input)\n","lstm_out = Bidirectional(LSTM(100, return_sequences=True))(embedded_sequences)\n","att_out, _, _ = AttentionLayer()(lstm_out, mask = valid_len_input)\n","preds = Dense(2, activation='softmax')(att_out)\n","#Early stopper\n","callback = tf.keras.callbacks.EarlyStopping(\n","    monitor=\"val_loss\",\n","    min_delta=0,\n","    patience=10,\n","    verbose=0,\n","    mode=\"auto\",\n","    baseline=None,\n","    restore_best_weights=False,\n",")\n","#Model Train\n","model = Model([sequence_input, valid_len_input], preds)\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['acc'])\n","print(\"model fitting - attention Bi-LSTM network\")\n","model.summary()\n","model.fit([x_train, vallen_train], y_train, validation_data=([x_val, vallen_val], y_val),\n","          batch_size = 50 , epochs = 500, callbacks=[callback])"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G6ewjLYZ1EFo","outputId":"d33ddbba-fe19-4226-f1ae-aa1edad56ce0","executionInfo":{"status":"ok","timestamp":1655470114368,"user_tz":-540,"elapsed":12058,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as softmax_layer_call_fn, softmax_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: ./model_save_check_small/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: ./model_save_check_small/assets\n","WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f70397ac810> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f7039879f90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"]}],"source":["model.save('./model_save_check_small')"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9Gp8FTk1n50","outputId":"34038902-9185-43c9-ef53-691add7b6f5a","executionInfo":{"status":"ok","timestamp":1655470114875,"user_tz":-540,"elapsed":528,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["updating: model_save_check_small/ (stored 0%)\n","updating: model_save_check_small/variables/ (stored 0%)\n","updating: model_save_check_small/variables/variables.data-00000-of-00001 (deflated 13%)\n","updating: model_save_check_small/variables/variables.index (deflated 65%)\n","updating: model_save_check_small/keras_metadata.pb (deflated 89%)\n","updating: model_save_check_small/saved_model.pb (deflated 91%)\n","updating: model_save_check_small/assets/ (stored 0%)\n"]}],"source":["!zip -r model_save_check_small.zip model_save_check_small"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0-sc0kPucSX","outputId":"dfc2c732-07ad-41eb-fd90-307e71846427","executionInfo":{"status":"ok","timestamp":1655466157657,"user_tz":-540,"elapsed":977,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["                                                  text  label\n","0    Any of those gas chambers still available? I w...      0\n","1    He will never propose to you, when you get old...      0\n","2    there is no difference between white women and...      0\n","3    The world would be so much better if we were d...      0\n","4    comparing maternity leave with paternity leave...      0\n","..                                                 ...    ...\n","395  I read an article where black men said it can ...      1\n","396  Honestly, I was pretty shocked to hear that sh...      1\n","397  I still believe all of that is because most of...      1\n","398  you can not come with us because you are weari...      1\n","399                               some men are abusers      1\n","\n","[400 rows x 2 columns]\n"]}],"source":["Test_uniform_df = pd.read_csv(\"/content/CS376_Final_Project/datasets/2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.2.csv\",sep=',')\n","Test_uniform_df.dropna(inplace=True)\n","Test_uniform_df = Test_uniform_df[['text','label']]\n","Test_uniform_df_hate = Test_uniform_df.loc[Test_uniform_df['label'] == 'hate',:]\n","Test_uniform_df_nothate = Test_uniform_df.loc[Test_uniform_df['label'] == 'nothate',:]\n","Test_uniform_df = pd.concat([Test_uniform_df_hate[:200], Test_uniform_df_nothate[:200]], axis=0).reset_index(drop=True)\n","#Test_uniform_df = Test_uniform_df.sample(frac=1).reset_index(drop=True)\n","Test_uniform_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in Test_uniform_df[\"label\"]]\n","\n","print(Test_uniform_df)\n","\n","texts_test = list(Test_uniform_df['text'])\n","results_True = list(Test_uniform_df['label'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_75E0C4vp5Zx"},"outputs":[],"source":["valid_lengths_test = []\n","split_input_test = []\n","for text in texts_test:\n","  valid_lengths_test.append(len(text.split(' '))) \n","  split_input_test.append(text.split(' '))\n","sequences_test = tokenizer.texts_to_sequences(texts_test)\n","\n","word_index = tokenizer.word_index\n","\n","data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n","split_test = pad_sequences(split_input_test, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post', dtype = object, value = '_PAD_')\n","\n","indicess = np.arange(data_test.shape[0])\n","# np.random.shuffle(indices)\n","\n","valid_lengths_test = np.array(valid_lengths_test)\n","split_test = np.array(split_test)\n","textss = np.array(texts_test)\n","\n","x_test = data_test[:]\n","vallen_test = valid_lengths_test[:]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BfrNpUdaY8Lg","outputId":"1b1250fc-53b0-4de9-ee89-99d1d89c04cb","executionInfo":{"status":"ok","timestamp":1655466164024,"user_tz":-540,"elapsed":8,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1\n"," 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n"," 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n"," 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1\n"," 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1\n"," 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0\n"," 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0\n"," 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1\n"," 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1\n"," 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1\n"," 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1]\n"]}],"source":["get_fil_tgt = K.function([model.layers[0].input, model.layers[3].input],\n","                                  [model.layers[4].output, model.layers[5].output])\n","x = [x_test, vallen_test]\n","[_, fil_tgt, ai], preds = get_fil_tgt(x)\n","result = preds.argmax(axis = -1)\n","print(result)\n","\n","# for i in range(len(fil_tgt)):\n","#   # print(fil_tgt[0][1][i])\n","#   if result[i] == 0 and results_True[i] == 0:\n","#     print(split_test[i, fil_tgt[i]], textss[i], ai[i][fil_tgt[i]])\n","#   # else:\n","#   #   print(textss[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MQUReIB2Q0aD","outputId":"f4e0887a-67f4-4063-be42-b4d594af8b67","executionInfo":{"status":"ok","timestamp":1655466168174,"user_tz":-540,"elapsed":970,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["120\n","[('fucking', 5), ('are', 5), ('rude', 4)]\n"]}],"source":["words = []\n","for i in range(len(fil_tgt)):\n","  # print(fil_tgt[0][1][i])\n","  if result[i] == 0 and results_True[i] == 0:\n","    words += list(split_test[i, fil_tgt[i]])\n","  \n","print(len(words))\n","\n","from collections import Counter\n","occurence_count = Counter(words)\n","print(occurence_count.most_common(3))"]},{"cell_type":"markdown","metadata":{"id":"KgPoZ_GV6S5W"},"source":["# Scoring (400 samples w/ uniform-distribution)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GfmSLMXlzCD4","outputId":"8de7760b-c27b-44fe-9d9a-c9b4ca28009f","executionInfo":{"status":"ok","timestamp":1655466170399,"user_tz":-540,"elapsed":9,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"]}],"source":["print(results_True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nNUigAxnzRP4","outputId":"1cec743e-ae99-4e4b-bab8-0de5ebd9283d","executionInfo":{"status":"ok","timestamp":1655466176003,"user_tz":-540,"elapsed":4779,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.9.1-py3-none-any.whl (419 kB)\n","\u001b[K     |████████████████████████████████| 419 kB 31.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.2.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.11.0+cu113)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.9.1\n"]}],"source":["!pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dU6vga3KdeQ4","outputId":"b5a9307d-4502-4638-ced6-199931cb0c71","executionInfo":{"status":"ok","timestamp":1655466176719,"user_tz":-540,"elapsed":732,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.3775)\n","tensor(0.3775) tensor(0.3775)\n","tensor(0.3775)\n"]}],"source":["from torchmetrics.functional import precision_recall\n","from torchmetrics import F1Score\n","from torchmetrics import Accuracy\n","\n","#Scoring\n","#results_True / results\n","labels_true = torch.tensor(results_True)\n","labels_pred = torch.tensor(result)\n","\n","acc = Accuracy()\n","accuracy = acc(labels_pred, labels_true)\n","\n","precision, recall = precision_recall(labels_pred, labels_true)\n","f1 = F1Score()\n","f1_score = f1(labels_pred, labels_true)\n","\n","print(accuracy)\n","print(precision, recall)\n","print(f1_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOJpKvxi2gUS","outputId":"4a3691a9-9f31-4f42-f13e-ccca70f1c583","executionInfo":{"status":"ok","timestamp":1655466178569,"user_tz":-540,"elapsed":10,"user":{"displayName":"김동근","userId":"08688198781637620604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[6, 9, 16, 23, 31, 34, 35, 38, 51, 53, 56, 71, 72, 74, 84, 87, 92, 106, 119, 121, 124, 131, 136, 142, 143, 150, 153, 155, 158, 161, 163, 164, 167, 170, 174, 179, 181, 183, 187, 195, 200, 201, 208, 210, 215, 216, 218, 219, 220, 223, 224, 225, 235, 236, 237, 239, 240, 242, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 260, 262, 265, 269, 270, 271, 275, 276, 277, 278, 280, 281, 283, 284, 285, 287, 288, 291, 292, 295, 296, 297, 300, 303, 304, 305, 306, 307, 308, 309, 311, 312, 317, 319, 320, 321, 322, 323, 325, 327, 329, 330, 332, 333, 334, 336, 337, 338, 339, 342, 344, 346, 347, 351, 352, 353, 354, 356, 357, 358, 359, 363, 364, 366, 369, 372, 373, 376, 377, 382, 385, 386, 389, 390, 393, 394, 395, 396, 397, 398, 399]\n"]}],"source":["crr = list(labels_pred==labels_true)\n","print(([i for i in range(len(crr)) if crr[i]]))"]},{"cell_type":"markdown","metadata":{"id":"NwSi72Mg6Yrx"},"source":["#Scoring Done."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Colab_Keras_small_data.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}