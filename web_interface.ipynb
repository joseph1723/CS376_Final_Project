{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch keras tensorflow pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cwwojin/opt/anaconda3/envs/cwwojin/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "MAX_SEQUENCE_LENGTH = 54\n",
    "\n",
    "\n",
    "class TestDataset(Dataset) :\n",
    "  #Dataset - English/typo-added/labeled\n",
    "  def __init__(self, df) :\n",
    "    self.df = df\n",
    "  \n",
    "  def __len__(self) :\n",
    "    return len(self.df)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    text = self.df.iloc[idx, 0]\n",
    "    item = self.df.iloc[idx, 1]\n",
    "    return text, item\n",
    "\n",
    "def get_prediction(data) :\n",
    "    #Tokenizer building\n",
    "    total_df = pd.read_csv('augmented_data/Dataset_aug_complex_10424_original.csv', sep=',')\n",
    "    total_df.dropna(inplace=True)\n",
    "    total_df = total_df[[\"text\", \"label\"]]\n",
    "    total_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in total_df[\"label\"]]\n",
    "    total_dataset = TestDataset(total_df)\n",
    "    total_loader = DataLoader(total_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    texts = []\n",
    "    for text, _ in total_loader:\n",
    "        texts.append(text[0])\n",
    "        \n",
    "    tokenizer = Tokenizer(nb_words=20000)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    received_text = [data]\n",
    "    valid_lengths = []\n",
    "\n",
    "    for text in received_text:\n",
    "        valid_lengths.append(len(text.split(' '))) \n",
    "    sequences_test = tokenizer.texts_to_sequences(received_text)\n",
    "    sequences_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "    model = keras.models.load_model('./model_save')\n",
    "    valid_lengths = np.array(valid_lengths)\n",
    "    get_fil_tgt = K.function([model.layers[0].input, model.layers[3].input],\n",
    "                                    [model.layers[4].output, model.layers[5].output])\n",
    "\n",
    "    x = [sequences_test, valid_lengths]\n",
    "    [_, filter_index], preds = get_fil_tgt(x)\n",
    "    result = preds.argmax(axis = -1)\n",
    "    return result, filter_index\n",
    "t = 'fuck china'\n",
    "print(get_prediction(t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('cwwojin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c834dc66de8df9eb0f15badab6bbcafe38fec2b208ae0e4e6389e655cd2c3a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
