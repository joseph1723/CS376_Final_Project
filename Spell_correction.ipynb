{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() :\n",
    "  print(\"CUDA\")\n",
    "  device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available() :\n",
    "#   print(\"M1-mps\")\n",
    "#   device = torch.device(\"mps\")\n",
    "else :\n",
    "  print(\"CPU\")\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset) :\n",
    "  #Dataset - English/typo-added/labeled\n",
    "  def __init__(self, df) :\n",
    "    self.df = df\n",
    "  \n",
    "  def __len__(self) :\n",
    "    return len(self.df)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    text = self.df.iloc[idx, 0]\n",
    "    label = self.df.iloc[idx, 1]\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rate, test_rate = 0.9, 0.09\n",
    "itr = 1\n",
    "p_itr = 100\n",
    "epochs = 5\n",
    "batch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  label\n",
      "0      Capitalism is so focrin9 8hitty, there is noth...      1\n",
      "1      any normal person with a brain would agree tha...      1\n",
      "2      I hate the mentally Eetard4e and hope they die...      1\n",
      "3            go my life, I ran into trannies at the pub!      1\n",
      "4      Of You're a foreign student who doesn't have t...      1\n",
      "...                                                  ...    ...\n",
      "10419         keep calling muslims t0we1head8, they are!      0\n",
      "10420                    so happy I wasn't born r3t@rded      1\n",
      "10421          foreigners have ruined our!udzing economy      1\n",
      "10422  calling them an huc!iMg ugly dirty 7igZer is a...      1\n",
      "10423     my cat loves to spoon, you Pit!heX could never      1\n",
      "\n",
      "[10424 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "total_df = pd.read_csv('augmented_data/Dataset_aug_complex_10424_.csv', sep=',')\n",
    "#total_df = pd.read_csv('augmented_data/Dataset_aug_complex_10424_original.csv', sep=',')\n",
    "\n",
    "total_df.dropna(inplace=True)\n",
    "total_df = total_df[[\"text\", \"label\"]]\n",
    "total_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in total_df[\"label\"]]\n",
    "print(total_df)\n",
    "total_dataset = TestDataset(total_df)\n",
    "total_loader = DataLoader(total_dataset, batch_size=batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938 9381\n"
     ]
    }
   ],
   "source": [
    "test_df, train_df, _ = np.split(total_df, [int(test_rate*len(total_df)), int(test_rate*len(total_df) + train_rate*len(total_df))])\n",
    "print(len(test_df), len(train_df))\n",
    "train_dataset = TestDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "test_dataset = TestDataset(test_df)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of SpellChecker-Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence -> word-tokenize (whitespace-tokenize) -> check out-of-vocabulary for each word\n",
    "#if word is out-of-vocabulary : collect the words (from each sentence)\n",
    "#pass the words through the correction model as 1 batch -> get list of output words\n",
    "#replace the original words with output words\n",
    "import nltk, re, string\n",
    "from nltk.tokenize import WhitespaceTokenizer, TreebankWordTokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from random import sample\n",
    "\n",
    "VOCAB_SIZE = 128 + 2\n",
    "EOS, SOS, PAD = 128, 129, 0\n",
    "\n",
    "def preprocess_words(texts, max_len, is_source=True) :\n",
    "    #max_len = max([len(t) for t in texts])\n",
    "    out = []\n",
    "    for text in texts :\n",
    "        encoded = [ord(c) for c in text] + [EOS] + [PAD] * (max_len - len(text))\n",
    "        if not is_source :\n",
    "            encoded = [SOS] + encoded\n",
    "        out.append(torch.tensor(encoded))\n",
    "    return out  #, max_len\n",
    "\n",
    "def tensor_to_str(t) :\n",
    "  #tensor t : (N,Max_len)\n",
    "  N = t.shape[0]\n",
    "  str_list = []\n",
    "  for i in range(N) :\n",
    "    row = t[i,:]\n",
    "    decoded = \"\".join([chr(c) for c in row if c in range(1,128)])\n",
    "    str_list.append(decoded)\n",
    "  return str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcheck(words, model, device, model_type=\"lstm\") :\n",
    "    #input : list of strings(words)\n",
    "\n",
    "    #Get model\n",
    "    model = model.to(device)\n",
    "\n",
    "    #Preprocess input\n",
    "    max_len = max([len(i) for i in words])\n",
    "    src = preprocess_words(words, max_len)\n",
    "\n",
    "    src_len = torch.tensor([t.shape[0] for t in src], dtype=torch.int64)\n",
    "    src = torch.cat([torch.unsqueeze(s, dim=0) for s in src], dim=0).to(device)\n",
    "\n",
    "    #Do the prediction & print\n",
    "    if model_type==\"lstm\" :\n",
    "        prediction = tensor_to_str(model.predict(src, src_len))\n",
    "    elif model_type==\"rnn\" :\n",
    "        prediction = tensor_to_str(model(src, src_len)[0])\n",
    "    else :\n",
    "        prediction = None\n",
    "    return prediction\n",
    "\n",
    "def spell_correction(text, tokenizer, vocab, model, model_type=\"lstm\") :\n",
    "    tokenized_words = tokenizer.tokenize(text)\n",
    "    #new_words = []\n",
    "    new_text = text\n",
    "    re_punkt = re.compile(\"[\" + string.punctuation + \"]+\")\n",
    "    \n",
    "    out_of_vocab_words = [word for word in tokenized_words if (word.lower() not in vocab) and not re_punkt.fullmatch(word)]\n",
    "    #pred_words = [word.lower() for word in out_of_vocab_words]\n",
    "    pred_words = spellcheck(out_of_vocab_words, model, device, model_type=model_type)\n",
    "\n",
    "    for (word, new_word) in zip(out_of_vocab_words, pred_words) :\n",
    "        new_text = text.replace(word, new_word)\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN & LSTM Model Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_length):\n",
    "  \"\"\"\n",
    "  inputs:\n",
    "    X: 3-D tensor\n",
    "    valid_length: 1-D or 2-D tensor\n",
    "  \"\"\"\n",
    "  mask_value = -1e7 \n",
    "\n",
    "  if len(X.shape) == 2:\n",
    "    X = X.unsqueeze(1)\n",
    "\n",
    "  N, n, m = X.shape\n",
    "\n",
    "  if len(valid_length.shape) == 1:\n",
    "    valid_length = valid_length.repeat_interleave(n, dim=0)\n",
    "  else:\n",
    "    valid_length = valid_length.reshape((-1,))\n",
    "\n",
    "  mask = torch.arange(m)[None, :].to(X.device) >= valid_length[:, None]\n",
    "  X.view(-1, m)[mask] = mask_value\n",
    "\n",
    "  Y = torch.softmax(X, dim=-1)\n",
    "  return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module): \n",
    "  def __init__(self):\n",
    "      super(DotProductAttention, self).__init__()\n",
    "\n",
    "  def forward(self, query, key, value, valid_length=None):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "      query: tensor of size (B, n, d)\n",
    "      key: tensor of size (B, m, d)\n",
    "      value: tensor of size (B, m, dim_v)\n",
    "      valid_length: (B, )\n",
    "    \"\"\"\n",
    "\n",
    "    B, n, d = query.shape\n",
    "    d_sqrt = torch.sqrt(query.new_tensor([d]))\n",
    "    a = torch.div(torch.bmm(query, torch.transpose(key,1,2)), d_sqrt)\n",
    "\n",
    "    b = masked_softmax(a, valid_length)\n",
    "    #print(value.shape, value)\n",
    "    #rint(b.shape, b)\n",
    "\n",
    "    attention = torch.bmm(b, value)\n",
    "\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_size, device=None):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.enc = nn.LSTM(embedding_dim, hidden_size, batch_first=True, bidirectional=False)  #input_size, hidden_size, num_layers, bias, batch_first(TRUE -> (B,MAX_LEN,emb_dim)), dropout, bi-directional\n",
    "    self.hidden_size = hidden_size\n",
    "    \n",
    "  def forward(self, sources, valid_len):\n",
    "    #(B,Max_len)\n",
    "    #print(sources)\n",
    "    word_embedded = self.embedding(sources)\n",
    "    packed_input = pack_padded_sequence(word_embedded, valid_len, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    N = word_embedded.shape[0]  #(N, Max_len, emb_dim)\n",
    "    max_len = word_embedded.shape[1]\n",
    "    \n",
    "    #(D*num_layers), N, H_out / D=2(bi-directional), num_layers=1, N=batch_size, H_out=hidden_size \n",
    "    h = sources.new_zeros(1, N, self.hidden_size).float()\n",
    "    c = sources.new_zeros(1, N, self.hidden_size).float()\n",
    "\n",
    "    #output_size : (N, L, D*H_out) when batch_first=True\n",
    "    outputs, (h, c) = self.enc(packed_input, (h, c))\n",
    "    packed_output, _ = pad_packed_sequence(outputs, padding_value= 0, batch_first=True, total_length=max_len)\n",
    "\n",
    "    return packed_output, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.ma.core import nonzero\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.enc = nn.LSTM(embedding_dim+hidden_size, hidden_size, batch_first=True, bidirectional=False)\n",
    "    self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
    "    self.att = DotProductAttention()\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.hidden_size = hidden_size\n",
    "    \n",
    "  def forward(self, state, target, valid_len):\n",
    "    loss = 0\n",
    "    preds = []\n",
    "    enc_output, (h, c), src_len = state\n",
    "    enc_output = enc_output.to(device)\n",
    "\n",
    "    #print(target)\n",
    "    target_embedded = self.embedding(target)\n",
    "    N, max_len = target_embedded.shape[:2]  #T : MAX sequence-length\n",
    "\n",
    "    dec_output = enc_output.new_zeros(N,max_len,self.hidden_size).to(device)\n",
    "    for i in range(max_len) :\n",
    "      context = self.att(h.transpose(0,1), enc_output, enc_output, valid_length=src_len.to(device))\n",
    "\n",
    "      dec_input = torch.cat((target_embedded[:,i,:].reshape(N,1,-1), context), dim=2)\n",
    "      dec_words, (h, c) = self.enc(dec_input, (h, c))    #dec_words : (N,1,hidden_size)\n",
    "      dec_output[:,i,:] = dec_words.reshape(N,self.hidden_size)   #(N,T,hidden_size)\n",
    "\n",
    "    preds = self.output_emb(dec_output)   #preds : (N,Max_len,vocab_size)\n",
    "\n",
    "    loss = F.nll_loss(F.log_softmax(preds[:, :max_len-1].transpose(1,2), dim = 1), target[:, 1:], ignore_index=0, reduction = 'none')\n",
    "    loss = loss.sum(1).mean()\n",
    "\n",
    "    preds = preds.argmax(dim=-1)\n",
    "    # END OF YOUR CODE\n",
    "    return loss, preds\n",
    "  \n",
    "  def predict(self, state, target=None, valid_len=None):\n",
    "    pred = None\n",
    "    enc_output, (h, c), src_len = state\n",
    "    enc_output = enc_output.to(device)\n",
    "    N, max_len = enc_output.shape[:2]  #T : MAX sequence-length\n",
    "\n",
    "    # if target is not None :\n",
    "    #   target_embedded = self.embedding(target)\n",
    "    #   pred_prev = target_embedded[:, :1].reshape(N,1,-1)  #(N,1,embedding_dim)\n",
    "    # else :\n",
    "    #   pred_prev = self.embedding(torch.full((N,1,1),fill_value=SOS).to(device)).reshape(N,1,-1)\n",
    "\n",
    "    preds = []\n",
    "    pred_prev = self.embedding(torch.full((N,1,1),fill_value=SOS).to(device)).reshape(N,1,-1)\n",
    "\n",
    "    for i in range(max_len) :\n",
    "      context = self.att(h.transpose(0,1), enc_output, enc_output, valid_length=src_len.to(device))\n",
    "      dec_input = torch.cat((pred_prev, context), dim=2)\n",
    "      dec_words, (h, c) = self.enc(dec_input, (h, c))    #dec_words : (N,1,hidden_size)\n",
    "      dec_words_output = self.output_emb(dec_words.to(device)).argmax(dim=-1)   #(hidden_size -> vocab_size)\n",
    "      preds.append(dec_words_output)\n",
    "      pred_prev = self.embedding(dec_words_output)    #(vocab_size -> emb_dim)\n",
    "    \n",
    "    pred = torch.cat(preds, dim=1).to(device)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTLSTM(nn.Module):\n",
    "  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size, device):\n",
    "    super(NMTLSTM, self).__init__()\n",
    "    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size, device)\n",
    "    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size, device)\n",
    "    \n",
    "  def forward(self, src, src_len, tgt, tgt_len):\n",
    "    outputs, (h, c) = self.enc(src, src_len)\n",
    "    loss, pred = self.dec((outputs, (h, c), src_len), tgt, tgt_len)\n",
    "    return loss, pred\n",
    "  \n",
    "  def predict(self, src, src_len, tgt=None, tgt_len=None):\n",
    "    outputs, (h, c) = self.enc(src, src_len)\n",
    "    pred = self.dec.predict((outputs, (h, c), src_len), tgt, tgt_len)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRNN(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_size, device=None):\n",
    "    super(BaseRNN, self).__init__()\n",
    "    self.num_layers = 1\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.enc = nn.RNN(embedding_dim, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=True)  #input_size, hidden_size, num_layers, bias, batch_first(TRUE -> (B,MAX_LEN,emb_dim)), dropout, bi-directional\n",
    "    self.ln1 = nn.Linear(2*hidden_size, vocab_size)\n",
    "\n",
    "    self.hidden_size = hidden_size\n",
    "    self.embedding_dim = embedding_dim\n",
    "    \n",
    "  def forward(self, sources, valid_len):\n",
    "    #(B,Max_len)\n",
    "    word_embedded = self.embedding(sources)\n",
    "    packed_input = pack_padded_sequence(word_embedded, valid_len, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    N = word_embedded.shape[0]  #(N, Max_len, emb_dim)\n",
    "    max_len = word_embedded.shape[1]\n",
    "    \n",
    "    #(D*num_layers), N, H_out / D=2(bi-directional), num_layers=1, N=batch_size, H_out=hidden_size \n",
    "    h = sources.new_zeros(2*self.num_layers, N, self.hidden_size).float()\n",
    "    #c = sources.new_zeros(2*self.num_layers, N, self.hidden_size).float()\n",
    "\n",
    "    #output_size : (N, L, D*H_out) when batch_first=True\n",
    "    outputs, h = self.enc(packed_input, h)\n",
    "    packed_output, _ = pad_packed_sequence(outputs, padding_value= 0, batch_first=True, total_length=max_len)\n",
    "\n",
    "    #linear_output : (N, L, Vocab_size)\n",
    "    lin_output = self.ln1(packed_output)\n",
    "\n",
    "    #Final_Output : (N, L, 1)\n",
    "    preds = lin_output.argmax(dim=-1)\n",
    "    return preds, lin_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams\n",
    "\n",
    "1. LSTM w/ Attention <br>\n",
    "lr = 5e-4 <br>\n",
    "embedding_dim = 512 <br>\n",
    "hidden_size = 512 <br>\n",
    "epochs = 10 <br>\n",
    "batch = 10 <br>\n",
    "\n",
    "2. RNN(Baseline) <br>\n",
    "lr = 5e-4 <br>\n",
    "embedding_dim = 512 <br>\n",
    "hidden_size = 512 <br>\n",
    "epochs = 10 <br>\n",
    "batch = 10 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnetdict = wn.words(lang='eng')\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "model_path = \"models/\"\n",
    "lstm_name = \"spelling_lstm.model\"\n",
    "rnn_name = \"spelling_base_rnn.model\"\n",
    "\n",
    "model = torch.load(model_path + rnn_name, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "[True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "#Do THIS at the preprocessing-part of training loop of the classification model\n",
    "for text, label in test_loader :\n",
    "    text_corrected = [spell_correction(t, tokenizer, wordnetdict, model, model_type=\"rnn\") for t in text]\n",
    "    print([len(t.split(\" \")) == len(tc.split(\" \")) for t, tc in zip(text, text_corrected)])\n",
    "    #print(text_corrected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('cwwojin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c834dc66de8df9eb0f15badab6bbcafe38fec2b208ae0e4e6389e655cd2c3a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
