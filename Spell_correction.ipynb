{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() :\n",
    "  print(\"CUDA\")\n",
    "  device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available() :\n",
    "#   print(\"M1-mps\")\n",
    "#   device = torch.device(\"mps\")\n",
    "else :\n",
    "  print(\"CPU\")\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset) :\n",
    "  #Dataset - English/typo-added/labeled\n",
    "  def __init__(self, df) :\n",
    "    self.df = df\n",
    "  \n",
    "  def __len__(self) :\n",
    "    return len(self.df)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    text = self.df.iloc[idx, 0]\n",
    "    label = self.df.iloc[idx, 1]\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rate, test_rate = 0.9, 0.09\n",
    "itr = 1\n",
    "p_itr = 100\n",
    "epochs = 5\n",
    "batch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.read_csv('augmented_data/Dataset_aug_complex_10424_.csv', sep=',')\n",
    "#total_df = pd.read_csv('augmented_data/Dataset_aug_complex_10424_original.csv', sep=',')\n",
    "\n",
    "total_df.dropna(inplace=True)\n",
    "total_df = total_df[[\"text\", \"label\"]]\n",
    "total_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in total_df[\"label\"]]\n",
    "print(total_df)\n",
    "total_dataset = TestDataset(total_df)\n",
    "total_loader = DataLoader(total_dataset, batch_size=batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, train_df, _ = np.split(total_df, [int(test_rate*len(total_df)), int(test_rate*len(total_df) + train_rate*len(total_df))])\n",
    "print(len(test_df), len(train_df))\n",
    "train_dataset = TestDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "test_dataset = TestDataset(test_df)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of SpellChecker-Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence -> word-tokenize (whitespace-tokenize) -> check out-of-vocabulary for each word\n",
    "#if word is out-of-vocabulary : collect the words (from each sentence)\n",
    "#pass the words through the correction model as 1 batch -> get list of output words\n",
    "#replace the original words with output words\n",
    "import nltk, re, string\n",
    "from nltk.tokenize import WhitespaceTokenizer, TreebankWordTokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from random import sample\n",
    "\n",
    "VOCAB_SIZE = 128 + 2\n",
    "EOS, SOS, PAD = 128, 129, 0\n",
    "\n",
    "def preprocess_words(texts, max_len, is_source=True) :\n",
    "    #max_len = max([len(t) for t in texts])\n",
    "    out = []\n",
    "    for text in texts :\n",
    "        encoded = [ord(c) for c in text] + [EOS] + [PAD] * (max_len - len(text))\n",
    "        if not is_source :\n",
    "            encoded = [SOS] + encoded\n",
    "        out.append(torch.tensor(encoded))\n",
    "    return out  #, max_len\n",
    "\n",
    "def tensor_to_str(t) :\n",
    "  #tensor t : (N,Max_len)\n",
    "  N = t.shape[0]\n",
    "  str_list = []\n",
    "  for i in range(N) :\n",
    "    row = t[i,:]\n",
    "    decoded = \"\".join([chr(c) for c in row if c in range(1,128)])\n",
    "    str_list.append(decoded)\n",
    "  return str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcheck(words, model, device, model_type=\"lstm\") :\n",
    "    #input : list of strings(words)\n",
    "\n",
    "    #Get model\n",
    "    model = model.to(device)\n",
    "\n",
    "    #Preprocess input\n",
    "    max_len = max([len(i) for i in words])\n",
    "    src = preprocess_words(words, max_len)\n",
    "\n",
    "    src_len = torch.tensor([t.shape[0] for t in src], dtype=torch.int64)\n",
    "    src = torch.cat([torch.unsqueeze(s, dim=0) for s in src], dim=0).to(device)\n",
    "\n",
    "    #Do the prediction & print\n",
    "    if model_type==\"lstm\" :\n",
    "        prediction = tensor_to_str(model.predict(src, src_len))\n",
    "    elif model_type==\"rnn\" :\n",
    "        prediction = tensor_to_str(model(src, src_len)[0])\n",
    "    else :\n",
    "        prediction = None\n",
    "    print(prediction)\n",
    "    return prediction\n",
    "\n",
    "def spell_correction(text, tokenizer, vocab, model) :\n",
    "    tokenized_words = tokenizer.tokenize(text)\n",
    "    new_words = []\n",
    "    new_text = text\n",
    "    re_punkt = re.compile(\"[\" + string.punctuation + \"]+\")\n",
    "    \n",
    "    out_of_vocab_words = [word for word in tokenized_words if (word.lower() not in vocab) and not re_punkt.fullmatch(word)]\n",
    "    #pred_words = [word.lower() for word in out_of_vocab_words]\n",
    "    pred_words = spellcheck(out_of_vocab_words, model, device)\n",
    "\n",
    "    for (word, new_word) in zip(out_of_vocab_words, pred_words) :\n",
    "        print(word)\n",
    "        new_text = text.replace(word, new_word)\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams\n",
    "\n",
    "1. LSTM w/ Attention <br>\n",
    "lr = 5e-4 <br>\n",
    "embedding_dim = 512 <br>\n",
    "hidden_size = 512 <br>\n",
    "epochs = 10 <br>\n",
    "batch = 10 <br>\n",
    "\n",
    "2. RNN(Baseline) <br>\n",
    "lr = 5e-4 <br>\n",
    "embedding_dim = 512 <br>\n",
    "hidden_size = 512 <br>\n",
    "epochs = 10 <br>\n",
    "batch = 10 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnetdict = wn.words(lang='eng')\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "model_path = \"models/\"\n",
    "lstm_name = \"spelling_lstm.model\"\n",
    "rnn_name = \"spelling_base.model\"\n",
    "\n",
    "model = torch.load(model_path + lstm_name)\n",
    "\n",
    "\n",
    "#Do THIS at the preprocessing-part of training loop of the classification model\n",
    "for text, label in test_loader :\n",
    "    text_corrected = [spell_correction(t, tokenizer, wordnetdict, model) for t in text]\n",
    "    print(text)\n",
    "    print(text_corrected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('cwwojin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c834dc66de8df9eb0f15badab6bbcafe38fec2b208ae0e4e6389e655cd2c3a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
