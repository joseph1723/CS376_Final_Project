{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cwwojin/opt/anaconda3/envs/cwwojin/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() :\n",
    "  print(\"CUDA\")\n",
    "  device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available() :\n",
    "#   print(\"M1-mps\")\n",
    "#   device = torch.device(\"mps\")\n",
    "else :\n",
    "  print(\"CPU\")\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset) :\n",
    "  #Dataset - English/typo-added/labeled\n",
    "  def __init__(self, df) :\n",
    "    self.df = df\n",
    "  \n",
    "  def __len__(self) :\n",
    "    return len(self.df)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    text = self.df.iloc[idx, 0]\n",
    "    label = self.df.iloc[idx, 1]\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rate, test_rate = 0.9, 0.09\n",
    "itr = 1\n",
    "p_itr = 100\n",
    "epochs = 5\n",
    "batch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  label\n",
      "0      just called this hotch a cum dumpster on the t...      0\n",
      "1      This dress looks fudk7nf bad on me and I look ...      1\n",
      "2      If they people don't react well, to you coming...      1\n",
      "3      \"There is no correlation between black populat...      0\n",
      "4                F6sk yeah, I would kill for some fries.      1\n",
      "...                                                  ...    ...\n",
      "10419  would you like to be called brown? Then stop u...      1\n",
      "10420  i hate being a lesbn#1 living in this country,...      1\n",
      "10421  The curfew you wanted but now you don't, make ...      1\n",
      "10422  I love how HE has to come on to this forum, ch...      0\n",
      "10423  would you like to be called a q7eeT? Then stop...      1\n",
      "\n",
      "[10424 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "total_df = pd.read_csv('augmented_data/Dataset_aug_complex_10424_.csv', sep=',')\n",
    "#total_df = pd.read_csv('augmented_data/Dataset_aug_complex_10424_original.csv', sep=',')\n",
    "\n",
    "total_df.dropna(inplace=True)\n",
    "total_df = total_df[[\"text\", \"label\"]]\n",
    "total_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in total_df[\"label\"]]\n",
    "print(total_df)\n",
    "total_dataset = TestDataset(total_df)\n",
    "total_loader = DataLoader(total_dataset, batch_size=batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938 9381\n"
     ]
    }
   ],
   "source": [
    "test_df, train_df, _ = np.split(total_df, [int(test_rate*len(total_df)), int(test_rate*len(total_df) + train_rate*len(total_df))])\n",
    "print(len(test_df), len(train_df))\n",
    "train_dataset = TestDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "test_dataset = TestDataset(test_df)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of SpellChecker-Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams\n",
    "\n",
    "1. LSTM (Single-Direction) w/ Attention <br>\n",
    "lr = 5e-4 <br>\n",
    "embedding_dim = 512 <br>\n",
    "hidden_size = 512 <br>\n",
    "epochs = 20 <br>\n",
    "batch = 10 <br>\n",
    "\n",
    "2. RNN (Bi-Direction) <br>\n",
    "lr = 5e-4 <br>\n",
    "embedding_dim = 512 <br>\n",
    "hidden_size = 512 <br>\n",
    "epochs = 20 <br>\n",
    "batch = 10 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Spell_correction_model import *\n",
    "\n",
    "wordnetdict = wn.words(lang='eng')\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "model_path = \"models/\"\n",
    "lstm_name = \"spelling_lstm.model\"\n",
    "rnn_name = \"spelling_base_rnn.model\"\n",
    "\n",
    "model = torch.load(model_path + rnn_name, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do THIS at the preprocessing-part of training loop of the classification model\n",
    "# corrected_texts = []\n",
    "# for text, label in train_loader :\n",
    "#     text_ascii = [\"\".join([c for c in t if ascii_range(c)]) for t in text]\n",
    "#     text_corrected = [spell_correction(t, tokenizer, wordnetdict, model, device=device, model_type=\"rnn\") for t in text_ascii]\n",
    "#     corrected_texts += text_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i : 0 / len : 1\n",
      "i : 1000 / len : 1001\n",
      "i : 2000 / len : 2001\n",
      "i : 3000 / len : 3001\n",
      "i : 4000 / len : 4001\n",
      "i : 5000 / len : 5001\n",
      "i : 6000 / len : 6001\n",
      "i : 7000 / len : 7001\n",
      "i : 8000 / len : 8001\n",
      "i : 9000 / len : 9001\n",
      "i : 10000 / len : 10001\n"
     ]
    }
   ],
   "source": [
    "corrected_texts = []\n",
    "i=0\n",
    "interval=1000\n",
    "for text in total_df['text'] :\n",
    "    text_ascii = \"\".join([c for c in text if ascii_range(c)])\n",
    "    text_corrected = spell_correction(text_ascii, tokenizer, wordnetdict, model, device=device, model_type=\"rnn\")\n",
    "    corrected_texts.append(text_corrected)\n",
    "    if i%interval==0 : print(\"i : {} / len : {}\".format(i, len(corrected_texts)))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(corrected_texts)==len(total_df))\n",
    "total_df['text_corrected'] = corrected_texts\n",
    "total_df.to_csv(\"augmented_data/Dataset_aug_complex_{}_spellcheck_rnn.csv\".format(len(total_df)),sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('cwwojin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c834dc66de8df9eb0f15badab6bbcafe38fec2b208ae0e4e6389e655cd2c3a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
