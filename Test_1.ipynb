{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Test_1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Test #1. Pre-Trained BERT Classifier / Misspelled Text"],"metadata":{"id":"KIR1zP0dvAOa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"87QI-OOVt9nk","executionInfo":{"status":"ok","timestamp":1654610002162,"user_tz":-540,"elapsed":17637,"user":{"displayName":"Yong Bin kwon","userId":"14294643249978865196"}},"outputId":"999f9688-2dea-40a0-ff2b-7b2756e7438e","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-transformers\n","  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n","\u001b[K     |████████████████████████████████| 176 kB 9.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.21.6)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2019.12.20)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.11.0+cu113)\n","Collecting boto3\n","  Downloading boto3-1.24.3-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 17.6 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 71.2 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 21.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.64.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (4.2.0)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 9.0 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n","Collecting botocore<1.28.0,>=1.27.3\n","  Downloading botocore-1.27.3-py3-none-any.whl (8.9 MB)\n","\u001b[K     |████████████████████████████████| 8.9 MB 43.5 MB/s \n","\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 52.1 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.3->boto3->pytorch-transformers) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.3->boto3->pytorch-transformers) (1.15.0)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 16.4 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2022.5.18.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=82af97eec5572dfee4ae5852e656947ca1e38090bcbbd26d1bcdbe0b9aa84563\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, pytorch-transformers\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.24.3 botocore-1.27.3 jmespath-1.0.0 pytorch-transformers-1.2.0 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.96 urllib3-1.25.11\n"]}],"source":["!pip install pytorch-transformers"]},{"cell_type":"code","source":["!pip install nlpaug"],"metadata":{"id":"Z0wbEXwRxKrN","outputId":"e7e5e634-b0ea-49f3-e903-64919763e5c2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654610005251,"user_tz":-540,"elapsed":3093,"user":{"displayName":"Yong Bin kwon","userId":"14294643249978865196"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting nlpaug\n","  Downloading nlpaug-1.1.10-py3-none-any.whl (410 kB)\n","\u001b[K     |████████████████████████████████| 410 kB 30.7 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.3.5)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (2.23.0)\n","Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.21.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->nlpaug) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (2022.5.18.1)\n","Installing collected packages: nlpaug\n","Successfully installed nlpaug-1.1.10\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/joseph1723/CS376_Final_Project.git"],"metadata":{"id":"CJ4koSJ1xbLr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654610007891,"user_tz":-540,"elapsed":2643,"user":{"displayName":"Yong Bin kwon","userId":"14294643249978865196"}},"outputId":"de3446ec-b0f5-478a-cc54-fe4d78cf34ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'CS376_Final_Project'...\n","remote: Enumerating objects: 174, done.\u001b[K\n","remote: Counting objects: 100% (174/174), done.\u001b[K\n","remote: Compressing objects: 100% (120/120), done.\u001b[K\n","remote: Total 174 (delta 67), reused 138 (delta 40), pack-reused 0\u001b[K\n","Receiving objects: 100% (174/174), 11.88 MiB | 16.35 MiB/s, done.\n","Resolving deltas: 100% (67/67), done.\n"]}]},{"cell_type":"code","source":["import nlpaug\n","import nlpaug.augmenter.char as nac\n","import nlpaug.augmenter.word as naw\n","import nlpaug.augmenter.word.context_word_embs as nawcwe\n","import nlpaug.augmenter.word.word_embs as nawwe\n","import nlpaug.augmenter.word.spelling as naws\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from pytorch_transformers import BertTokenizer, BertForSequenceClassification, BertConfig, BertModel\n","from torch.optim import Adam\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import math\n","from keras.preprocessing.sequence import pad_sequences\n"],"metadata":{"id":"YHWRrOYpuyPV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","# model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n","bert = BertModel.from_pretrained(\"bert-base-uncased\")\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","if torch.cuda.is_available() :\n","  print(\"GPU\")\n","  device = torch.device(\"cuda\")\n","  # model.to(device)\n","  bert.to(device)\n","else :\n","  print(\"No GPU\")\n","  device = torch.device(\"cpu\")\n","  # model.to(device)\n","  bert.to(device)"],"metadata":{"id":"Htadzmjwu_Dh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"53c110e3-17c8-4db0-b147-0b85867b21e1","executionInfo":{"status":"ok","timestamp":1654610058457,"user_tz":-540,"elapsed":40448,"user":{"displayName":"Yong Bin kwon","userId":"14294643249978865196"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 433/433 [00:00<00:00, 315465.28B/s]\n","100%|██████████| 440473133/440473133 [00:22<00:00, 19469315.77B/s]\n","100%|██████████| 231508/231508 [00:00<00:00, 16462623.64B/s]\n"]},{"output_type":"stream","name":"stdout","text":["GPU\n"]}]},{"cell_type":"code","source":["class TestDataset(Dataset) :\n","  #Dataset - English/typo-added/labeled\n","  def __init__(self, df) :\n","    self.df = df\n","  \n","  def __len__(self) :\n","    return len(self.df)\n","  \n","  def __getitem__(self, idx):\n","    text = self.df.iloc[idx, 0]\n","    label = self.df.iloc[idx, 1]\n","    return text, label"],"metadata":{"id":"CjInRHDwyuUU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hyperparameters"],"metadata":{"id":"0stfCVXzA2yC"}},{"cell_type":"code","source":["train_size = 1000\n","test_size = 100\n","itr = 1\n","p_itr = 100\n","epochs = 5"],"metadata":{"id":"G_vc7qipA2S_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def masked_softmax(X, valid_length):\n","  \n","  mask_value = -1e7 \n","\n","  if len(X.shape) == 2:\n","    X = X.unsqueeze(1)\n","\n","  N, n, m = X.shape\n","\n","  if len(valid_length.shape) == 1:\n","    valid_length = valid_length.repeat_interleave(n, dim=0)\n","  else:\n","    valid_length = valid_length.reshape((-1,))\n","\n","  mask = torch.arange(m)[None, :].to(X.device) >= valid_length[:, None]\n","  X.view(-1, m)[mask] = mask_value\n","\n","  Y = torch.softmax(X, dim=-1)\n","\n","  return Y"],"metadata":{"id":"EuCHOIDNXOFY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def flatten(x, start_dim=1, end_dim=-1):\n","  return x.flatten(start_dim=start_dim, end_dim=end_dim)\n","  \n","class Feed_Forward_network(nn.Module):\n","  def __init__(self, dh, dff):\n","    super(Feed_Forward_network, self).__init__()\n","    # self.first_fc_layer = nn.Linear(dh, dff)\n","    # self.second_fc_layer = nn.Linear(dff, 1)\n","    self.linear = (dh,1)\n","  def forward(self, x, y, valid_length):\n","    out1 = self.linear(x)\n","    out2 = F.relu(out1)\n","    # out3 = self.second_fc_layer(out2)\n","    # out_soft = F.softmax(out2.T, dim=-1)\n","    out_soft = masked_softmax(out2.T, valid_length)\n","    out4 = x.mul(out_soft.T\n","    out = nn.Sigmoid(torch.sum(out4))\n","    # out_fil = F.softmax(out4, dim = -1)\n","    out_fil = masked_softmax(torch.sum(out, axis = -1), valid_length)\n","    result = 0\n","    if out>0.5: result = 1\n","    fil_tgt = out_fil > 0.5\n","    loss = F.nll_loss(out, y, ignore_index = 0, reduction = 'none')\n","    loss = loss.sum(1).mean()\n","    return result, fil_tgt, loss\n","\n","class BiClassBert(nn.Module):\n","    def __init__(self, bert):\n","        super().__init__()\n","        self.bert = bert\n","        embedding_dim = bert.config.to_dict()['hidden_size']\n","        self.ffn = Feed_Forward_network(embedding_dim, 1)\n","    def forward(self, input, attention_mask, valid_len):\n","        b_out = self.bert(input, attention_mask=attention_mask)[0]\n","        output, _ = self.fnn(b_out, valid_len)\n","\n","model = BiClassBert(bert)\n","train_rate, test_rate = 0.1, 0.01\n","total_df = pd.read_csv('/content/CS376_Final_Project/augmented_data/Dataset_aug_complex_6758_.csv', sep=',')\n","total_df.dropna(inplace=True)\n","total_df = total_df[[\"text\", \"label\"]]\n","total_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in total_df[\"label\"]]\n","print(total_df)\n","total_dataset = TestDataset(total_df)\n","total_loader = DataLoader(total_dataset, batch_size=1, shuffle=True)\n","max_len = 0\n","for text, _ in total_loader:\n","    encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n","    l = max(len(e) for e in encoded_list)\n","    max_len = l if l>max_len else max_len\n","\n","print(max_len)\n","#Train Set\n","test_df, train_df, _ = np.split(total_df, [int(test_rate*len(total_df)), int(test_rate*len(total_df) + train_rate*len(total_df))])\n","print(len(test_df), len(train_df))\n","train_dataset = TestDataset(train_df)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_dataset = TestDataset(test_df)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3AaP5n2IMsMg","executionInfo":{"status":"ok","timestamp":1654610428872,"user_tz":-540,"elapsed":25903,"user":{"displayName":"Yong Bin kwon","userId":"14294643249978865196"}},"outputId":"e994ee86-13d6-4eb8-8cdc-beaf8ef19c9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                    text  label\n","0      my sister is being a tranny again      0\n","1      my sister is being a Tranny again      0\n","2      my sister is being a Tranny again      0\n","3      my siQtef is being a traGmy again      0\n","4      my si8tek is being a tkanny again      0\n","...                                  ...    ...\n","40543        you are such a fat cuntits!      1\n","40544          you are such a fat cuunt!      1\n","40545           you are such a fat D^nt!      1\n","40546           you are such a fat cont!      1\n","40547           you are such a fat cHnU!      1\n","\n","[40548 rows x 2 columns]\n","499\n","405 4055\n"]}]},{"cell_type":"markdown","source":["Data Augmentation / Generating Typo using nlpaug"],"metadata":{"id":"1UdqKyCWICsZ"}},{"cell_type":"markdown","source":["Test & Evaluation\n","\n","*주의해야하는 것은, 학습 샘플의 인풋이 (batch_size, sequence_length)로 들어간다는 것이다. 따라서 zero-padding을 직접 해줘서 model의 forward에 넣어줘야한다."],"metadata":{"id":"f7_jQsyL0-e-"}},{"cell_type":"code","source":["optimizer = Adam(model.parameters(), lr=1e-6)\n","total_loss = 0\n","total_len = 0\n","total_correct = 0\n","\n","\n","model.train()\n","\n","\n","def padding(input):\n","    input = pad_sequences(input, maxlen=max_len, dtype=\"long\", padding='post')\n","    return input\n","\n","def attention_mask(input):\n","    masked_lst = []\n","    for sentence in input:\n","        masked = [float(emb_word>0) for emb_word in sentence]\n","        masked_lst.append(masked)\n","    return masked_lst\n","\n","for epoch in range(epochs):\n","    for text, label in train_loader:\n","        optimizer.zero_grad()\n","        # encoding and zero padding\n","        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n","        print(tokenizer['hi'])\n","        valid_len = torch.tensor([len(input) for input in encoded_list])\n","        padded_list =  padding(encoded_list)\n","        attention_mask = torch.tensor(attention_mask(padded_list))\n","        data = torch.tensor(padded_list)\n","        data, label, attention_mask, valid_len = data.to(device), label.to(device), attention_mask.to(device), valid_len.to(device)\n","        labels = torch.tensor(label)\n","        outputs, _ = model(data, attention_mask, valid_len)\n","        print(outputs)\n","        loss, logits = outputs\n","        pred = torch.argmax(F.softmax(logits), dim=1)\n","        correct = pred.eq(labels)\n","        total_correct += correct.sum().item()\n","        total_len += len(labels)\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","        \n","        \n","        if itr % p_itr == 0:\n","            print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n","            total_loss = 0\n","            total_len = 0\n","            total_correct = 0\n","        itr+=1\n","\n","\n"],"metadata":{"id":"DkwtjS28yMRF","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"error","timestamp":1654611784652,"user_tz":-540,"elapsed":13,"user":{"displayName":"Yong Bin kwon","userId":"14294643249978865196"}},"outputId":"27c77814-6cb6-46f2-959e-af3e7758c324"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-cb24b950fbc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# encoding and zero padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mencoded_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mvalid_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mpadded_list\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'BertTokenizer' object is not subscriptable"]}]},{"cell_type":"code","source":["model.eval()\n","\n","total_loss = 0\n","total_len = 0\n","total_correct = 0\n","\n","for text, label in test_loader:\n","    encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n","    MAX_LEN = max(len(e) for e in encoded_list)\n","    padded_list =  [e + [0] * (MAX_LEN-len(e)) for e in encoded_list]\n","    sample = torch.tensor(padded_list)\n","    sample, label = sample.to(device), label.to(device)\n","    labels = torch.tensor(label)\n","    outputs = model(sample, labels=labels)\n","    _, logits = outputs\n","\n","    pred = torch.argmax(F.softmax(logits), dim=1)\n","    correct = pred.eq(labels)\n","    total_correct += correct.sum().item()\n","    total_len += len(labels)\n","\n","print('Test accuracy: ', total_correct / total_len)"],"metadata":{"id":"Daj6lX75AJL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]], maxlen=3, padding='post')\n","a = torch.tensor(pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]], maxlen=3, padding='post'))\n","print(a)\n","def attention_mask(ids):\n","    masks = []\n","    for id in ids:\n","        mask = [float(i>0) for i in id]\n","        masks.append(mask)\n","    return masks\n","res = torch.tensor(attention_mask(a))\n","\n"],"metadata":{"id":"UKgrKIFjLXca"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizerFast \n","\n","tokenizer1 = BertTokenizer.from_pretrained('bert-base-uncased')\n","tokenizer2 = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","sentence = \"Tokyo to report nearly 370 new coronavirus cases, setting new single-day record\"\n","tokens1 = tokenizer1.encode(sentence)\n","print(tokens1)\n","tokens2 = tokenizer2(sentence, add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n","print(tokens2.word_ids())\n","word_ids = tokens2.word_ids()\n","lst = []\n","for i in range(len(word_ids)):\n","  if word_ids[i] not in lst:\n","    lst.append(i)\n","print(lst)\n","encodings = tokenizer2(sentence, return_offsets_mapping=True)\n","for token_id, pos in zip(encodings['input_ids'], encodings['offset_mapping']):\n","    print(token_id, pos, sentence[pos[0]:pos[1]])\n","print(encodings)\n","\n","from transformers import BertTokenizerFast\n","\n","t = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","\n","tokens = t('word embeddings are vectors', add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n","print(tokens)\n","print(tokens.word_ids())\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HYy0-Vl3kmXM","executionInfo":{"status":"ok","timestamp":1654614405982,"user_tz":-540,"elapsed":616,"user":{"displayName":"Yong Bin kwon","userId":"14294643249978865196"}},"outputId":"8b0191ec-8cea-464d-8c06-195941a8e267"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["[5522, 2000, 3189, 3053, 16444, 2047, 21887, 23350, 3572, 1010, 4292, 2047, 2309, 1011, 2154, 2501]\n","[0, 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n","[0, 1, 2, 3, 4, 5, 6, 8, 10, 12, 14]\n","101 (0, 0) \n","5522 (0, 5) Tokyo\n","2000 (6, 8) to\n","3189 (9, 15) report\n","3053 (16, 22) nearly\n","16444 (23, 26) 370\n","2047 (27, 30) new\n","21887 (31, 37) corona\n","23350 (37, 42) virus\n","3572 (43, 48) cases\n","1010 (48, 49) ,\n","4292 (50, 57) setting\n","2047 (58, 61) new\n","2309 (62, 68) single\n","1011 (68, 69) -\n","2154 (69, 72) day\n","2501 (73, 79) record\n","102 (0, 0) \n","{'input_ids': [101, 5522, 2000, 3189, 3053, 16444, 2047, 21887, 23350, 3572, 1010, 4292, 2047, 2309, 1011, 2154, 2501, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 5), (6, 8), (9, 15), (16, 22), (23, 26), (27, 30), (31, 37), (37, 42), (43, 48), (48, 49), (50, 57), (58, 61), (62, 68), (68, 69), (69, 72), (73, 79), (0, 0)]}\n","{'input_ids': [2773, 7861, 8270, 4667, 2015, 2024, 19019]}\n","[0, 1, 1, 1, 1, 2, 3]\n"]}]},{"cell_type":"markdown","source":["TEST RESULTS :\n","\n","No Augmentation - 0.78\n","\n","Augmentation - 0.7"],"metadata":{"id":"RNWth7lmZt7Z"}}]}