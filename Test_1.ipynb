{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Test #1. Pre-Trained BERT Classifier / Misspelled Text"
      ],
      "metadata": {
        "id": "KIR1zP0dvAOa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "87QI-OOVt9nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae477a7-a817-4ba0-a608-2c7d2add95c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (0.1.96)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.21.6)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (0.0.53)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (4.2.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (0.5.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (1.0.0)\n",
            "Requirement already satisfied: botocore<1.27.0,>=1.26.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (1.26.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.1->boto3->pytorch-transformers) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.1->boto3->pytorch-transformers) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.0,>=1.26.1->boto3->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug"
      ],
      "metadata": {
        "id": "Z0wbEXwRxKrN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28f293e1-a6c6-499d-bf02-48eb381b1490"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.7/dist-packages (1.1.10)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (2.23.0)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->nlpaug) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (2021.10.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/joseph1723/CS376_Final_Project.git"
      ],
      "metadata": {
        "id": "CJ4koSJ1xbLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "685e31d0-33d8-4ad7-afb0-f55ae264bd54"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CS376_Final_Project' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.word.context_word_embs as nawcwe\n",
        "import nlpaug.augmenter.word.word_embs as nawwe\n",
        "import nlpaug.augmenter.word.spelling as naws\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "YHWRrOYpuyPV"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n",
        "if torch.cuda.is_available() :\n",
        "  device = torch.device(\"cuda\")\n",
        "  model.to(device)\n",
        "else :\n",
        "  device = torch.device(\"cpu\")\n",
        "  model.to(device)"
      ],
      "metadata": {
        "id": "Htadzmjwu_Dh"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(Dataset) :\n",
        "  #Dataset - English/typo-added/labeled\n",
        "  def __init__(self, df) :\n",
        "    self.df = df\n",
        "  \n",
        "  def __len__(self) :\n",
        "    return len(self.df)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    text = self.df.iloc[idx, 0]\n",
        "    label = self.df.iloc[idx, 1]\n",
        "    return text, label"
      ],
      "metadata": {
        "id": "CjInRHDwyuUU"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "0stfCVXzA2yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 1000\n",
        "test_size = 100\n",
        "itr = 1\n",
        "p_itr = 100\n",
        "epochs = 1"
      ],
      "metadata": {
        "id": "G_vc7qipA2S_"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_df = pd.read_csv('/content/CS376_Final_Project/2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.1.csv', sep=',')\n",
        "total_df.dropna(inplace=True)\n",
        "\n",
        "#Train Set - 50\n",
        "train_df = total_df.sample(train_size, random_state=999)\n",
        "train_df = train_df[[\"text\", \"label\"]]\n",
        "train_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in train_df[\"label\"]]\n",
        "\n",
        "#Test Set - 10\n",
        "test_df = total_df.sample(test_size, random_state=999)\n",
        "test_df = test_df[[\"text\", \"label\"]]\n",
        "test_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in test_df[\"label\"]]\n"
      ],
      "metadata": {
        "id": "Ra3JtiuS1gNS"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TestDataset(train_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = TestDataset(test_df)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "X7cIiujJ5BUl"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation / Generating Typo using nlpaug"
      ],
      "metadata": {
        "id": "1UdqKyCWICsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_data(aug, dataframe, key, N):\n",
        "  dataframe_copy = dataframe.copy()\n",
        "  rows = []\n",
        "  for i, row in dataframe.iterrows() :\n",
        "    line = row[key]\n",
        "    line_augmented = aug.augment(line)\n",
        "    row.iloc[0] = line_augmented\n",
        "    row_list = list(row)\n",
        "    rows.append(row_list)\n",
        "  rows = pd.DataFrame(rows, columns = ['text', 'label'])\n",
        "  dataframe_copy = dataframe_copy.append(pd.DataFrame(rows), ignore_index=True)\n",
        "  return dataframe_copy"
      ],
      "metadata": {
        "id": "ya9HLYblIcKF"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug = naw.SpellingAug()\n",
        "n = 1\n",
        "key = \"text\"\n",
        "\n",
        "train_aug_df = augment_data(aug, train_df, key, n)\n",
        "test_aug_df = augment_data(aug, test_df, key, n)"
      ],
      "metadata": {
        "id": "M4d1oxmYICia"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TestDataset(train_aug_df)\n",
        "test_dataset = TestDataset(test_aug_df)\n",
        "#train_dataset = TestDataset(train_df)\n",
        "#test_dataset = TestDataset(test_df)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "zcWJmnWcWpXt"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test & Evaluation\n",
        "\n",
        "*주의해야하는 것은, 학습 샘플의 인풋이 (batch_size, sequence_length)로 들어간다는 것이다. 따라서 zero-padding을 직접 해줘서 model의 forward에 넣어줘야한다."
      ],
      "metadata": {
        "id": "f7_jQsyL0-e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-6)\n",
        "\n",
        "total_loss = 0\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    for text, label in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # encoding and zero padding\n",
        "        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "        MAX_LEN = max(len(e) for e in encoded_list)\n",
        "        padded_list =  [e + [0] * (MAX_LEN-len(e)) for e in encoded_list]\n",
        "        \n",
        "        sample = torch.tensor(padded_list)\n",
        "        sample, label = sample.to(device), label.to(device)\n",
        "        labels = torch.tensor(label)\n",
        "        outputs = model(sample, labels=labels)\n",
        "        loss, logits = outputs\n",
        "\n",
        "        pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "        correct = pred.eq(labels)\n",
        "        total_correct += correct.sum().item()\n",
        "        total_len += len(labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if itr % p_itr == 0:\n",
        "            print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
        "            total_loss = 0\n",
        "            total_len = 0\n",
        "            total_correct = 0\n",
        "        itr+=1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DkwtjS28yMRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15bd3ae3-921f-4465-8a4b-50d99829748f"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/1] Iteration 100 -> Train Loss: 0.7016, Accuracy: 0.500\n",
            "[Epoch 1/1] Iteration 200 -> Train Loss: 0.6677, Accuracy: 0.630\n",
            "[Epoch 1/1] Iteration 300 -> Train Loss: 0.6135, Accuracy: 0.710\n",
            "[Epoch 1/1] Iteration 400 -> Train Loss: 0.6781, Accuracy: 0.550\n",
            "[Epoch 1/1] Iteration 500 -> Train Loss: 0.6552, Accuracy: 0.600\n",
            "[Epoch 1/1] Iteration 600 -> Train Loss: 0.6684, Accuracy: 0.580\n",
            "[Epoch 1/1] Iteration 700 -> Train Loss: 0.6541, Accuracy: 0.610\n",
            "[Epoch 1/1] Iteration 800 -> Train Loss: 0.6474, Accuracy: 0.590\n",
            "[Epoch 1/1] Iteration 900 -> Train Loss: 0.6539, Accuracy: 0.610\n",
            "[Epoch 1/1] Iteration 1000 -> Train Loss: 0.6247, Accuracy: 0.640\n",
            "[Epoch 1/1] Iteration 1100 -> Train Loss: 0.6393, Accuracy: 0.630\n",
            "[Epoch 1/1] Iteration 1200 -> Train Loss: 0.6162, Accuracy: 0.660\n",
            "[Epoch 1/1] Iteration 1300 -> Train Loss: 0.6365, Accuracy: 0.620\n",
            "[Epoch 1/1] Iteration 1400 -> Train Loss: 0.6280, Accuracy: 0.650\n",
            "[Epoch 1/1] Iteration 1500 -> Train Loss: 0.5784, Accuracy: 0.690\n",
            "[Epoch 1/1] Iteration 1600 -> Train Loss: 0.6509, Accuracy: 0.620\n",
            "[Epoch 1/1] Iteration 1700 -> Train Loss: 0.5823, Accuracy: 0.720\n",
            "[Epoch 1/1] Iteration 1800 -> Train Loss: 0.6227, Accuracy: 0.650\n",
            "[Epoch 1/1] Iteration 1900 -> Train Loss: 0.6313, Accuracy: 0.650\n",
            "[Epoch 1/1] Iteration 2000 -> Train Loss: 0.5721, Accuracy: 0.760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "total_loss = 0\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "\n",
        "for text, label in test_loader:\n",
        "    encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "    MAX_LEN = max(len(e) for e in encoded_list)\n",
        "    padded_list =  [e + [0] * (MAX_LEN-len(e)) for e in encoded_list]\n",
        "    sample = torch.tensor(padded_list)\n",
        "    sample, label = sample.to(device), label.to(device)\n",
        "    labels = torch.tensor(label)\n",
        "    outputs = model(sample, labels=labels)\n",
        "    _, logits = outputs\n",
        "\n",
        "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "    correct = pred.eq(labels)\n",
        "    total_correct += correct.sum().item()\n",
        "    total_len += len(labels)\n",
        "\n",
        "print('Test accuracy: ', total_correct / total_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Daj6lX75AJL_",
        "outputId": "963541c2-14e4-4db3-b277-048bba3529ca"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy:  0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEST RESULTS :\n",
        "\n",
        "No Augmentation - 0.78\n",
        "\n",
        "Augmentation - 0.7"
      ],
      "metadata": {
        "id": "RNWth7lmZt7Z"
      }
    }
  ]
}