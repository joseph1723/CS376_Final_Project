{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIR1zP0dvAOa"
      },
      "source": [
        "Test #1. Pre-Trained BERT Classifier / Misspelled Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87QI-OOVt9nk",
        "outputId": "fae477a7-a817-4ba0-a608-2c7d2add95c9"
      },
      "outputs": [],
      "source": [
        "#!pip install pytorch-transformers\n",
        "#!pip install nlpaug\n",
        "#!git clone https://github.com/joseph1723/CS376_Final_Project.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YHWRrOYpuyPV"
      },
      "outputs": [],
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Htadzmjwu_Dh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at murali1996/bert-base-cased-spell-correction were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n",
        "model_spelling = BertModel.from_pretrained('murali1996/bert-base-cased-spell-correction')\n",
        "tokenizer_spelling = BertTokenizer.from_pretrained('murali1996/bert-base-cased-spell-correction')\n",
        "if torch.cuda.is_available() :\n",
        "  print(\"CUDA\")\n",
        "  device = torch.device(\"cuda\")\n",
        "  model.to(device)\n",
        "else :\n",
        "  print(\"CPU\")\n",
        "  device = torch.device(\"cpu\")\n",
        "  model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CjInRHDwyuUU"
      },
      "outputs": [],
      "source": [
        "class TestDataset(Dataset) :\n",
        "  #Dataset - English/typo-added/labeled\n",
        "  def __init__(self, df) :\n",
        "    self.df = df\n",
        "  \n",
        "  def __len__(self) :\n",
        "    return len(self.df)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    text = self.df.iloc[idx, 0]\n",
        "    label = self.df.iloc[idx, 1]\n",
        "    return text, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0stfCVXzA2yC"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "G_vc7qipA2S_"
      },
      "outputs": [],
      "source": [
        "train_size = 1000\n",
        "test_size = 100\n",
        "itr = 1\n",
        "p_itr = 100\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Ra3JtiuS1gNS"
      },
      "outputs": [],
      "source": [
        "total_df = pd.read_csv('augmented_data/Dataset_aug_profanity_6758_.csv', sep=',')\n",
        "total_df.dropna(inplace=True)\n",
        "\n",
        "#Train Set\n",
        "train_df = total_df.sample(train_size, random_state=999)\n",
        "train_df = train_df[[\"text\", \"label\"]]\n",
        "train_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in train_df[\"label\"]]\n",
        "\n",
        "#Test Set\n",
        "test_df = total_df.sample(test_size, random_state=999)\n",
        "test_df = test_df[[\"text\", \"label\"]]\n",
        "test_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in test_df[\"label\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "X7cIiujJ5BUl"
      },
      "outputs": [],
      "source": [
        "train_dataset = TestDataset(train_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "test_dataset = TestDataset(test_df)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UdqKyCWICsZ"
      },
      "source": [
        "Data Augmentation / Generating Typo using nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ya9HLYblIcKF"
      },
      "outputs": [],
      "source": [
        "def augment_data(aug, dataframe, key, N):\n",
        "  dataframe_copy = dataframe.copy()\n",
        "  rows = []\n",
        "  for i, row in dataframe.iterrows() :\n",
        "    line = row[key]\n",
        "    line_augmented = aug.augment(line)\n",
        "    row.iloc[0] = line_augmented\n",
        "    row_list = list(row)\n",
        "    rows.append(row_list)\n",
        "  rows = pd.DataFrame(rows, columns = ['text', 'label'])\n",
        "  dataframe_copy = dataframe_copy.append(pd.DataFrame(rows), ignore_index=True)\n",
        "  return dataframe_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "#from transformers import BertModel\n",
        "\n",
        "class CustomBERTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "          super(CustomBERTModel, self).__init__()\n",
        "          self.bert = model_spelling\n",
        "          # add your additional layers here, for example a dropout layer followed by a linear classification head\n",
        "          self.dropout = nn.Dropout(0.3)\n",
        "          self.out = nn.Linear(768, 2)\n",
        "\n",
        "    def forward(self, ids=None, mask=None, token_type_ids=None):\n",
        "            sequence_output, pooled_output = self.bert(\n",
        "                  ids, \n",
        "                  attention_mask=mask,\n",
        "                  token_type_ids=token_type_ids,\n",
        "                  return_dict=False\n",
        "            )\n",
        "            #print(sequence_output)\n",
        "\n",
        "\n",
        "            # we apply dropout to the sequence output, tensor has shape (batch_size, sequence_length, 768)\n",
        "            sequence_output = self.dropout(sequence_output)\n",
        "\n",
        "            # next, we apply the linear layer. The linear layer (which applies a linear transformation)\n",
        "            # takes as input the hidden states of all tokens (so seq_len times a vector of size 768, each corresponding to\n",
        "            # a single token in the input sequence) and outputs 2 numbers (scores, or logits) for every token\n",
        "            # so the logits are of shape (batch_size, sequence_length, 2)\n",
        "            logits = self.out(sequence_output)\n",
        "\n",
        "            return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "M4d1oxmYICia"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/kx/qy_c0smd4c14rc316r9rcq3m0000gn/T/ipykernel_53492/3520144061.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  pred = torch.argmax(F.softmax(logits), dim=1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n",
            "tensor([[0, 0]])\n"
          ]
        }
      ],
      "source": [
        "model_custom = CustomBERTModel()\n",
        "\n",
        "for text, label in test_loader:\n",
        "    encoded_list = [tokenizer_spelling.encode(t, add_special_tokens=True) for t in text]\n",
        "    #print(encoded_list)\n",
        "    logits = model_custom(torch.tensor(encoded_list))\n",
        "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "    print(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7_jQsyL0-e-"
      },
      "source": [
        "Test & Evaluation\n",
        "\n",
        "*주의해야하는 것은, 학습 샘플의 인풋이 (batch_size, sequence_length)로 들어간다는 것이다. 따라서 zero-padding을 직접 해줘서 model의 forward에 넣어줘야한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkwtjS28yMRF",
        "outputId": "15bd3ae3-921f-4465-8a4b-50d99829748f"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-6)\n",
        "\n",
        "total_loss = 0\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "X=1\n",
        "\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    for text, label in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # encoding and zero padding\n",
        "        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "        MAX_LEN = max(len(e) for e in encoded_list)\n",
        "        padded_list =  [e + [0] * (MAX_LEN-len(e)) for e in encoded_list]\n",
        "        \n",
        "        sample = torch.tensor(padded_list)\n",
        "        sample, label = sample.to(device), label.to(device)\n",
        "        labels = torch.tensor(label)\n",
        "        outputs = model(sample, labels=labels, return_dict=False)\n",
        "        loss, logits = outputs\n",
        "\n",
        "        pred = torch.argmax(F.softmax(logits), dim=X)\n",
        "        correct = pred.eq(labels)\n",
        "        total_correct += correct.sum().item()\n",
        "        total_len += len(labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if itr % p_itr == 0:\n",
        "            print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
        "            total_loss = 0\n",
        "            total_len = 0\n",
        "            total_correct = 0\n",
        "        itr+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Daj6lX75AJL_",
        "outputId": "963541c2-14e4-4db3-b277-048bba3529ca"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "total_loss = 0\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "\n",
        "for text, label in test_loader:\n",
        "    encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "    MAX_LEN = max(len(e) for e in encoded_list)\n",
        "    padded_list =  [e + [0] * (MAX_LEN-len(e)) for e in encoded_list]\n",
        "    sample = torch.tensor(padded_list)\n",
        "    sample, label = sample.to(device), label.to(device)\n",
        "    labels = torch.tensor(label)\n",
        "    outputs = model(sample, labels=labels, return_dict=False)\n",
        "    _, logits = outputs\n",
        "\n",
        "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "    correct = pred.eq(labels)\n",
        "    total_correct += correct.sum().item()\n",
        "    total_len += len(labels)\n",
        "\n",
        "print('Test accuracy: ', total_correct / total_len)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Test_1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "2c834dc66de8df9eb0f15badab6bbcafe38fec2b208ae0e4e6389e655cd2c3a2"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('cwwojin')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
