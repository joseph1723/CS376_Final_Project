{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87QI-OOVt9nk",
        "outputId": "fae477a7-a817-4ba0-a608-2c7d2add95c9"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install nlpaug\n",
        "# !git clone https://github.com/joseph1723/CS376_Final_Project.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YHWRrOYpuyPV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Htadzmjwu_Dh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2, output_attentions=True)\n",
        "\n",
        "if torch.cuda.is_available() :\n",
        "  print(\"CUDA\")\n",
        "  device = torch.device(\"cuda\")\n",
        "  model.to(device)\n",
        "else :\n",
        "  print(\"CPU\")\n",
        "  device = torch.device(\"cpu\")\n",
        "  model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CjInRHDwyuUU"
      },
      "outputs": [],
      "source": [
        "class TestDataset(Dataset) :\n",
        "  #Dataset - English/typo-added/labeled\n",
        "  def __init__(self, df) :\n",
        "    self.df = df\n",
        "  \n",
        "  def __len__(self) :\n",
        "    return len(self.df)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    text = self.df.iloc[idx, 0]\n",
        "    label = self.df.iloc[idx, 1]\n",
        "    return text, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0stfCVXzA2yC"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "G_vc7qipA2S_"
      },
      "outputs": [],
      "source": [
        "train_rate, test_rate = 0.5, 0.05\n",
        "itr = 1\n",
        "p_itr = 100\n",
        "epochs = 5\n",
        "batch = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ra3JtiuS1gNS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    text  label\n",
            "0      Ellen Page has come out as a man according to ...      1\n",
            "1      Ellen Page has come out as a meat according to...      1\n",
            "2      Ellen Page has come out as a chowder according...      1\n",
            "3      Ell$B (aYe has come out as a man accoTRinr to ...      1\n",
            "4      Ellen Page has come out as a man according to ...      1\n",
            "...                                                  ...    ...\n",
            "39073                 Being b1tch is unnatural and wrong      0\n",
            "39074                   Being ass is unnatural and wrong      0\n",
            "39075                   hsing gay is unnatural and wrigg      0\n",
            "39076                   Eein9 gay is unnatural and wrun9      0\n",
            "39077                   Begjg gay is Dnnat&r*l and wrong      0\n",
            "\n",
            "[39078 rows x 2 columns]\n",
            "163\n"
          ]
        }
      ],
      "source": [
        "total_df = pd.read_csv('augmented_data/Dataset_aug_complex_6513_.csv', sep=',')\n",
        "total_df.dropna(inplace=True)\n",
        "total_df = total_df[[\"text\", \"label\"]]\n",
        "total_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in total_df[\"label\"]]\n",
        "print(total_df)\n",
        "total_dataset = TestDataset(total_df)\n",
        "total_loader = DataLoader(total_dataset, batch_size=batch, shuffle=True)\n",
        "max_len = 0\n",
        "\n",
        "for text, _ in total_loader:\n",
        "    encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "    l = max(len(e) for e in encoded_list)\n",
        "    max_len = l if l>max_len else max_len\n",
        "\n",
        "print(max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1953 19539\n"
          ]
        }
      ],
      "source": [
        "#Train Set\n",
        "test_df, train_df, _ = np.split(total_df, [int(test_rate*len(total_df)), int(test_rate*len(total_df) + train_rate*len(total_df))])\n",
        "print(len(test_df), len(train_df))\n",
        "train_dataset = TestDataset(train_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
        "test_dataset = TestDataset(test_df)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkwtjS28yMRF",
        "outputId": "15bd3ae3-921f-4465-8a4b-50d99829748f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/kx/qy_c0smd4c14rc316r9rcq3m0000gn/T/ipykernel_70493/1009792008.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(label)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/Users/cwwojin/Woojin/machinelearning/CS376_Final_Project/Test_2.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cwwojin/Woojin/machinelearning/CS376_Final_Project/Test_2.ipynb#ch0000011?line=20'>21</a>\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(label)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cwwojin/Woojin/machinelearning/CS376_Final_Project/Test_2.ipynb#ch0000011?line=21'>22</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(sample, labels\u001b[39m=\u001b[39mlabels, return_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cwwojin/Woojin/machinelearning/CS376_Final_Project/Test_2.ipynb#ch0000011?line=22'>23</a>\u001b[0m loss, logits \u001b[39m=\u001b[39m outputs\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cwwojin/Woojin/machinelearning/CS376_Final_Project/Test_2.ipynb#ch0000011?line=24'>25</a>\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(F\u001b[39m.\u001b[39msoftmax(logits), dim\u001b[39m=\u001b[39mX)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cwwojin/Woojin/machinelearning/CS376_Final_Project/Test_2.ipynb#ch0000011?line=25'>26</a>\u001b[0m correct \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39meq(labels)\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-6)\n",
        "\n",
        "total_loss = 0\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "X=1\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    for text, label in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # encoding and zero padding\n",
        "        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "        MAX_LEN = max(len(e) for e in encoded_list)\n",
        "        padded_list =  [e + [0] * (MAX_LEN-len(e)) for e in encoded_list]\n",
        "        \n",
        "        sample = torch.tensor(padded_list)\n",
        "        sample, label = sample.to(device), label.to(device)\n",
        "        labels = torch.tensor(label)\n",
        "        outputs = model(sample, labels=labels, return_dict=False)\n",
        "        loss, logits = outputs\n",
        "\n",
        "        pred = torch.argmax(F.softmax(logits), dim=X)\n",
        "        correct = pred.eq(labels)\n",
        "        total_correct += correct.sum().item()\n",
        "        total_len += len(labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if itr % p_itr == 0:\n",
        "            print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
        "            total_loss = 0\n",
        "            total_len = 0\n",
        "            total_correct = 0\n",
        "        itr+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Daj6lX75AJL_",
        "outputId": "963541c2-14e4-4db3-b277-048bba3529ca"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "total_loss = 0\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "\n",
        "for text, label in test_loader:\n",
        "    encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "    MAX_LEN = max(len(e) for e in encoded_list)\n",
        "    padded_list =  [e + [0] * (MAX_LEN-len(e)) for e in encoded_list]\n",
        "    sample = torch.tensor(padded_list)\n",
        "    sample, label = sample.to(device), label.to(device)\n",
        "    labels = torch.tensor(label)\n",
        "    outputs = model(sample, labels=labels, return_dict=False)\n",
        "    _, logits = outputs\n",
        "\n",
        "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "    correct = pred.eq(labels)\n",
        "    total_correct += correct.sum().item()\n",
        "    total_len += len(labels)\n",
        "\n",
        "print('Test accuracy: ', total_correct / total_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class neuNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(neuNet,self).__init__()\n",
        "        ## This is the number of hidden nodes in each layer (512)\n",
        "        ## This is the linear layer (784 -> hidden_1)\n",
        "        self.fc1 = nn.Linear(163, 768)\n",
        "        ## This is also linear layer but (n_hidden -> hidden_2)\n",
        "        self.fc2 = nn.Linear(768,1)\n",
        "        #The dropout layer (p=0.2)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = x.view(-1,768)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.droput(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "MLP_model = neuNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Feed_Forward_network(nn.Module):\n",
        "  def __init__(self, dh, dff):\n",
        "    super(Feed_Forward_network, self).__init__()\n",
        "    self.linear1 = nn.Linear(dh, 1)\n",
        "    # self.linear2 = nn.Linear(dff,1)\n",
        "    self.final = nn.Linear(dh, 2)\n",
        "  def forward(self, x, y, valid_length, word_index):\n",
        "    #(Batch, Max_len, dh/emb_dim)\n",
        "    out1 = self.linear1(x)\n",
        "    out1 = F.relu(out1)\n",
        "    print(\"out1: \", out1.shape)\n",
        "    out_soft = masked_softmax(out1.transpose(1,2), valid_length)\n",
        "    print(\"out_soft: \", out_soft.shape)\n",
        "    out_weighted= x.mul(out_soft.transpose(1,2))\n",
        "    print(\"out_weighted: \", out_weighted.shape)\n",
        "    weighted_sum = torch.sum(out_weighted, axis=1)\n",
        "    print(\"weighted_sum: \", weighted_sum.shape)\n",
        "    logits = self.final(weighted_sum)\n",
        "    fil_tgt = 1\n",
        "    loss = F.nll_loss(F.log_softmax(logits,dim=-1), y, ignore_index = 0, reduction = 'none')\n",
        "    loss = loss.sum().mean()\n",
        "    preds = (torch.softmax(logits, dim=-1)).argmax(dim=-1)\n",
        "    return preds, fil_tgt, loss"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Test_1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "2c834dc66de8df9eb0f15badab6bbcafe38fec2b208ae0e4e6389e655cd2c3a2"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('cwwojin')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
