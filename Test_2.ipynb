{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87QI-OOVt9nk",
        "outputId": "fae477a7-a817-4ba0-a608-2c7d2add95c9"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install nlpaug\n",
        "# !git clone https://github.com/joseph1723/CS376_Final_Project.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHWRrOYpuyPV"
      },
      "outputs": [],
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Htadzmjwu_Dh"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('murali1996/bert-base-cased-spell-correction')\n",
        "model = BertForSequenceClassification.from_pretrained('murali1996/bert-base-cased-spell-correction')\n",
        "model_spelling = BertModel.from_pretrained('murali1996/bert-base-cased-spell-correction')\n",
        "#tokenizer_spelling = BertTokenizer.from_pretrained('murali1996/bert-base-cased-spell-correction')\n",
        "if torch.cuda.is_available() :\n",
        "  print(\"CUDA\")\n",
        "  device = torch.device(\"cuda\")\n",
        "  model.to(device)\n",
        "else :\n",
        "  print(\"CPU\")\n",
        "  device = torch.device(\"cpu\")\n",
        "  model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjInRHDwyuUU"
      },
      "outputs": [],
      "source": [
        "class TestDataset(Dataset) :\n",
        "  #Dataset - English/typo-added/labeled\n",
        "  def __init__(self, df) :\n",
        "    self.df = df\n",
        "  \n",
        "  def __len__(self) :\n",
        "    return len(self.df)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    text = self.df.iloc[idx, 0]\n",
        "    label = self.df.iloc[idx, 1]\n",
        "    return text, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0stfCVXzA2yC"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_vc7qipA2S_"
      },
      "outputs": [],
      "source": [
        "train_rate, test_rate = 0.1, 0.01\n",
        "itr = 1\n",
        "p_itr = 100\n",
        "epochs = 1\n",
        "batch = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra3JtiuS1gNS"
      },
      "outputs": [],
      "source": [
        "total_df = pd.read_csv('augmented_data/Dataset_aug_complex_6513_.csv', sep=',')\n",
        "total_df.dropna(inplace=True)\n",
        "total_df = total_df[[\"text\", \"label\"]]\n",
        "total_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in total_df[\"label\"]]\n",
        "print(total_df)\n",
        "total_dataset = TestDataset(total_df)\n",
        "total_loader = DataLoader(total_dataset, batch_size=batch, shuffle=True)\n",
        "max_len = 0\n",
        "\n",
        "for text, _ in total_loader:\n",
        "    encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "    l = max(len(e) for e in encoded_list)\n",
        "    max_len = l if l>max_len else max_len\n",
        "\n",
        "print(max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Train Set\n",
        "test_df, train_df, _ = np.split(total_df, [int(test_rate*len(total_df)), int(test_rate*len(total_df) + train_rate*len(total_df))])\n",
        "print(len(test_df), len(train_df))\n",
        "train_dataset = TestDataset(train_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
        "test_dataset = TestDataset(test_df)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomBERTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "          super(CustomBERTModel, self).__init__()\n",
        "          self.bert = model_spelling\n",
        "          # add your additional layers here, for example a dropout layer followed by a linear classification head\n",
        "          self.dropout = nn.Dropout(0.3)\n",
        "          self.out = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, ids, mask=None, token_type_ids=None, labels=None):\n",
        "          sequence_output, pooled_output = self.bert(\n",
        "               ids, \n",
        "               attention_mask=mask,\n",
        "               token_type_ids=token_type_ids,\n",
        "               return_dict=False\n",
        "          )\n",
        "\n",
        "          # we apply dropout to the sequence output, tensor has shape (batch_size, sequence_length, 768)\n",
        "          sequence_output = self.dropout(sequence_output)\n",
        "          pooled_output = self.dropout(pooled_output)\n",
        "    \n",
        "          # next, we apply the linear layer. The linear layer (which applies a linear transformation)\n",
        "          logits = self.out(pooled_output)\n",
        "\n",
        "          loss_fct = nn.MSELoss()\n",
        "          loss = loss_fct(logits.view(-1).to(torch.float32), labels.view(-1).to(torch.float32))\n",
        "\n",
        "          return loss, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-6)\n",
        "model_custom = CustomBERTModel()\n",
        "\n",
        "total_loss = 0\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "X=1\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    for text, label in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # encoding and zero padding\n",
        "        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "        MAX_LEN = max(len(e) for e in encoded_list)\n",
        "        padded_list =  [e + [0] * (MAX_LEN-len(e)) for e in encoded_list]\n",
        "        \n",
        "        sample = torch.tensor(padded_list)\n",
        "        sample, label = sample.to(device), label.to(device)\n",
        "        labels = torch.tensor(label)\n",
        "        outputs = model_custom(sample, labels=labels)\n",
        "        loss, logits = outputs\n",
        "\n",
        "        pred = torch.argmax(F.softmax(logits), dim=X)\n",
        "        correct = pred.eq(labels)\n",
        "        total_correct += correct.sum().item()\n",
        "        total_len += len(labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if itr % p_itr == 0:\n",
        "            print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
        "            total_loss = 0\n",
        "            total_len = 0\n",
        "            total_correct = 0\n",
        "        itr+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkwtjS28yMRF",
        "outputId": "15bd3ae3-921f-4465-8a4b-50d99829748f"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-6)\n",
        "\n",
        "total_loss = 0\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "X=1\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    for text, label in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # encoding and zero padding\n",
        "        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "        MAX_LEN = max(len(e) for e in encoded_list)\n",
        "        padded_list =  [e + [0] * (MAX_LEN-len(e)) for e in encoded_list]\n",
        "        \n",
        "        sample = torch.tensor(padded_list)\n",
        "        sample, label = sample.to(device), label.to(device)\n",
        "        labels = torch.tensor(label)\n",
        "        outputs = model(sample, labels=labels, return_dict=False)\n",
        "        loss, logits = outputs\n",
        "\n",
        "        pred = torch.argmax(F.softmax(logits), dim=X)\n",
        "        correct = pred.eq(labels)\n",
        "        total_correct += correct.sum().item()\n",
        "        total_len += len(labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if itr % p_itr == 0:\n",
        "            print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
        "            total_loss = 0\n",
        "            total_len = 0\n",
        "            total_correct = 0\n",
        "        itr+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Daj6lX75AJL_",
        "outputId": "963541c2-14e4-4db3-b277-048bba3529ca"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "total_loss = 0\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "\n",
        "for text, label in test_loader:\n",
        "    encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "    MAX_LEN = max(len(e) for e in encoded_list)\n",
        "    padded_list =  [e + [0] * (MAX_LEN-len(e)) for e in encoded_list]\n",
        "    sample = torch.tensor(padded_list)\n",
        "    sample, label = sample.to(device), label.to(device)\n",
        "    labels = torch.tensor(label)\n",
        "    outputs = model(sample, labels=labels, return_dict=False)\n",
        "    _, logits = outputs\n",
        "\n",
        "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "    correct = pred.eq(labels)\n",
        "    total_correct += correct.sum().item()\n",
        "    total_len += len(labels)\n",
        "\n",
        "print('Test accuracy: ', total_correct / total_len)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Test_1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "2c834dc66de8df9eb0f15badab6bbcafe38fec2b208ae0e4e6389e655cd2c3a2"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('cwwojin')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
