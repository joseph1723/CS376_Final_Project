{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIR1zP0dvAOa"
      },
      "source": [
        "Test #1. Pre-Trained BERT Classifier / Misspelled Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ4koSJ1xbLr",
        "outputId": "b493498b-3ab5-4bdd-a7a5-86fbe92c9de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CS376_Final_Project'...\n",
            "remote: Enumerating objects: 328, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 328 (delta 27), reused 61 (delta 23), pack-reused 254\u001b[K\n",
            "Receiving objects: 100% (328/328), 139.06 MiB | 33.45 MiB/s, done.\n",
            "Resolving deltas: 100% (149/149), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/joseph1723/CS376_Final_Project.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHWRrOYpuyPV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjInRHDwyuUU"
      },
      "outputs": [],
      "source": [
        "class TestDataset(Dataset) :\n",
        "  #Dataset - English/typo-added/labeled\n",
        "  def __init__(self, df) :\n",
        "    self.df = df\n",
        "  \n",
        "  def __len__(self) :\n",
        "    return len(self.df)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    text = self.df.iloc[idx, 0]\n",
        "    item = self.df.iloc[idx, 1]\n",
        "    return text, item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0stfCVXzA2yC"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AaP5n2IMsMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88132880-f5ae-4362-b8b4-d29726b4a41e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  text  label\n",
            "0    A strategy for GME fomo aka fuck the market ma...      0\n",
            "1    A view on silver and why it's shit - \"Fuck you...      0\n",
            "2    And I remember that the whole of the privilege...      1\n",
            "3    However, if these procedures are applied more ...      1\n",
            "4    wetting a wart with this saliva on wakening th...      1\n",
            "..                                                 ...    ...\n",
            "851  London explains that the very distinct directi...      1\n",
            "852                                         Pp. 62-63.      1\n",
            "853          Baby ape needing some hand holding on GME      0\n",
            "854  New here, from mexico, watching shit unfold an...      0\n",
            "855        I bought NVDA after June 21. Am I retarded?      0\n",
            "\n",
            "[856 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "train_rate, test_rate = 0.9, 0.05\n",
        "total_df = pd.read_csv('/content/CS376_Final_Project/augmented_data/Dataset_new_856.csv', sep=',')\n",
        "total_df.dropna(inplace=True)\n",
        "total_df = total_df[[\"text\", \"label\"]]\n",
        "total_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in total_df[\"label\"]]\n",
        "print(total_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UdqKyCWICsZ"
      },
      "source": [
        "Data Augmentation / Generating Typo using nlpaug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7_jQsyL0-e-"
      },
      "source": [
        "Test & Evaluation\n",
        "\n",
        "*주의해야하는 것은, 학습 샘플의 인풋이 (batch_size, sequence_length)로 들어간다는 것이다. 따라서 zero-padding을 직접 해줘서 model의 forward에 넣어줘야한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXbBmkz3XJ9"
      },
      "source": [
        "LSTM 데이타 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6Xrr5bD4w85"
      },
      "outputs": [],
      "source": [
        "total_dataset = TestDataset(total_df)\n",
        "total_loader = DataLoader(total_dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-Z5G2AhlqzT",
        "outputId": "bc77f5c8-3c6a-4992-dead-ad6cc1809922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'textClassifier'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 56 (delta 0), reused 1 (delta 0), pack-reused 55\u001b[K\n",
            "Unpacking objects: 100% (56/56), done.\n"
          ]
        }
      ],
      "source": [
        "# clone the repo\n",
        "!git clone https://github.com/richliao/textClassifier.git\n",
        "# install Dependent library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFoIN0a3mICT",
        "outputId": "013e8287-25be-4416-faa5-69452b46bbb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting backports.weakref==1.0.post1\n",
            "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.3 in /usr/local/lib/python3.7/dist-packages (from -r ./textClassifier/req.txt (line 2)) (4.6.3)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading bleach-1.5.0-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: bs4==0.0.1 in /usr/local/lib/python3.7/dist-packages (from -r ./textClassifier/req.txt (line 4)) (0.0.1)\n",
            "Collecting Cython==0.27.3\n",
            "  Downloading Cython-0.27.3.tar.gz (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting enum34==1.1.6\n",
            "  Downloading enum34-1.1.6-py3-none-any.whl (12 kB)\n",
            "Collecting funcsigs==1.0.2\n",
            "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
            "Collecting h5py==2.7.1\n",
            "  Downloading h5py-2.7.1.tar.gz (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 46.9 MB/s \n",
            "\u001b[?25hCollecting html5lib==0.9999999\n",
            "  Downloading html5lib-0.9999999.tar.gz (889 kB)\n",
            "\u001b[K     |████████████████████████████████| 889 kB 38.7 MB/s \n",
            "\u001b[?25hCollecting Keras==2.0.8\n",
            "  Downloading Keras-2.0.8-py2.py3-none-any.whl (276 kB)\n",
            "\u001b[K     |████████████████████████████████| 276 kB 52.6 MB/s \n",
            "\u001b[?25hCollecting laspy==1.5.0\n",
            "  Downloading laspy-1.5.0-py3-none-any.whl (489 kB)\n",
            "\u001b[K     |████████████████████████████████| 489 kB 50.1 MB/s \n",
            "\u001b[?25hCollecting lda==1.0.5\n",
            "  Downloading lda-1.0.5.tar.gz (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 47.3 MB/s \n",
            "\u001b[?25hCollecting Markdown==2.6.9\n",
            "  Downloading Markdown-2.6.9.tar.gz (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 45.2 MB/s \n",
            "\u001b[?25hCollecting mock==2.0.0\n",
            "  Downloading mock-2.0.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 273 kB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 45.5 MB/s \n",
            "\u001b[?25hCollecting numpy==1.13.3\n",
            "  Downloading numpy-1.13.3.zip (5.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.0 MB 41.3 MB/s \n",
            "\u001b[?25hCollecting olefile==0.44\n",
            "  Downloading olefile-0.44.zip (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting pandas==0.20.3\n",
            "  Downloading pandas-0.20.3.tar.gz (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 915 kB/s \n",
            "\u001b[?25hCollecting pbr==3.1.1\n",
            "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting Pillow==4.3.0\n",
            "  Downloading Pillow-4.3.0.tar.gz (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 22.9 MB/s \n",
            "\u001b[?25hCollecting protobuf==3.4.0\n",
            "  Downloading protobuf-3.4.0-py2.py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 60.1 MB/s \n",
            "\u001b[?25hCollecting pypinyin==0.29.0\n",
            "  Downloading pypinyin-0.29.0-py2.py3-none-any.whl (987 kB)\n",
            "\u001b[K     |████████████████████████████████| 987 kB 65.5 MB/s \n",
            "\u001b[?25hCollecting python-dateutil==2.6.1\n",
            "  Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194 kB)\n",
            "\u001b[K     |████████████████████████████████| 194 kB 71.9 MB/s \n",
            "\u001b[?25hCollecting pytz==2017.2\n",
            "  Downloading pytz-2017.2-py2.py3-none-any.whl (484 kB)\n",
            "\u001b[K     |████████████████████████████████| 484 kB 56.3 MB/s \n",
            "\u001b[?25hCollecting PyYAML==3.12\n",
            "  Downloading PyYAML-3.12.zip (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 57.8 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.19.1\n",
            "  Downloading scikit-learn-0.19.1.tar.gz (9.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5 MB 25.6 MB/s \n",
            "\u001b[?25hCollecting scipy==0.19.1\n",
            "  Downloading scipy-0.19.1.tar.gz (14.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.1 MB 86 kB/s \n",
            "\u001b[?25hCollecting six==1.11.0\n",
            "  Downloading six-1.11.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r ./textClassifier/req.txt (line 29)) (0.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.3.0 (from versions: 1.13.1, 1.13.2, 1.14.0, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.0+zzzcolab20220506153740, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.4+zzzcolab20220516125453, 2.6.5, 2.6.5+zzzcolab20220523104206, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.0+zzzcolab20220506150900, 2.7.1, 2.7.2, 2.7.2+zzzcolab20220516114640, 2.7.3, 2.7.3+zzzcolab20220523111007, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.0+zzzcolab20220506162203, 2.8.1, 2.8.1+zzzcolab20220516111314, 2.8.1+zzzcolab20220518083849, 2.8.2, 2.8.2+zzzcolab20220523105045, 2.8.2+zzzcolab20220527125636, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for tensorflow==1.3.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# !cd textClassifier\n",
        "# !ls\n",
        "# !pip install -r req.txt\n",
        "!pip install -r ./textClassifier/req.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GbH06wH27U3",
        "outputId": "a5253dac-a6b7-4912-cf80-08975e8bfa85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.46.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHzEjw4moKBn",
        "outputId": "a519d392-571a-4a3d-f082-219396773a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-12 08:24:53--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-06-12 08:24:53--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-06-12 08:24:53--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.10MB/s    in 2m 43s  \n",
            "\n",
            "2022-06-12 08:27:36 (5.05 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQLAP7ANoN1E",
        "outputId": "5358d421-f4e2-4a39-cdbc-3ef21b8a21d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XAVNanDpBzX",
        "outputId": "db7323f4-e121-404e-dc6d-c737b5e63c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-12 08:28:21--  https://www.kaggle.com/c/word2vec-nlp-tutorial/download/labeledTrainData.tsv\n",
            "Resolving www.kaggle.com (www.kaggle.com)... 35.244.233.98\n",
            "Connecting to www.kaggle.com (www.kaggle.com)|35.244.233.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /account/login?returnUrl=%2Fcompetitions%2Fword2vec-nlp-tutorial [following]\n",
            "--2022-06-12 08:28:21--  https://www.kaggle.com/account/login?returnUrl=%2Fcompetitions%2Fword2vec-nlp-tutorial\n",
            "Reusing existing connection to www.kaggle.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘labeledTrainData.tsv’\n",
            "\n",
            "labeledTrainData.ts     [ <=>                ]   6.61K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-06-12 08:28:21 (46.0 MB/s) - ‘labeledTrainData.tsv’ saved [6769]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.kaggle.com/c/word2vec-nlp-tutorial/download/labeledTrainData.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw_PaR6wodXY",
        "outputId": "522d959e-418d-4d54-e5cf-239d4b65565e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxOeDJSAnHfa",
        "outputId": "900f769b-3fa5-4dd1-9417-6296fc5f2410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.3.0 (from versions: 1.13.1, 1.13.2, 1.14.0, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.0+zzzcolab20220506153740, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.4+zzzcolab20220516125453, 2.6.5, 2.6.5+zzzcolab20220523104206, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.0+zzzcolab20220506150900, 2.7.1, 2.7.2, 2.7.2+zzzcolab20220516114640, 2.7.3, 2.7.3+zzzcolab20220523111007, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.0+zzzcolab20220506162203, 2.8.1, 2.8.1+zzzcolab20220516111314, 2.8.1+zzzcolab20220518083849, 2.8.2, 2.8.2+zzzcolab20220523105045, 2.8.2+zzzcolab20220527125636, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for tensorflow==1.3.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow==1.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbFHFHwKiqRB",
        "outputId": "a049d2c6-83cc-4795-ac75-9ae54d0d2dc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('He showed little interest in measures put forward by the regular Democrats.',)\n",
            "('They kill and destroy, retard or prevent reproduction and growth, impair and damage values, and disrupt uses.',)\n",
            "('If the answer to what was that noise??',)\n",
            "('My GME expires tomorrow and im not in the money thanks to Robinhoods little trick. What the hell.',)\n",
            "('True, there had been raids on Naples--but Naples was pretty far north on the opposite coast.',)\n",
            "('If for some reason your broker is gay',)\n",
            "('Will it hurt you retard if I bet against gme?',)\n",
            "(\"I, who until that day before had been Mrs. Salter's friend, her equal, was the servant now.\",)\n",
            "('Now how the hell am I (are we) going to make our money back?',)\n",
            "('Ritter died in 1810 and Oersted not only lived to see the event occur but was the author of it.',)\n",
            "('This Class can be just as successful in the dog world if it is given a chance.',)\n",
            "('And even if they stay in resorts part of the time, they might, if the right salesman gets them in tow, develop a yearning to spice the usual vacation fare with a camping trip into the wide open spaces.',)\n",
            "('They often threw tidbits overboard.',)\n",
            "(\"I'm tired of seeing people saying that the shorts could not have covered with shit assumptions and subpar mathematics to back it up\",)\n",
            "('Styka shook his head slowly.',)\n",
            "('He would not be indebted to Sam!!',)\n",
            "('did i just fuck myself over?',)\n",
            "(\"You apes are retarded, it'll never go back up, face it you lost....\",)\n",
            "('Pp. 62-63.',)\n",
            "('retard in training here. (AAPL)',)\n",
            "('Miss Schwarzkopf is too great an artist to need them.',)\n",
            "('let Stein.',)\n",
            "('Information is also available from this autocollimator system to set the drift bias for the Z-axis gyro.',)\n",
            "('And then the station wagon and the Ford would seek him out again.',)\n",
            "('A commendation to all the people mobilising on other platforms and fuck RH',)\n",
            "(\"I'm here til its at fucking $4 fuck all those billionaire fuckers.\",)\n",
            "('Does anyone else feel nervous as hell about making any substantial moves right now?',)\n",
            "('This is cheesy as hell but WSB has given me hope for humanity.',)\n",
            "('After this whole debacle is over, stop using Robinhood and short the hell out of its IPO when it comes out as revenge.',)\n",
            "(\"I wrote a script to parse Robinhood's 1099 pdf into an excel sheet so yall don't have to do that shit by hand\",)\n",
            "('If we look about the world today, we can see clearly that there are two especially significant factors shaping the future of our civilization: science and religion.',)\n",
            "('Super retard here. Options or straight stonk?',)\n",
            "('The award is the first such honor in the 11-year history of JA activities in Portland, according to Ralph Scolatti, local executive director for Junior Achievement.',)\n",
            "('Wing Commanders in the RAF do not imply survival in the future either in their orders or in their attitudes, to their men or to themselves.',)\n",
            "(\"Again you're doing half ass work\",)\n",
            "('And he said, \"The same fellow who has been running it, Sam Rayburn\".',)\n",
            "('Last month in Ghana an American missionary discovered when he came to pay his hotel bill that the usual rate had been doubled.',)\n",
            "(\"i don't have much, but i threw $20 toward AMC because fuck these hedge fund elites\",)\n",
            "(\"With the speed and power of the body beautiful he saw before him, Ol' Case wanted No. 7 to be not only the best homerun hitter, but also the best bunter, base-runner and outfielder.\",)\n",
            "('I could not cling to my past nor did I wish to.',)\n",
            "('GME technical analysis from a retard Part 3. Why I think the short squeeze is nearly here.',)\n",
            "('To prepare the latter, silver chloride was precipitated from a solution containing Af obtained from the Oak Ridge National Laboratory.',)\n",
            "('Can amc stop acting like coke dick',)\n",
            "('Trachea clear of mucus and foreign objects.',)\n",
            "('Who the fuck needs the news???',)\n",
            "('$WWE - Can I get a hell yeah',)\n",
            "(\"IN W/ 7 GME @ $69, I'M DOING MY PART\",)\n",
            "('Can we all agree that this GME/WSB movie is gonna be trash?',)\n",
            "('Who the fuck is buying BBW?!',)\n",
            "('What trading site should I switch to since Robinhood is scum?',)\n",
            "('The Palazzo Caetani, still inhabited by the Caetani family, adjoins the Palazzo Mattei.',)\n",
            "('I saw then, too, the stake driven straight and hard into the plowed soil, through something there where I had been not long before.',)\n",
            "(\"If you don't believe hedge funders don't have enough money to pay people to fuck up this sub, you have gone from Autist Elite + to Harvard Grad.\",)\n",
            "(\"Had Faith Constable's explanation of her confidence, so uninvited, been a little thin??\",)\n",
            "('I bought NVDA after June 21. Am I retarded?',)\n",
            "('Nevertheless, it will seem funny to have to send for a mechanic to improve conversation.',)\n",
            "(\"Lads fuck off with your posts and just buy or hold. It's going down\",)\n",
            "('Livingston Birdwood, producer of Uprising.',)\n",
            "(\"He might not have gone that far if Pa hadn't been locked in laughing fit to shake the house.\",)\n",
            "('Even if it did not, how would this little world of gentle people cope with its new reality of grenades and submachine guns??',)\n",
            "('DD: DASH is trash. Or how the hell is it worth only 20% less than FEDEX.',)\n",
            "('And to an industry that prides itself on authenticity, he urged greater realism.',)\n",
            "('a man mountain running to lard in his middle-age.',)\n",
            "('I think Adam Herberet is guilty of being too hopeful and better informed on defense financing than on the technical side.',)\n",
            "('A nation may go to war on some trifling pretext, when in reality it may have been guided by an unconscious instinct that its very life was at stake.',)\n",
            "('The sound may be good;;',)\n",
            "('by means of an actual label or',)\n",
            "('They surged around him, fingers pointing, eyes prying.',)\n",
            "('Went full retard',)\n",
            "('We can stay retarded longer than WSB can stay solvent.',)\n",
            "('Bought Doge coin Bc fuck it',)\n",
            "('It stipulates, in addition, that all amounts remaining as a result of imposing the \"ceiling\", and not used for insuring the \"floor\", be redistributed to those States still below their maximums.',)\n",
            "('If we all piss our pants at the same time we can flood wall street.',)\n",
            "('I must confess that I prefer the Liberal who is personally affected, who is willing to send his own children to a mixed school as proof of his faith.',)\n",
            "('Hold your shit.',)\n",
            "('This place has turned to shit',)\n",
            "('Even the states remain primarily in an assisting role, providing leadership and teacher training.',)\n",
            "(\"can't make this shit up gme peaked at $469.420 today\",)\n",
            "('Fatties rejoice NVO has legal drugs to loose weight',)\n",
            "('He knows me as your niece, which, of course, I am.',)\n",
            "('At least this seemed to be the working hypothesis for \"Chicago And All That Jazz\", presented on NBC-TV Nov. 26.',)\n",
            "(\"I'm a rookie ape but I want to help support the cause! Does someone have a quick guide for idiots that want in?\",)\n",
            "('On the morning of November 17th, Cornwallis and 2,000 men had left Philadelphia with the object of capturing Fort Mercer at Red Bank, New Jersey.',)\n",
            "('The answer to both questions is immediately obvious.',)\n",
            "('NVAX part II - Last call before a shit ton of catalysts',)\n",
            "('In describing the initial Allied occupation of a middle-sized German city, the picture has color, pictorial pull and genuinely moving moments.',)\n",
            "('The event was so successful that the Interior Secretary plans to serve as impresario for similar ones from time to time, hoping thereby to add to the cultural enrichment of the Administration.',)\n",
            "('Woodruff had supported him all the way, both as a chief executive and as a man.',)\n",
            "('Poor retard here, bought both AMC and GME.',)\n",
            "('But for purely definition purposes--used in conjunction with your regular Squatting, Leg Curling, Leg Extensor programs--a heavy weight is not needed.',)\n",
            "(\"Sup, apetards! I've been DOING some technical ANALysis recently (fractals and shit), and found out that GeeMEeee is going to fking PENETRATE $1K next week. Hope you enjoy the read!\",)\n",
            "('Big time retard move planned way ahead. AMZN put strategy',)\n",
            "('GTFO with your pessimism and trash talking profit takers...',)\n",
            "('Dill was silent as if he hated to answer, and Barton had a cold, sick feeling of apprehension.',)\n",
            "('Fuck GME: a word from a gay bear',)\n",
            "(\"PSA: If you take Stimulus money and drop it into meme stocks, you're a piece of shit. And I hope you get flushed like the turd you are.\",)\n",
            "('(2)',)\n",
            "('Other basement windows should be blocked when an emergency threatens.',)\n",
            "('A few of these people are still around\".',)\n",
            "('Why the earnings fiasco could be good for the future (fuck George Sherman)',)\n",
            "(\"But she couldn't, not yet.\",)\n",
            "('Whether the only price of our redemption were not the death of Christ on the cross, with the rest of his sufferings and obediences, in the time of his life here, after he was born of the Virgin Mary??',)\n",
            "(\"Hey guys! I'm gonna buy puts on Robinhood when it IPOs because I really do not like the stock (I would short it but I'm too retarded to know how to do that)\",)\n",
            "('Now that shit has somewhat calmed down here, lets try this again: Ammo. Sports. Orgies. ; ASO, a DD:',)\n",
            "(\"As a result, most people don't have more than a vague idea what folklore actually is;;\",)\n",
            "(\"DD - How to best prolong the endgame orgasm. (Hint: We haven't started yet) - Pics and Emojis for smooth brains included!\",)\n",
            "(\"I've been trading Silver futures August. Don't fall for their shit by listening to your doomer uncle finally\",)\n",
            "('Do I qualify as a real retard yet?',)\n",
            "('Your a retard if you sell amc tomorrow',)\n",
            "('Sweating so much??',)\n",
            "(\"Yo this shit really the wild west rn. Some of these post just egging y'all on btw\",)\n",
            "('GME on the rise my ape bretheren, but do not forget to support your bretheren in AMC, BB',)\n",
            "(\"Today I'd like to say fuck you to the hedgefunds from an exploited working stiff RANT\",)\n",
            "('Nevertheless, most of the teen-agers I interviewed believed in maintaining their Jewish identity and even envisioned joining a synagogue or temple.',)\n",
            "(';;',)\n",
            "('Lol the dude on CNBC is such a passive aggressive douche',)\n",
            "(\"OG's don't hate $GME, just a chunk of the people trading it: Stop being cultish it's cringey and annoying -- let's clear some shit up and dispel some $GME myths\",)\n",
            "('Breakfast was at the Palace Hotel, luncheon was somewhere in the mountain forest, and dinner was either at Boulder Creek or at Santa Cruz.',)\n",
            "('Temporary abstention, i.e., postponement, is one thing;;',)\n",
            "('The mythological private eye differs from his counterpart in real life in two essential ways.',)\n",
            "('Robinhood fucked with wrong one. Time to go full retard.',)\n",
            "(\"Fed can NEVER raise rates, inflation is NOT transitory, and if you think otherwise you're literally retarded\",)\n",
            "('We need some wholesome shit to keep the comrades alive',)\n",
            "(\"later he flees in panic from the family table just as his theft is about to be discovered and is blocked at the front door by a soldier who accusingly holds out a pair of handcuffs which he has brought to Gargery's forge for mending.\",)\n",
            "('Nervous as shit',)\n",
            "('There is much to be said for such a college--and Dartmouth men have been accused of saying it too often and too loudly.',)\n",
            "('The use of this concept does not specify the origin of the radiation, and only if the planet really radiates as a black body, will the apparent black-body temperature correspond to the physical temperature of the emitting material.',)\n",
            "('It was her work to go among her neighbors and collect their checks.',)\n",
            "(\"Gulf's holdings could have been converted into 2,700,877 shares of Union Oil common upon surrender of debentures plus cash, according to Union.\",)\n",
            "(\"IMPORTANT ACTION. I'm going to repost this until yall get your shit together. Don't rely on the media to bail you out, put in the work.\",)\n",
            "('Sometimes they get their initial experience in church haflis, conducted by Lebanese and Syrians in the U.S., where they dance with just as few veils across their bodies as in nightclubs.',)\n",
            "('I have a good feeling retards, my retard instincts never lie..Hold GME',)\n",
            "('For everyone involved knew that the whole valley was a powder keg, and Mitchell Barton the fuse which could send it into explosive violence.',)\n",
            "('Please read the comments and guidelines before posting your little dick .5 GME share',)\n",
            "(\"But Jack always derived vicarious sensual thrills from Charles' revelations (even when he suspected his friend of exaggeration or invention), so he usually invited them, as he did now.\",)\n",
            "(\"Everyone here is trying to save their own investment by forcing other people to buy sayin shit like 'We knew this day would come'\",)\n",
            "('She loved the children.',)\n",
            "('Holy shit! $AMC Volume is through the roof! Over 295 Million!',)\n",
            "('When this shit is over we are gonna reward those holding...',)\n",
            "('of which one is deliberately willed or intended and the other not intended or not directly intended, but still both are done, while the evil effect is, with equal consciousness on the part of the agent, foreknown to be among the consequences.',)\n",
            "(\"How is Wish supposed to be worth anything? Most of what they sell is garbage. Every Wish merchandise review I've seen says it's trash\",)\n",
            "('Or at least appear to reject it!!',)\n",
            "('Used to by my dawg, You were my Robinhood, Screamed \"ride or die,\" I thought you would die with me, Found out you a bitch, you can\\'t even ride with me, Now it\\'s a war and you ain\\'t on the side with me',)\n",
            "('Fidelity rocks, fuck Robinhood.',)\n",
            "('Finally, the image of a general bundle of lines is a congruence whose order is the order of the congruence of invariant lines, namely Af and whose class is the order of the image congruence of a general plane field of lines, namely Af.',)\n",
            "('\"I see there are some cars here.',)\n",
            "('Alexander the Great, who used runners as message carriers, did not have to worry about having every officer in his command hear what he said and having hundreds of them comment at once.',)\n",
            "('repost: Fellow retard: still holding $BB, and getting that sinking feeling? You are not alone !',)\n",
            "('Robinhood sucks ape bananas, the Public App is better',)\n",
            "('He judged her to be a woman of some pride, though not much sense.',)\n",
            "('in the exciting salons of his lovely, brilliant Paris--mistress of gaiety--excess and abandon--world theatre of new-found freedoms in tone, color, dance, design, and thought.',)\n",
            "('Everything is down. Fuck us but hold the line (because of the stupid ass hedge fund and sec I have to say this is not financial advice)',)\n",
            "('The dust-thick saliva came from his mouth like balled cotton.',)\n",
            "('Small wonder, then, that we fear.',)\n",
            "('(Along the way there, about one hundred yards on your right, you pass a simple restaurant, La Sacrestia, where you can have the best pizza in Rome.',)\n",
            "(\"Sweat started out on William's forehead, whether from relief or disquietude he could not tell.\",)\n",
            "('Petitioner registered with Local Board No. 9, Boulder, Colorado, on March 17, 1952.',)\n",
            "('Mass spectrometric analysis of gases evolved upon heating to 410-degrees-C indicated nitrogen oxides and water vapor.',)\n",
            "('Nobody liked to hire him because you never could tell when he was going to be taken drunk.',)\n",
            "('Signora Ferraro, bobbing her head encouragingly, asked Sam about Canada, having a special interest.',)\n",
            "('want to open account NOW to buy shit ton of $GME',)\n",
            "('but many against combining to raise it\".',)\n",
            "('and the dwindling built-soap powders as well as soap flakes and granules, none of which are now nationally advertised.',)\n",
            "('The law could not suspend economics.',)\n",
            "('4.',)\n",
            "('Thus the cocktail party would appear to be the ideal system, but there is one weakness.',)\n",
            "('200$ of WISH calls expiring in 2 months am I retarded or smart',)\n",
            "('I have 150 dollars in my name. How can I help fuck up wallstreet?',)\n",
            "('Woodrow Wilson, with whom he began his years in Washington, Warren G. Harding, Calvin Coolidge, FDR, with whom he managed a social revolution.',)\n",
            "('Something in my voice must have touched her deeply because her anger passed quickly, and she turned away to keep me from seeing her face.',)\n",
            "(\"Yo I'm dumb as fuck.\",)\n",
            "(\"Watch closely while browning them, as it doesn't take long.\",)\n",
            "('Andy invented a job to keep him busy, sending him ahead to El Dorado to supervise last minute arrangements.',)\n",
            "('We could seriously fuck over the hedgie bot that detects the most mentioned stocks here if we spam posts that mention a random stock we decide on an excessive amount of times.',)\n",
            "('Fuck Robinhood in the ass',)\n",
            "(\"Let's make WSB a hedge fund, for the people, by the people. If they won't let the little guy play by their rules, then they can fuck right off, we'll BEAT them at their own game\",)\n",
            "('What the fucking fuck is going on markets are tanking here is why!',)\n",
            "('If you have any cash on these shit sites withdraw it all.',)\n",
            "('The results of the election of 1859 found Republican candidates not only winning the offices of governor and lieutenant-governor but also obtaining the two Congressional offices from the eastern and western sections of the state.',)\n",
            "('It did not serve to contrast the existing order of society with a possible alternative order, because the age of innocence was not a possible alternative once man had sinned.',)\n",
            "('It is apparent from the above and from experimental evidence that the cooling requirements for the anode of free burning arcs are large compared with those for the cathode.',)\n",
            "('Taylor said Mrs. Huntley and her husband also will be questioned about a series of 15 Portland robberies in spring of 1959 in which the holdup men bound their victims with tape before fleeing.',)\n",
            "('Assumption 1.',)\n",
            "('She clung to him, talking to him, and dabbing at her eyes.',)\n",
            "(\"Should I move my stocks out of Robinhood? And if so, what are better trading platforms that give a shit about you.... and won't fuck you over by allowing Daddy to force them to manipulate the market? Or is that all of them?\",)\n",
            "('\"You\\'re Gavin\\'s son\", Joe Purvis had said.',)\n",
            "('Stepson vindicated',)\n",
            "('WSB: I found something retarded that cannot be true. Explain to me what I am not seeing (GME options)',)\n",
            "('Literally fuck a hedge fund in the face',)\n",
            "(\"What's the most likely reason all these godforsaken meme stock price charts follow the same god damn pattern?????\",)\n",
            "('I looked away.',)\n",
            "(\"I'm a retard.\",)\n",
            "('Robinhood and Webull can Kiss my ass',)\n",
            "(\"Whee the People: Lovely Thrush Annamorena gave up a promising show biz career to apply glamor touches to her hubby, Ray Lenobel's fur firm here.\",)\n",
            "('Mr. Robards--Jenny was the only person she knew of in the Mt. Pleasant neighborhood who called him that--was kind but too easygoing.',)\n",
            "('Women actually began to appear unaccompanied in the stands, where they still occasionally ran the risk of coming home with a tobacco-juice stain on a clean skirt or a new curse word tingling their ears.',)\n",
            "('The American Constitution was historic at this point in providing that \"Congress shall make no law respecting an establishment of religion or prohibiting the free exercise thereof\".',)\n",
            "(\"If RobinHood is down.. use eToro or another market to buy, simple.. just don't bitch out and sell, were in this together\",)\n",
            "('Just the same, the old woman said, she would write to her nephew in his boxcar and tell him she had met a nice man from his adopted country.',)\n",
            "(\"Student musical organizations are the Knights of Carleton and the Overtones (men's vocal groups), and the Keynotes (a women's singing group).\",)\n",
            "('The impact of technological factors is also illustrated by the history of the high-energy fuel program.',)\n",
            "('Then was it a final desperate plea from her, to whom??',)\n",
            "('AMC ready to take a shit?',)\n",
            "('It is true, of course, the uncertainties of life being what they are, that as now and then the Christian killed the lion, homecoming days have been ruined by a visiting team.',)\n",
            "('Not only were the court costs prohibitive, but I was subjected to crippling fines, in addition to usurious interest on the unpaid \"debts\" which the government claimed that Metronome and I owed--a severe financial blow.',)\n",
            "('69 calls (110k) on PRLB (Protolabs) - Part 1, Qualitative analysis',)\n",
            "(\"BuT ANALySts SaY RkT IsNt A bUy; No shit jello-brains and here's why: $RKT DD Part I of III\",)\n",
            "('True Short interest in GEE EM EE could be anywhere from 250% to 967% of the float. Yes short sellers are that fucking retarded.',)\n",
            "('I accidentally revealed to people that i bought 15 GME @ 310 so can you tards hurry up and pick me up so i dont look like an actual retard',)\n",
            "('Hope surged within him.',)\n",
            "('No ape shit: the Shorts are simply preparing for the squeeze',)\n",
            "('Just some reassurance from a fellow retard',)\n",
            "('NAKD, GME, AMC...hold that shit.',)\n",
            "('Am i the only ape who thinks yesterdays hearing was a joke?',)\n",
            "('Spectra were also obtained from a third sample of Af which had been diluted to three times its original volume with powdered, anhydrous alundum (Af).',)\n",
            "(\"Fromm's analysis of alienation in the sphere of production centers around the concepts of the bureaucratization of the corporation, the separation of ownership from control, and the broad (and thus from the point of view of corporate control, ineffective) dispersion of stock ownership.\",)\n",
            "('My commitment to going full retard. Day 4/30.',)\n",
            "('Another problem is who will pay for the training.',)\n",
            "('The Company of the Indies promised to send over a supply of Indian trading goods, and to price them more cheaply in terms of deerskins.',)\n",
            "('therefore, the recruitment or missionary outreach of the congregation will be co-optation rather than proclamation--like elements will have to be assimilated.',)\n",
            "('\"Well I object.',)\n",
            "('He is one of the few men in history who plays jazz on a violin.',)\n",
            "('The service is over, and a number of people come from the church with their spokesman Mityukh in the lead.',)\n",
            "('Can the hedge funds do more shady shit?',)\n",
            "('Why this ape is holding',)\n",
            "('In California is located one of the most popular of the national parks--Yosemite.',)\n",
            "('Have I been brainwashed or have I become retarded?',)\n",
            "('Clov Fundamentally sound outlook. Why Clover Health is retarded.',)\n",
            "('I\\'m not afraid to tangle with the Republican nominee\".',)\n",
            "('The men mewed and scratched, begging to be taken away.',)\n",
            "('Hey Melvin, CNBC asshats, other random gay bears, listen up cuz I have an offer for you.',)\n",
            "(\"Robinhood and these other shit apps aren't the only players in town!!!! MEGA THREAD for apps that are working and easy:\",)\n",
            "('All I know is that they are gone\".',)\n",
            "('Bought a shit load of NOK $ AMC HOLLLLDDD',)\n",
            "(\"Can't GameStop just issue more shares & fuck us all?\",)\n",
            "('It especially bothered the older hands.',)\n",
            "(\"After we got a script and the spots for the songs were blocked out, we'd get together for an hour or so every day.\",)\n",
            "('\"Remember to call me Jan\", Andrei said.',)\n",
            "(\"All in on Alteryx or no? I don't give a fuck about analysis, fundamentals, fucking...candles, drawing lines, whatever...I know Alteryx made sick gains before, and the price is low as shit right now. So, after reading my TA, what we saying? All in or no?\",)\n",
            "('Robinhood deactivated my account with over $13,000 left on it! What the fuck?',)\n",
            "('$69 is my jam',)\n",
            "('Now I am retarded as the next guy on this sub, but an honest question...',)\n",
            "('Opendoor Technologies -- Currently selling my home and after experience want to short the shit out of this company-- Am I missing something?',)\n",
            "('He pulled it over, climbed up, and lifted out the big volume, almost losing his balance from the weight of it.',)\n",
            "('These items, and most of the others listed above, seem quite comparable to items whose right of survival is provided for in section 381.',)\n",
            "('RH gives middle finger to WSB',)\n",
            "('A vision handicap that may produce nervous tension and reading disability for one child may spur another child on to even greater achievement in reading.',)\n",
            "('Not at all.',)\n",
            "('Paper bitch',)\n",
            "('A view on silver and why it\\'s shit - \"Fuck you\" edition',)\n",
            "('He found that if he was tired enough at night, he went to sleep simply because he was too exhausted to stay awake.',)\n",
            "(') The film was called The Diet of Worms, which I felt was just what Letch deserved.',)\n",
            "('[Question] Is there a way to transfer funds or owned shares to another broker service (as in get the fuck off of Robinhood) without paper-handing and losing it all?',)\n",
            "('However, if the child has been constantly surrounded, during nursery and early school age, by peer groups;;',)\n",
            "('Starting to act like a retard by giving zero fucks about diversification and over half of my portfolio is now in video game stocks (ATVI, TTWO, EA)',)\n",
            "('Almost immediately Howard and his daughter Debora drove up in the Cadillac.',)\n",
            "(\"I'm willing to lose my $10k. Fuck you Melvin, and fuck you Robinhood\",)\n",
            "('They too loved their families, longed for their villages: yet lacked the faith that drove one to dare the fearful chance of escape\".',)\n",
            "('Regrets attack',)\n",
            "('A further regulation is that commands always go down, unaccompanied by statements, and statements always go up, unaccompanied by commands.',)\n",
            "('At first glance, this hero seems to be more rather than less of an individualist than any of his predecessors.',)\n",
            "('John, an engineer and anthropologist with a doctorate from the London School of Economics, headed the rural development division of USOM, the United States Operations Mission administering U.S. aid.',)\n",
            "('What are your thoughts from one dumb ape to another on $RKT?',)\n",
            "('You need answers to four important questions.',)\n",
            "(\"I'm finally a retarded shit stirring ape monke\",)\n",
            "(\"See what happens when you paper hand like a bitch? You lose out. This squeeze hasn't even started yet.\",)\n",
            "('The housing bill is expected to encounter strong opposition by the coalition of Southern Democrats and conservative Republicans.',)\n",
            "(\"Except for the wine waiter in a restaurant--always an inscrutable plenipotentiary unto himself, the genii with the keys to unlock the gates of the wine world are one's dealer, and the foreign shipper or negociant who in turn supplies him.\",)\n",
            "('They\\'re up there in that freezing climate and all of us have to try and help them\".',)\n",
            "(\"Think I'm down 20K AUD (don't even try to keep score anymore - don't give a shit), HOLDING\",)\n",
            "('$VRT is the best shit that nobody is talking about',)\n",
            "('From a \"Dad\" to all you retarded apes!',)\n",
            "('Are Trading 212 trying to do their own Robinhood and fuck over the UK market?',)\n",
            "(\"I'm a virgin ape how do I join?\",)\n",
            "(\"I'm shit at trading options\",)\n",
            "(\"I'm about to post more unpopular shit on this channel because y'all dumbasses need to hear it - GME\",)\n",
            "('-- it evaporated, disappeared, and came back to the earth as rain--maybe for another well or another stream or another Alfred Alpert.',)\n",
            "(\"I couldn't make out what his racket was.\",)\n",
            "(\"Too the moon and never back!!! Let's go boys squeeze the shit out of $NAKD\",)\n",
            "('For prevention of foamy bloat, feed at a rate of 0.5 to 2 milligrams per head per day in mineral or salt or feed.',)\n",
            "('Vietnamese dong?',)\n",
            "('$ASO ape here. Some fast fact DD for you all.',)\n",
            "('Not to get into conspiracies, but Vlad is full of shit in Gamestopped on Hulu',)\n",
            "('With leather cups fitted in his handlebars, he steered his bicycle.',)\n",
            "(\"Don't buy anymore GME or AMC (especially if you're new) unless you got money to burn or are too retarded to read this. Just hold.\",)\n",
            "('How the fuck do I buy GME right now??',)\n",
            "('Generally, they go to school with a girl named Gloriana, who lives down the block, and has a car.',)\n",
            "(\"So it is with Great Expectations, whether the hands be Orlick's as he strikes down Mrs. Gargery or Pip's as he steals a pie from her pantry.\",)\n",
            "('\"Don\\'t you dare\".',)\n",
            "('\"And the water would be still colder\", Ceecee seemed to shiver at the thought of it.',)\n",
            "(\"Robinhood alternative that doesn't suck dingleberries\",)\n",
            "('What the actual fuck is happening to BB today?',)\n",
            "(\"My final PLTR play, she's been an absolute twat to me\",)\n",
            "('LAST WEEK - \"You fucking morons, those degenerates don\\'t give a fuck about silver. Astroturf distractions of companies they can be tricked into caring about!\"',)\n",
            "('Neither was his wife.',)\n",
            "(\"We don't need this type of protection any more.\",)\n",
            "('\"President Kennedy once again interpreted the Soviet proposals, to sign a peace treaty with Germany as a threat, as part of the world menace allegedly looming over the countries of capitalism.',)\n",
            "('The kiss outraged our friends but it was done and meanwhile had released in me all the remote, exciting premonitions of lust, all the mysterious sensations that I had imagined a truly consummated kiss would convey to me.',)\n",
            "('Among his gangland buddies, he said, were Joseph (Joey) Glimco, a mob labor racketeer, and four gang gambling chiefs, Gus (Slim) Alex, Ralph Pierce, Joe (Caesar) DiVarco, and Jimmy (Monk) Allegretti.',)\n",
            "(\"From a normie who doesn't know shit about shit...Well fucking done!\",)\n",
            "(\"I don't even own the house I'm standing in.\",)\n",
            "('Next big dick play. Fuck RH.',)\n",
            "('a movie about this fiasco would be literally the most stupid fuck thing done in the history of mankind',)\n",
            "('Lets combine our retard strength and make some gain porn together',)\n",
            "('The ape motivation for dark days',)\n",
            "(\"Fly me to the moon or drag me to hell I'll hold the damn line.\",)\n",
            "('Did you just seen this shit ?',)\n",
            "('A humble retard that loves you all',)\n",
            "('Other brokers with no fees - fuck RH',)\n",
            "('\"Maybe in time to make a cross and dig our graves\".',)\n",
            "(\"Call me paper hands but let's be honest, we are retarded.\",)\n",
            "('New here, from mexico, watching shit unfold and ROOTING FOR MY AMIGOS ON REDDIT',)\n",
            "('Why the fuck is BB and NOK Honig down???',)\n",
            "('This is no criticism of them, as they obviously cannot get a half-hour program into a fifteen-minute news summary.',)\n",
            "(\"THIS TYPE OF ENERGY WE NEED - I'm tired of the doomer shit over the last few days, keep the energy up.\",)\n",
            "('The announcement that the secrets of the Dreadnought had been stolen was made in Bow St. police court here at the end of a three day hearing.',)\n",
            "('I went to bed early last night and awoke to this commotion about Robinhood, AMC, Gamestop, etc. This post might get taken down, but what the hell is going on?',)\n",
            "('And I remember that the whole of the privileges, not counting the Beech Pasture, was valued at twenty pounds.',)\n",
            "('The only exception to this is certain bees that have become parasites.',)\n",
            "('im gay for dick kings (dkng)',)\n",
            "('if the succession is not of record, all documents relating to such succession, properly certified, are required.',)\n",
            "('Similar findings have been noted in a patient with congenital absence of the organification enzymes, whose thyroid tissue could only concentrate iodide.',)\n",
            "('To everyone complaining about people talking shit about GME',)\n",
            "('At the same time the orchestra announced that next season it would be giving twenty-five programs at Carnegie, and that it would be taking these concerts to the suburbs, repeating each of them in five different communities.',)\n",
            "(\"Promptly at one-thirty he entered Hohlbein and Garth's elegant suite of offices in Medfield's newest professional building.\",)\n",
            "('Declarative statements',)\n",
            "('CNBC, WSJ, NYT, any other piece of trash media publication...know this',)\n",
            "(\"Non-YOLO options questions because I'm r-tard degenerate newb cunt\",)\n",
            "('Both men knew it was in the Norberg family holdings, but to which of the cousins did it belong, Anta or Freya??',)\n",
            "('$PFE ape',)\n",
            "(\"PSA: Don't join the Wallstreebets discord server - it is next level retarded in there\",)\n",
            "('A bunch of retarded autists have the top 0.01% TERRIFIED of us.',)\n",
            "(\"this sub went to fucking shit and you're all fucking retarded\",)\n",
            "('I am financially ruined and my life is hell thanks to cockroach futures',)\n",
            "('Question for all my BROTHERS and SISTERS, my KILLERS, my IPHONE FUCKING INSURGENTS: ................where the fuck do we go?',)\n",
            "('He attended New York University before switching to Georgetown University in Washington.',)\n",
            "('How to really fuck the hedge funds: educate the world about how you can outperform the hedge funds and why hedge funds will always eventually screw the investors',)\n",
            "('The anode plug (Figure 2) was inserted into a carbon anode holder.',)\n",
            "('Why tomorrow may be the the day to buy $CNK... a potentially epic play to go along with the $AMC piss missile',)\n",
            "('Alright ladies and gentlemen gather around. There is a chance we retail investors can take control of Wall street. This is a crazy and stupid as hell idea that is very flawed but here we go.',)\n",
            "('\"Everything went real smooth\", the sheriff said.',)\n",
            "('Through the Frankfurt Jewish Kulturbund he began to give sonata recitals in synagogues, with Cellist Emanuel Feuermann.',)\n",
            "('shit is so grimy',)\n",
            "('That was some scary shit!',)\n",
            "('Joel knew what he was about, however.',)\n",
            "('Related to this is the fact that most of the higher religions define for the individual his place in the universe and give him a feeling that he is relatively secure in an ordered, dependable universe.',)\n",
            "('(3)',)\n",
            "('The simple pragmatic success of the sociology of small groups needs to be questioned.',)\n",
            "('\"Need a pumpkin to get to the party\"??',)\n",
            "(\"I can't throw.\",)\n",
            "('FUCK ROBINHOOD! $GME up to 500 and beyond, TODAY!! this is pure market manipulation. we have to stick together and give them an even bigger fuck you.',)\n",
            "('These losses depend on fiber diameter and length, absorption coefficient, the mean value of the loss per internal reflection and last, but not least, on the angular distribution of the incident light.',)\n",
            "('Holy shit, 5 trademarks have been filed for WallStreetBets in the last 2 weeks',)\n",
            "('He went into the print shop, where Fletcher had just finished cleaning the press.',)\n",
            "('Secretary of State Seward was a sick man.',)\n",
            "('Lmao who the fuck is this dork',)\n",
            "('This will form the sugar bowl.',)\n",
            "('Inventors joined lawyers in the clamor for reform, inevitably centering upon the Selden litigation as a \"horrible example\".',)\n",
            "('Her eyes were bright with anticipation.',)\n",
            "('Three inches of porous material will do a good job of keeping weeds down and the soil moist and cool.',)\n",
            "('Complete boegginer, wanna buy a GME stock for the memes and because fuck Wall Street. What is the best way to go about it?',)\n",
            "('Am I doing this right? What the fuck happened today?',)\n",
            "('Robinhood is a scam. I know you know but holy hell they actually just scammed me out of $600.',)\n",
            "('They always have a good orchestra\".',)\n",
            "('But once the strike trend hits hoosegows, there is no telling how far it may go.',)\n",
            "('The mean temperature of the surface was then computed according to the following relation: Af where x is the fraction of the plug area covered by the hot spot.',)\n",
            "('Whatever the long-range impact of integration, the owners of Negro-appeal radio stations these days know they have an audience and that it is loyal.',)\n",
            "(\"You wouldn't pull my leg, old man??\",)\n",
            "('In contrast to all this, primary data are data of a self involved in environing processes and powers.',)\n",
            "('It was a hell of a ride boys.',)\n",
            "('The new Riverside pitcher turned out to have an overhand fast ball that took a hop.',)\n",
            "(\"Mrs. Meeker hadn't got around to taking care of that.\",)\n",
            "('The United States must plan to absorb the exported goods of the country, at what he termed a \"social cost\".',)\n",
            "('And so she was, and would remain.',)\n",
            "('Can your plant nurse be replaced by a trained first-aid man who works full-time on some other assignment??',)\n",
            "('What the fuck is going on!',)\n",
            "('Shut the fuck up about GME',)\n",
            "('When you get the chance, make Robinhood pay. If they lose users their IPO goes tits up',)\n",
            "('Where the fuck can I transfer my positions in Robinhood to without any transfer fees or transfer minimums/limits?',)\n",
            "('Brokerage for a retard',)\n",
            "('Roblox $RBLX direct listing - some half ass DD',)\n",
            "('Sold my shitty booming coded currency for some good GME. Feels great to be a monkey brained retard just before a trip to the moon.',)\n",
            "('I know you were apes, but not THAT retarded',)\n",
            "('So many of you retards keep saying to not make this subreddit political and in the next breath accuse the industry of being in a conspiracy cabal to fuck you over.',)\n",
            "('Short the shit out of AAL!!!!',)\n",
            "('What a know-nothing retarded skeptic such as myself is learning from the GME \"Squeeze\"',)\n",
            "('BE FUCKING CAREFUL if your are moving your shit off Robinhood, the process can take over a week and your account will be frozen in the meantime.',)\n",
            "('Flatness may now monopolize everything, but it is a flatness become so ambiguous and expanded as to turn into illusion itself--at least an optical if not, properly speaking, a pictorial illusion.',)\n",
            "('Disclaimers for new diamond-handed, autistic, retarded apes',)\n",
            "(\"3 months ago I posted about NUE, yesterday I saw my tiny account grow larger than it's ever grown before - holy shit my wife's boyfriend might finally leave her for me (also a tax question)\",)\n",
            "('-60% I feel like a true fucking retard.',)\n",
            "(\"He'd told Hank Maguire and Luis Hernandez about his wife's refusal to come with him and about what he now intended to do.\",)\n",
            "('Convenience held key',)\n",
            "('Who else of all the dumbfucks bought amc? Retards together strong! Bring this bitch up!',)\n",
            "('Holy shit, I am on the same side as a Winklevoss',)\n",
            "('$75k GME YOLO SHORT. May the best retard win',)\n",
            "('What the hell are they doing, opening and closing the market...',)\n",
            "('The performances were variable, those of the full ensemble being generally satisfying, some by soloists proving rather trying.',)\n",
            "('But I want this to sink in awhile.',)\n",
            "('\"Listen to me, Leigh.',)\n",
            "('You US retards seriously need to boycott the fuck out of Robinhood. Fuck them. Those of you that made bank need to hire lawyers and file action lawsuits against them.',)\n",
            "(\"But millions of human beings were exposed to Lueger's propaganda and record.\",)\n",
            "('Super retarded idea: Create a culture here that frowns upon Robinhood.',)\n",
            "(\"Now I really need a financial advice. It's not that much fun to be a retard after all.\",)\n",
            "('Generally, however, there is an abundance of available machinery of coordination--in NATO, in O.E.C.D., in the U.N. and elsewhere.',)\n",
            "(\"Budding autist here who never invested before. I'm gonna say fuck it and buy GME stock But the market has closed. When will it open again?\",)\n",
            "('Professor Bondi disagrees with the expansion-contraction theory.',)\n",
            "('Suporting the case for SLV with a chart, historical data and a retard with Microsoft paint',)\n",
            "('Holding my 2 shares from Lebanon! Hold you retarded incels!!',)\n",
            "('The power was off for about five minutes in houses along Smith Street as far away as Fruit Hill Avenue shortly before 5 p.m. when the accident occurred.',)\n",
            "('It seems to have been chosen exclusively from the winners of beauty contests--Miss Omsk, Miss Pinsk, Miss Stalingr oops, skip it.',)\n",
            "('In a succession of scenes they appear in different guises--patrons of a cafe, performers in a circus and participants in a family picnic--but in each instance they inevitably put ugliness before beauty.',)\n",
            "('fuck the idiots calling it shilling.',)\n",
            "(\"Don't doubt how retarded I am. How dare you. I got the a tattoo.\",)\n",
            "('Sponsor quotes John McLendon of the McLendon-Ebony station group as saying that the Southern Negro is becoming conscious of quality and \"does not wish to be associated with radio which is any way degrading to his race;;',)\n",
            "('If you really want to fuck the hedge funds, you will close your RH account today. Otherwise they will become billionaires because of you',)\n",
            "('I am retarded and I love it',)\n",
            "('That was not reasonable either.',)\n",
            "('I could hardly believe such good luck was mine.',)\n",
            "('He stopped automatically at the street corners, waiting for the traffic lights to change, unheeding of other people, his coat open and flapping.',)\n",
            "(\"Yeah I'ma shill but fuck it\",)\n",
            "('The light supper over, Claire went to him and, slipping an arm about his shoulder, sat on his knee.',)\n",
            "('$DASH is trash',)\n",
            "('I am poor as shit.',)\n",
            "('Stop with the ape crap.',)\n",
            "('I was lucky they let me go, I guess\".',)\n",
            "('Investing 101: For WSB fresh meat',)\n",
            "('A hedge fund in London is kicking my ass',)\n",
            "('Can we make a pen for all these retarded newcomer apes hoping to get rich quick?',)\n",
            "('Get in line to fuck RH right back',)\n",
            "('At a 66.6% loss on $GME while the stock is at a nice $69.',)\n",
            "('My lovely caller--Joyce Holland was her name--had previously done three filmed commercials for zing, and this evening, the fourth, a super production, had been filmed at the home of Louis Thor.',)\n",
            "('Replied the Senator.',)\n",
            "(\"Charles Schwab don't give a fuck\",)\n",
            "('Will SPCE be used to fuck the big boys next?',)\n",
            "(\"Hold forever and you're not sending any message except that you are truly retarded.\",)\n",
            "('The Crash is Coming in 69 Days. Why? Because WE LIKE IT',)\n",
            "('There would seem to be some small solace in the prospect that the missile race between nations is at the same time accelerating the study of the space around us, giving us a long-sought ladder from which to peer at alien regions.',)\n",
            "('Offenses multiply',)\n",
            "('We should buy a Super Bowl ad (fuck Robinhood)',)\n",
            "('\"Robinhood\" ???? when did robinhood the character fuck people over like this?',)\n",
            "('Roberta was violently trembling.',)\n",
            "('Her voice was flat and dull.',)\n",
            "('And look what he done give us\".',)\n",
            "('I envy you lucky retard dip buyers',)\n",
            "(\"I'm a new retard and I need to know a few things from any og retards\",)\n",
            "('Robinhood seems intent on market manipulation (allegedly) lets all give them bad reviews until they get their shit together',)\n",
            "('The Irish accent is, as one would expect, combined with slight inflections from the French.',)\n",
            "('Simply out of bloodlust, their murderers dismembered the bodies and tossed the remains into the river.',)\n",
            "('Power and numbers and shit.',)\n",
            "('He\\'s really asking for it\".',)\n",
            "(')',)\n",
            "('What the fuck just happened at the close of Boomer Jones 30 (3.5 trillion in transactions)',)\n",
            "('Cloudflare $NET has made my dick hard for more than 4 hours',)\n",
            "('Who is the retard, really?',)\n",
            "(\"Europoor doing it's part here. Borrowed from everyone to BOY MOAR. Hold, for fuck's sake\",)\n",
            "('Ground meats such as fresh pork sausage and hamburger have a relatively short shelf life under refrigeration, and radiopasteurization might be thought to offer distinctly improved keeping qualities.',)\n",
            "(\"Bill Hwang's firm just went tits up, prime brokers like Goldman Sachs, Morgan Stanley, Credit Suisse, and Nomura still have $22-30 Billion of his books to liquidate\",)\n",
            "('penis',)\n",
            "('An old weakness for burrowing in records rose up to tempt him.',)\n",
            "(\"Can someone please recommend a brokerage that doesn't cater to gay bears?\",)\n",
            "('You guys are unironically retarded for selling RKT',)\n",
            "('As it is written, There is not one just man;;',)\n",
            "('When this occurs, I make the change on the sketch or on the final watercolor--if I have been working on a full sheet in the field.',)\n",
            "('Let not your heart be troubled, neither let it be afraid\".',)\n",
            "('Listen you retarded apes, all you crayon eating autistic idiots need to only do one thing, buy GME and never log into your account again. Set a trading alert on yahoo finance for $500 and then start watching when you receive the alert.',)\n",
            "('Are you retarded? Click here',)\n",
            "('Quint smothered a yawn.',)\n",
            "('One of the many things that was so nice about her was that she always took your questions seriously, particularly your very, very serious questions.',)\n",
            "('On $1.1 billion of 90-day bills, the average yield was 2.325%.',)\n",
            "('No sooner would I turn my head away from the counter before he would address me, at times quite sharply, in order to bring back my attention.',)\n",
            "('fuck it',)\n",
            "('Tailwinds for copper? Why does my butt hurt?',)\n",
            "('So, I have a bit of a retarded conspiracy theory that I am testing today.',)\n",
            "(\"Has anyone else noticed WSB has 400k active users but like fuck all posts per min, Dryer then a nun's gash\",)\n",
            "('When robinhood IPOS im going all in shorting that piece of shit. How can this be remotely legal?',)\n",
            "('\"But you want to ask me when B\\'dikkat is going to come back with the needle\".',)\n",
            "(\"I'm the guy who lost 250k on Luckin Coffee last year and ended up on yahoo finance and marketwatch. I'm still here and more retarded than ever\",)\n",
            "('fuck Robinhood whose buying on cashapp',)\n",
            "('For example, farm equipment shipments of International Harvester in August climbed about 5% above a year earlier, Mr. Keeler reports.',)\n",
            "('Anyone noticing that their significant others sex drive just so happens to follow the GME stock?',)\n",
            "(\"If we can't raise the capital, we're through.\",)\n",
            "('Locking shit down',)\n",
            "('The difference between a retard and a fan',)\n",
            "('\"If you can keep her here that long\", Pete said wryly.',)\n",
            "('Can someone explain to me what the fuck is happening',)\n",
            "('Seeds of the sago palm are used in Bermuda to make heads and faces of dolls sold to tourists.',)\n",
            "('Seated in front of the desk, Hank said, \"I\\'m looking for some information with very little to go on, Sheriff\".',)\n",
            "('Degiro is bidding at 237.75 and asking for 330.66, literally a $100 dollar spread. What the fuck is going on',)\n",
            "('I\\'m just an ape that likes bananas \"An ounce of prevention is worth a pound of cure\"',)\n",
            "('To my great surprise and delight, when they saw the two trees they went rushing off, returning shortly with decorations from their own trees.',)\n",
            "('Is there any public companies that makes butt plugs?',)\n",
            "('Message from a retarded mom',)\n",
            "(\"What is the common man's complaint??\",)\n",
            "('Stated in its simplest terms, the main job of the Planning Division is to plan for the future of the State of Rhode Island.',)\n",
            "('This bitch is just teasing my $61.50 buy order',)\n",
            "(\"You're unironically retarded if you're still using Robinhood\",)\n",
            "('In a frenzy of excitement, he considered his plan.',)\n",
            "('Dumb or just retarded?',)\n",
            "(\"Ex robinhood user's please read this before you fuck up & can't trade.\",)\n",
            "('Nucor Corp (NUE), am I a retard for going all-in?',)\n",
            "(\"Before hitting the pole, Mr. Stone's car brushed against a car driven by Alva W. Vernava, 21, of 23 Maple Ave., North Providence, tearing away the rear bumper and denting the left rear fender of the Vernava car, police said.\",)\n",
            "(\"I'm a new retard and have a bid/ask question\",)\n",
            "('Going long on my dick',)\n",
            "('-- For its final change of bill in its London season, the Leningrad State Kirov Ballet chose tonight to give one of those choreographic miscellanies known as a \"gala program\" at the Royal Opera House, Covent Garden.',)\n",
            "('The night before, they had telephoned the Andrus maid, Selena Masters, and she had arrived early, bursting her vigorous presence into the silent house with an assurance that amused McFeeley and confounded Moll.',)\n",
            "(\"Heads instinctively turned in Willis' direction.\",)\n",
            "(\"UPDATE: The Official Pride Month Gay Bear Thesis - We're here, we're queer, get used to it.\",)\n",
            "('Then it disappeared into the wings.',)\n",
            "(\"Let me tell you about my weird ass dream from last night that relates to GME in a positive way. I swear to God this is not me memeing but I chose the meme flair anyway because I'll probably be wrong.\",)\n",
            "('Wanna fuck a hedge fund?',)\n",
            "('Gamma squeeze may be coming next week. GME technical analysis from a retard, not financial advice',)\n",
            "('I hope someone buys 420 shares at 69, that would be nice.',)\n",
            "('How the fuck did Shorties become Personae non gratae?',)\n",
            "('no more RH for this retard',)\n",
            "('Up to the turn of the century, contraception was condemned by all Christian churches as immoral, unnatural and contrary to divine law.',)\n",
            "('My First YOLO: 18k on $GME (Using combinations of 420 and 69) What could go wrong?',)\n",
            "('But until we have an effective spacecraft, the answer to the hunter-killer problem is manned aircraft.',)\n",
            "('Being an adult ape.',)\n",
            "('\"Don\\'t take it like this\", Frankie said.',)\n",
            "(\"In all the madness this week, I briefly caught wind of Chamath Phlaplaitlatlalyaysas or whatever the fuck floating the idea that he wants to give 100k to the top 10 WSB posters --- here's why I think that's a terrible idea\",)\n",
            "('Any ape speculation for future increasing stocks to fund GME?',)\n",
            "('The group, upon the issuance of its first press release on December 21, 1957, designated itself a \"Committee of Investigation\".',)\n",
            "('The impression was unmistakable that, whatever one may choose to call it, natural law is a functioning generality with a certain objective existence.',)\n",
            "('On the negative side of the balance sheet must be set some disappointment that the United States leadership has not been as much in evidence as hoped for.',)\n",
            "('Buy buy buy all these dips are perfect find anyway to buy stock $GME $AMC $NOK $BB buy it all fuck these people oppressing us we can do wtv tf we want',)\n",
            "('Calhoun shouted.',)\n",
            "(\"Mike took the bayonet from Dean's hand and slashed the picket line.\",)\n",
            "('FORGET ROBINHOOD CLASS ACTION -- if you really want to fuck them... THIS IS HOW WE DO IT!!!!',)\n",
            "('Pressure transducer for pvt measurements.',)\n",
            "('Europoor Holding the line, fuck em.',)\n",
            "('there is room for you, too, Johnnie\".',)\n",
            "('Arm, shoulder, chest, upper and lower back strength will be aided with the Horse kick.',)\n",
            "('it introduces Pimen when he comes before Boris in the last act.',)\n",
            "('But when I saw that it was already ten past seven, I began to wonder if something had gone wrong.',)\n",
            "('Hudson deposed Juet and cut his pay.',)\n",
            "(\"An artistic autistic ape's astrology art album\",)\n",
            "('Baby ape needing some hand holding on GME',)\n",
            "('On their frequent hikes into the nearby mountains, the children carry whole grains to munch along the trail.',)\n",
            "('The cycle of disaster starts the moment they touch any belonging of ours, and dogs them unto the forty-fifth generation.',)\n",
            "('\"Melodious birds sing madrigals\" saith the poet and no better description of the madrigaling of the Deller Consort could be imagined.',)\n",
            "('Watching a real retard on youtube',)\n",
            "('AMC Stock is going really deep in the shit hole almost a %50 Loss',)\n",
            "('This theory eventually proved inexact.',)\n",
            "('Banks, Bonds, and Bears suck for tech! But I think there is a way to play this.',)\n",
            "('Big Business supports slave labour and genocide on the Uigurs in China - yet they claim we\\'re the \"nazis\" because we bought a stock that helps keep a videogame store from going bankrupt? - Am I takking crazy pills??? Am I retard?',)\n",
            "('fuck wallstreet & melvin',)\n",
            "('on Jan. 18 2:37.3--:36.1;;',)\n",
            "('For all practical purposes, the West stands disunited, undedicated, and unprepared for the tasks of world leadership.',)\n",
            "(\"That the perfect continuity was composed from the joblot of memory impressions in the professor's brain, or 2.\",)\n",
            "('Mahzeer would stand up, the prime minister would follow.',)\n",
            "('The court issued a temporary restraining order, directing us to resume referrals.',)\n",
            "('Water',)\n",
            "('Embraced my inner retard',)\n",
            "('\"Nadine was always too good to live in a little house like this!!',)\n",
            "('An Opportunity to Change the World through Crispr(CRSP) Real scifi shit.',)\n",
            "('Can we get a thread for brokers in each country that arent pieces of shit?',)\n",
            "('Please, dear God, make my pilots good, he prayed.',)\n",
            "('Intelligence jabbed at him accusingly.',)\n",
            "('Starting to understand the power of the retard play...',)\n",
            "('How to be less retarded',)\n",
            "('fuck the capital',)\n",
            "('What did one cunt say to the other cunt?',)\n",
            "('Any street meeting, sacred or secular, which he and his colleagues uneasily cover has as its explicit or implicit burden the cruelty and injustice of the white domination.',)\n",
            "('He thought of Simms Purdew, who once had risen at the edge of a cornfield, a maniacal scream on his lips, and swung a clubbed musket like a flail to beat down the swirl of Rebel bayonets about him.',)\n",
            "('The British government, concerned about the threat of unemployment in the shipbuilding industry, had put through a bill to give Cunard loans and grants totaling $50,400,000 toward the $84,000,000 cost of a new 75,000-ton passenger liner.',)\n",
            "('Now under me I could see him for what he really was, a boy dressed up in streaks of paint.',)\n",
            "(\"The fact that we got this far is already extraordinary. All the bitch moves they made yesterday and today show that they are shook. They're worried. It means we are doing something right. Hold. HOLD. HOLLDD\",)\n",
            "('Fellow retards currently putting cream on your raw ass I have data and info that has been unable to get through with the massive influx and lack of karma to post.',)\n",
            "('I have often searched for a graphic way of impressing our superiority on those Americans who have doubts, and I think Mr. Jameson Campaigne has done it well in his new book American Might And Soviet Myth.',)\n",
            "('What the fuck is happening',)\n",
            "('No poetry, no airplanes, no dancers.',)\n",
            "('When go represents itself and a complement (being equivalent, say, to go to Martinique) in which boat did Jack go on??',)\n",
            "(\"It's fun, and it's easy--so easy that there is time left after cooking, and tent keeping, for the women to get out and enjoy outdoor fun with their families.\",)\n",
            "('Valhalla can suck my cocky cunt',)\n",
            "(\"I got scared when the Robbin'hood dropped GME and sold. I feel like shit. Don't be like me.\",)\n",
            "('some DD on MRO, its not perfect and I might even suck at it..... But it does come with a few Facts.... Marathon Oil (MRO)',)\n",
            "('Can you autists shut the fuck up with these \"WhOs StIlL hOlDiNg\" posts.',)\n",
            "('However, if these procedures are applied more often, conditioned emotional responses are temporarily abolished.',)\n",
            "('Generally, the refund may be obtained by filing Form 1040A accompanied by the withholding statement (Form W-2).',)\n",
            "('Mr. Philip Toynbee writes, for example, that \"in terms of probability it is surely as likely as not that mutual fear will lead to accidental war in the near future if the present situation continues.',)\n",
            "('Finally Luis Hernandez said, \"What must be, must be.',)\n",
            "(\"Don't fuck with the goalie.\",)\n",
            "(\"I'll take the middle.\",)\n",
            "('Two very useful ways for modifying a form-dictionary are the addition to the dictionary of complete paradigms rather than single forms and the application of a single change to more than one dictionary form.',)\n",
            "('First trade was $333 - kept buying on the dips and HOLDING. Full retard mode until the end!',)\n",
            "('Reason: the cannery loses $3,000 yearly.',)\n",
            "('Their heights, that is.',)\n",
            "(\"can't buy the dip bc robinhood decided to fuck us!!!!\",)\n",
            "('Why the fuck are we acting like we lost? We made GME go from 4 to 400 and it took combined trading platforms to silence and cheat to bring it to 90.',)\n",
            "(\"I have the feeling GME is getting hotter and hotter, I'd like to burn my ass on it. And YES, it is possible | motivational speech\",)\n",
            "('Fuck your bots, you hedge fund douche bags.',)\n",
            "('Gavin paused wearily.',)\n",
            "('This is the moment that will separate the dumb from the truly retarded.',)\n",
            "('Anyone else suspicious about all of these beta ass weed memes?',)\n",
            "('-- For calves, feed not less than 50 grams of Aureomycin per ton complete feed as an aid in preventing bacterial diarrhea and foot rot.',)\n",
            "('The lawyer with whom I studied law steered me off the Socialist track.',)\n",
            "('Long ass DD on why being long on Macerich will make you rich',)\n",
            "('\"You\\'re awfully kind\", the girl said.',)\n",
            "('A rule on the Federal deductibility of state taxes is contested.',)\n",
            "('AMC to the moon fellow gorillas, correct me if I am too retarded for math',)\n",
            "('Inb4 \"Come on apes we HAVE to close over 60 before closing to fuck hedgie\"',)\n",
            "('hence they commute with D and with N.',)\n",
            "('\"What pictures\"??',)\n",
            "(\"But he didn't play golf, didn't seem to belong to any local clubs--his work took him away a lot, of course--which probably accounted for his tendency to keep to himself.\",)\n",
            "('\"They\\'re all here, back to 1865\", Carruthers told him.',)\n",
            "('Honest retarded question. What do you think of Tencent (TME)',)\n",
            "(\"AMKR - Amkor Technology - A Great Play in the Semi Conductor Space - CRIMINALLY Undervalued, literally can't go tits up!\",)\n",
            "('During the month of November hardly a day passed when there was not some mention of John Brown in the Rhode Island newspapers.',)\n",
            "(\"$ASO - Your YOLOs don't have to be retarded(A picturebook)\",)\n",
            "('Asked Willie at one stage of his excavation project.',)\n",
            "('Message from a TSLA veteran: Shut the fuck up and hold',)\n",
            "('Black and white is her favorite color combination along with lively glowing pinks, reds, blues and greens.',)\n",
            "(\"You can get year-'round air conditioners in the same variety of styles in which you buy a furnace alone--high or low boy, horizontal or counterflow.\",)\n",
            "(\"MNDY and the most retarded IPO pricing you've seen so far\",)\n",
            "('Just a reminder and recap spoken in retard.',)\n",
            "(\"Connally amendment's repeal held step toward world order\",)\n",
            "('Robinhood will fuck you when the short squeeze actually happens...',)\n",
            "('General requirements for land, labor, and equipment are discussed below.',)\n",
            "('What are some other funny ass stock names besides $GAYMF (Galway Metals) or $SLUT (S&P 500 Gold Hedged Index)',)\n",
            "('Gun cleared his throat.',)\n",
            "('Broke bitch wanting to help less broke bitch smack some rich people nuts',)\n",
            "('The dinner hour there was twelve noon.',)\n",
            "('Good facilities would be a decided help to pool operations and probably reduce vehicle costs even more.',)\n",
            "('the verse of Beowulf or of The Iliad and The Odyssey was not easy to create but was not impossible for poets who had developed their talents perforce in earning a livelihood.',)\n",
            "('The Battle of $69 Hold For 11am - Then We Be Captain Through Lunch',)\n",
            "(\"If everyone would follow the posting rules this place wouldn't be such a shit show for the last few weeks.\",)\n",
            "(\"You're up against it anyhow.\",)\n",
            "('My retarded prediction on what happens tomorrow with $GME$',)\n",
            "('He is innocent\".',)\n",
            "('PSA: Get the fuck off WSB this weekend',)\n",
            "('Stuck in RobinHood hell',)\n",
            "(\"Why naked shorting isn't needed to fuck the system\",)\n",
            "('To the moon or not - i will fly the damn rocket - holding GME',)\n",
            "('Other stocks/full retard.',)\n",
            "('The kid\".',)\n",
            "('He had to teach himself patiently that these traps were not for him.',)\n",
            "('Sec. 3.',)\n",
            "('Nowadays, we talk as though the blitz were just a short skirmish.',)\n",
            "('\"Drink, you son of a bitch\"!!',)\n",
            "('\"How do the valley people feel\"??',)\n",
            "('The Aricaras broke under the devastating fire, wheeled and retreated.',)\n",
            "(\"Don't buy RBLX unless you're a retard. I'm a retard\",)\n",
            "('The president who appoints strong men who have an all-college or university point of view and a talent and respect for administration can count on useful assistance.',)\n",
            "('Evidently, everyone has shit themselves',)\n",
            "('28.',)\n",
            "('It was the same old routine.',)\n",
            "('A strategy for GME fomo aka fuck the market makers too',)\n",
            "('For all of you retarded apes talking about the $GME $AMC short squeezing to the millions',)\n",
            "('fuck this',)\n",
            "(\"I'm broke and retarded but I want in on GME\",)\n",
            "('Imma retard',)\n",
            "('On top of an apparent movie deal, this shit comes across my feed',)\n",
            "(\"Sorry to sound like a retard but how did y'all screw over Wall Street?\",)\n",
            "('Oil companies are extremely undervalued and is a perfect contrarian investment. Read before you go ape on me.',)\n",
            "('Most children love the animated puppet faces and their flexible bodies, and they prefer to see them as though the puppets were in action, rather than put away in boxes.',)\n",
            "('The crayons called to me, boys. Am I retarded?',)\n",
            "('r/pennystocks can go fuck themselves! lets make Wallstreet bleed!',)\n",
            "('My commitment to going full retard. Day 2/30.',)\n",
            "(\"I'm holding, because I don't know shit about the stock market, but I feel like...\",)\n",
            "('Then from the branches of a near-by tree an Indian underclassman, disdaining both the platform and the English language, harangued the assemblage in his aboriginal tongue.',)\n",
            "(\"Jesus replied,' let it be so for the present;;\",)\n",
            "('What the actual fuck is going on with TDA',)\n",
            "('Robinhood is doing shady shit with other stonks also.. $BYDDY',)\n",
            "('But what is it??',)\n",
            "('Hmmmmm I must be retarded? Diamond Hands till I die for HYLN!!!!!',)\n",
            "(\"Old Gypsy Proverb: When you're drunk, the old whore taste like a virgin.\",)\n",
            "('My friend is a paper handed pussy. How do I get him diamond hands?',)\n",
            "('We have 2,500 such projects, and they add up to a lot more than just roads and wells and schools.',)\n",
            "('Sign me in fellow retarded apes. This is only the begining for GME and AMC',)\n",
            "('To the extent that the jurisdictional principle of 1875 stands unmodified by subsequent legislation, federal equitable relief against state action must be available--or so it seems to Mr. Justice Frankfurter.',)\n",
            "('Drill a No. 43 hole through the pieces and secure with a 2-56 nut and screw.',)\n",
            "(\"Charles Schwab is hiding a giant ball of shit and won't come out and say what happened. How big is the ball of shit?\",)\n",
            "('Trading 212 wont give this retard an account :(',)\n",
            "('Look at this lazy monkey -',)\n",
            "('These things may be happening many miles away from us but really they are right next door.',)\n",
            "('Last two to be added before the book went to press were the marriages of Meredith Jane Cooper, daughter of the Grant B. Coopers, to Robert Knox Worrell, and of Mary Alice Ghormley to Willard Pen Tudor.',)\n",
            "('GME - Someone please explain to a full-featured retard why volume is so low right now?',)\n",
            "('Lord laughed with secret amusement.',)\n",
            "('All you people hard coping with GME right now. Stop making shit up and instead help the company.',)\n",
            "('Fucked in the ass on my birthday.',)\n",
            "('YOLO Update - added more to my position in $HYLN bringing the total to 26,750 shares. I am all in on this train! What do you get when you multiply a retard by another retarded action? You get a DIAMOND HANDED APE!!!!!',)\n",
            "('David Portnoy is a piss poor trader lmao',)\n",
            "(\"I don't know what the fuck I'm doing\",)\n",
            "('In need of Confirmation of these damn Shorts',)\n",
            "('The fuck happened here... We were supposed to go to the moon..',)\n",
            "('She set out to make sure that no Jewish child anyplace in the world had to live in a place such as this\".',)\n",
            "('Seigner, however, is a fine actor and probably the busiest man in the company;;',)\n",
            "('The armchair traveler preserves his illusions\".',)\n",
            "('nobody should be selling jack shit',)\n",
            "('If Melvin & Citron actually closed their short positions on $GME, who the hell is being bought against?',)\n",
            "('Everyone mad at robinhood, but TDameritrade is doing the same shit.',)\n",
            "(\"Well.... That's a fuck fo a way to find out BofA has mutal reprocity agreements with Kuwait Finance House banking.\",)\n",
            "('Crystallographic, physical, optical, and chemical properties.',)\n",
            "('I am sick and fucking tired of the gamestop retailer FUD. The US is not in some magical fucking digital age. Read on for some actual retarded fundamentals.',)\n",
            "(\"TLRY - Breakout may be coming, I'm jacked to the tits\",)\n",
            "('How $HIMS will revolutionize medicine and telehealth. Anticipation for the upcoming earnings report in mid august has already got my dick hard',)\n",
            "('How to not be a dick to your loved ones after a bad day.',)\n",
            "('possible gay bear play for tendies',)\n",
            "('Some call me a whore. I prefer to be called a prostitute.',)\n",
            "('Time to go full retard on stonks again',)\n",
            "('No one knows jack about shit!!',)\n",
            "('Think logically about silver (not a bot 420 69 heehee)',)\n",
            "('How this sub fucked newbies. TL;DR: We need to call out the difference between retarded DD and desperate denying reality DD',)\n",
            "('This ape got lucky. How do I stop RH from fucking me??',)\n",
            "('By reminding ourselves of these factors in the situation, we should, I am sure, come to a fresh realization, however painful it be, that the battle between Parker and his neighbors was fought in earnest.',)\n",
            "('London explains that the very distinct directional effect in the Phase 4 series is due in large part to their novel methods of microphoning and recording the music on a number of separate tape channels.',)\n",
            "('This market manipulation should not STAND!!!!! CALL YOUR CONGRESSMEN AND MAKE A COMPLAINT! fuck robinhood and fuck the hedgefunds.',)\n",
            "('PLTR DD - brain cells required if you are an ape!',)\n",
            "('Yet the whole of Anne was something she had never learned in any college.',)\n",
            "('The red glow from the cove had died out of the sky.',)\n",
            "('After I paid Monsieur Prieur for Dandy, I brought him home, but he was ill at ease and ran away the same night.',)\n",
            "('This sub went to shit',)\n",
            "('GME Volume question from a stupid ape',)\n",
            "('So apparently RobinHood is trash, where should a complete new person start to drop a few hundred into this trash fire?',)\n",
            "('He told me to wake you\".',)\n",
            "('A few newer homes have Western flush toilets, but even with running water, they are usually Eastern style.',)\n",
            "('All orders originate with the officer of highest rank and terminate with action of the men in the ranks.',)\n",
            "('I might be a real retard. Sold at +-40% loss, without knowing.',)\n",
            "(\"On motion of the Amici Curiae, the court directed that a ruling be obtained from the Commissioner of Internal Revenue as to the federal income tax consequences of the Government's plan.\",)\n",
            "('Why Im holding: I dont know enough about the market, but I do know enough about marketing and Astroturf campaigns. Holy shit there are lot of them going on.',)\n",
            "(\"Some fund is trying to pay a WSBer $200k+ to do FD's and they are too retarded to post it here. Apply here.\",)\n",
            "('Many ape bagholders',)\n",
            "('Now the park is filled with marble busts and all the streets in the immediate area have the full and proper names of the men who fell.',)\n",
            "('The rise of the giant corporations in Western Europe and the United States dates from the period 1880-1900.',)\n",
            "('Helion, however, clung to the belief that \"in escaping from the Stalag I had also escaped from Abstraction\".',)\n",
            "('Big dick idea - can the USA government just invest in AMC and GME now and wipe out our national debt?',)\n",
            "('In carrying out this program science has undoubtedly performed a very considerable service for which it can claim due credit.',)\n",
            "('Only four towns indicated that they made any more than a normal effort to list property of this kind.',)\n",
            "('Historians have traditionally regarded the great debates of the Seventeen Nineties as polarizing the issues of centralized vs. limited government, with Hamilton and the nationalists supporting the former and Jefferson and Madison upholding the latter position.',)\n",
            "('When he had finished he led him and the mare to the porch.',)\n",
            "('Publisher Richardson has updated the Blue Book \"but it still remains the compact reference book used by so many for those ever-changing telephone numbers, addresses, other residences, club affiliations and marriages\".',)\n",
            "('Inflation my ass.',)\n",
            "('She smiled at Winston, and he saw the hateful hard glitter in her eyes.',)\n",
            "('The question is: what are we going to do about them??',)\n",
            "('In an over-all ASW concept, dependence on and effort expended for such systems should be limited to those with proven capabilities.',)\n",
            "('What do i (ape youngling) need to do one GME takes off to the Moon to take Max amount of bananas from citadel.',)\n",
            "('I met Claire, which was better.',)\n",
            "('The adolescent experiences two closely related crises: self-certainty vs. an identity consciousness;;',)\n",
            "('The basic truth in the reactionary response is to be found in its realistic assumption of the primacy of the real over the ideational.',)\n",
            "('Does anybody wanna talk about what the fuck just happend with SPY?',)\n",
            "('So my monkey brain tried some \"visual analysis\" (aka crayon art) in order to predict the future ... OTHER APES PLSS HELP!!',)\n",
            "(\"HFs played ya'll like a damn fiddle\",)\n",
            "('AMA Request: The retard that sunk his dental school tuition loan into GME @ $300 a share',)\n",
            "('She looked more like twenty-five or six.',)\n",
            "('More than a streak had ended.',)\n",
            "('Men spit on Him.',)\n",
            "('Is there an easy way for a south American to enter the retard train?',)\n",
            "('Other good miles have been by Debonnie (Dale Frost-Debby Hanover) and Prompt Time (Adios-On Time) in 2:28-:36;;',)\n",
            "('You autistic monkees turned this crayon eating retard into a more stable more mature father!',)\n",
            "(\"Solly Hemus announced a switch in his starting pitcher, from Bob Gibson to Ernie Broglio, for several reasons: 1 Broglio's 4-0 won-lost record and 1.24 earned-run mark against Pittsburgh a year ago;;\",)\n",
            "(\"But these prolusions that we have surviving from the Christ's College days are only one phase of his existence then.\",)\n",
            "('Sex was both.',)\n",
            "('NOK Announces 10G rollout at 9AM. Its gonna be a hell of a ride today.',)\n",
            "('$CLF, China, and Hookers: A love story (and some ape DD)',)\n",
            "('Doge~A to the damn moon! Make my poor retard A rich!!!!!! Yeahhhhhhhhhh',)\n",
            "('He had stood at a little distance, studying her, as though he would walk around next and look at the back of her head.',)\n",
            "('The revised draft was mailed in July, 1960, to 100 firms throughout the United States.',)\n",
            "('@robinhoodapp ending trading in GME because they are losing their ass on these trades!',)\n",
            "('How can we fuck Robinhood as hard as possible? We want REVENGE!!',)\n",
            "('When they first married he had been working in the fields all day, and she would get in the car and drive to wherever he was working, to take him a fresh hot meal.',)\n",
            "('What the hell is going on with this sub????',)\n",
            "(\"We'll not talk out of one side of our mouth in Morris County and out of the other side in Hudson.\",)\n",
            "(\"Pulling back the sheet, I examined the bruises around Julia Buck's once slender throat.\",)\n",
            "(\"DeepFuckingValue at the congressional hearing: Your excellencies. So I noticed they went full retard with the shorts. Perhaps some seriously illegal shit. You don't go full retard unless you have fuck you money. I like the stock. .\",)\n",
            "(\"I'm actually retarded and didn't buy the dip\",)\n",
            "('And she said that after this man had been dead for a week she had gone to Reuveni and accepted his proposal.',)\n",
            "('Demand for parts for home entertainment was strong in the first half, but purchases were cut back to lower levels during the fall as set manufacturers reduced their own operating rates.',)\n",
            "('Vegetables are served liberally.',)\n",
            "('Treat lambs with 12 grams per head for lambs weighing up to 50 pounds;;',)\n",
            "('He said Pike had stolen mules from Harris during the Santa Fe expedition;;',)\n",
            "('As examples at GE: Glen B. Warren, formerly manager of the Turbine Division, widely recognized as a turbine designer.',)\n",
            "('He was conscious of a growing sense of absurdity.',)\n",
            "('Feeing like a real retard',)\n",
            "('I took a deep breath, and the plunge.',)\n",
            "('The small intestine and colon contained approximately 300 cc. of foul-smelling, sanguineous material, and the mucosa throughout was hyperemic and mottled green-brown.',)\n",
            "('How in the hell do these mofuxckers blocked \"buy\" and keep sell are they stupid?',)\n",
            "(\"CCS- The Company Making a Shit Ton of Cash Currently, and that will make a metric shit ton of cash in the future. The Deeply Undervalued Stock That's The Supreme Anti Inflation Play.\",)\n",
            "('We must meet this situation by promoting a rising volume of exports and world trade.',)\n",
            "('The Augusta National Golf Club Course got up and bit both Player and Palmer.',)\n",
            "(\"Why are we selling BB? That's the best stock fundamentally out of all these shit shows lol\",)\n",
            "('shotshells--to break the little Targo clay targets.',)\n",
            "('In his own state of New York, the two Democratic bellwethers, State Leader Hill and Tammany Boss Murphy, were saying nothing openly against Hearst but industriously boosting their own favorites, Murphy being for Cleveland and Hill for Parker.',)\n",
            "('Lmao look at this shit',)\n",
            "('Some look deliberately to devices used by creators in the other arts and apply corresponding methods to their own work.',)\n",
            "('The knights for Warwickshire in this parliament, which ended its session on February 9, were Fulke Greville (the poet) and William Combe of Warwick, as Fulke Greville and Edward Greville had been in 1593.',)\n",
            "('she fits into the general scheme well enough.',)\n",
            "('The autist who has bad timing and always hears shit lastt',)\n",
            "(\"More important is the simple human point that all men suffer, and that it is a kind of anthropological-religious pride on the part of the Jew to believe that his suffering is more poignant than mine or anyone else's.\",)\n",
            "('What the fuck RH???',)\n",
            "('\"As Mrs. Morris may be in some want before that time\", Lafayette continued, \"I am going to trouble you with a commission which I beg you will execute with the greatest secrecy.',)\n",
            "(\"Alibaba DD Becuase, I'm sick of GME/AMC/PLTR..... (insert meme shit)\",)\n",
            "('Every plane that could fly was sent into the air.',)\n",
            "('What the fuck is going on',)\n",
            "('They have taken our tendies and used our ass for dipping sauce. Spicy buffalo sting will remind me to never give up.',)\n",
            "('Most orthodontists require an initial payment to cover the cost of diagnostic materials and construction of the appliances, but usually the remainder of the cost may be spread over a period of months or years.',)\n",
            "('It\\'s a matter of boredom, and the subconscious feeling that she is entitled to something, because she\\'s being deprived of something else\".',)\n",
            "('\"His interests range from astronomy and geology to electronics, tennis and swimming.',)\n",
            "(\"Just made my first investment and bought a few shares of AMC... I have no idea what I'm doing, can someone please explain whats going on? I'm retarded.\",)\n",
            "(\"What the hell is happening, can someone explain? I'm brand new to this.\",)\n",
            "('Hedge funds can eat shit.',)\n",
            "('Meltzer was a boarder with the Banks family.',)\n",
            "('He rejects all subjectively motivated continuity, any line of action related to the concept of cause and effect.',)\n",
            "('Some ape from r/GME with serious DD',)\n",
            "('Oh shit. I was fucking around and I bought two shares of AMC on cash app. It worked!',)\n",
            "('Adjustment of fiscal year.',)\n",
            "('There was no process of trial and error.',)\n",
            "('Usually a veteran has to hang himself to get space on the front page.',)\n",
            "('For the love of Harambe, learn the fuck up about options before you enter a trade',)\n",
            "(\"The day's rain had been added to the stagnant water.\",)\n",
            "('Trading halts are normal, stop making use look dumb, we are retarded. Get it right.',)\n",
            "('Enlighten this retard.',)\n",
            "(\"She didn't want to be the only one with a stove in her room, especially as her life span was nearly run out anyway, and she insisted that Hope have the heater.\",)\n",
            "(\"I'm a new retard and have a bid/ask question\",)\n",
            "('there were two or three colored maids employed there.',)\n",
            "('$CLF may be a boomer ape hybrid - first of its kind',)\n",
            "('Shocked at the response to our proclamations, we grow more defensive, and worse, we lose our sense of humor and proportion.',)\n",
            "(\"It's never too late, retarded here GME YOLO\",)\n",
            "('GME AT 69, NOICE',)\n",
            "('We know that in the C-plane both C and Af are analytic.',)\n",
            "('A little man with a \"a dark copper color\" skin, he was wearing \"calico trousers and a white cotton short gown\".',)\n",
            "('Help a confused retard?',)\n",
            "('Going downstairs with the tray, Winston wished he could have given in to Miss Ada, but he knew better than to do what she said when she had that little-girl look.',)\n",
            "('fuck robinhood',)\n",
            "('Thornburg popped in to advise, \"Andy, Skolman\\'s sending up smoke signals.',)\n",
            "('An Italian poet had noticed plainclothes policemen lounging around the area of Quirinal Palace, the first time since the war.',)\n",
            "('Close your WSB insta, your Twitter , your chat channels. Close all that shit',)\n",
            "('They have little \"esprit de corps\".',)\n",
            "('I sought out the gardener and asked him what he did to produce such beauties in that weather.',)\n",
            "('\"He had four children, two sets of twins.',)\n",
            "(\"A bit delayed here in Oz but I'm fucking in you retarded cunts.\",)\n",
            "('Presumably the same sun was shining upon little Drew also, and those who had kidnapped him.',)\n",
            "('Real talk, which is less ass: Fidelity or Vanguard',)\n",
            "('You\\'ve been riding on a pink ticket for six years, you know that\".',)\n",
            "('Hopefully retard here: Somebody takes GME private!',)\n",
            "('GME $69 AMC $6.9 I smell a sexy buying opportunity.',)\n",
            "('Suffice it to say that the usefulness of the latter apportionment is questionable.',)\n",
            "(\"It's too late now.\",)\n",
            "('Carruthers crossed the room to a metal door with an open grillework in the top half.',)\n",
            "('T212 can go fuck itself',)\n",
            "('We can, however, maximize its expected value.',)\n",
            "('WSB veterans, thanks for being retarded',)\n",
            "('It is remembered and has been commemorated by a bust in a park and a square in the city which was renamed Piazzo Lauro Di Bosis after the war.',)\n",
            "('How do I get into this? Shit look fun as hell and I need bread',)\n",
            "('Is this strategy for GME retarded?',)\n",
            "('(Here an entry is a form plus the information that pertains to it.',)\n",
            "('Trading App?? Since Robinhood is shit',)\n",
            "('The fuck is happening to amc retards?',)\n",
            "('The proposal, Sheets said, represents part of his program for election reforms necessary to make democracy in New Jersey more than a \"lip service word\".',)\n",
            "('Why the fuck is there actual porn on here now?',)\n",
            "('Most genius or retarded play',)\n",
            "('Early in the war it was not uncommon for planters\\' sons to retain in camp Negro \"body servants\" to perform the menial chores such as cooking, foraging, cleaning the quarters, shining shoes, and laundering clothes.',)\n",
            "('Q: How do you catch a monkey?',)\n",
            "('wetting a wart with this saliva on wakening the first thing in the morning was supposed to cause it to disappear after only a few treatments, and strangely enough many warts did just that.',)\n",
            "('Of these states the average \"change-over\" point (at which a car is substituted for allowances) is 13,200 miles per year.',)\n",
            "('A True Autists DD on why r/wallstreetbets is now forever shit.',)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/text.py:180: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 400000 word vectors.\n",
            "model fitting - attention Bi-LSTM network\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 54)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 54, 100)      376800      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 54, 200)      160800      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " att_layer (AttLayer)           ((None, 200),        0           ['bidirectional[0][0]',          \n",
            "                                 (None, 3),                       'input_2[0][0]']                \n",
            "                                 (None, 54))                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 2)            402         ['att_layer[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 538,002\n",
            "Trainable params: 538,002\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "14/14 [==============================] - 14s 373ms/step - loss: 0.5891 - acc: 0.6964 - val_loss: 0.4595 - val_acc: 0.8070\n",
            "Epoch 2/500\n",
            "14/14 [==============================] - 2s 148ms/step - loss: 0.3711 - acc: 0.8526 - val_loss: 0.3100 - val_acc: 0.8596\n",
            "Epoch 3/500\n",
            "14/14 [==============================] - 2s 133ms/step - loss: 0.2125 - acc: 0.9212 - val_loss: 0.1856 - val_acc: 0.9240\n",
            "Epoch 4/500\n",
            "14/14 [==============================] - 2s 134ms/step - loss: 0.1230 - acc: 0.9577 - val_loss: 0.1520 - val_acc: 0.9415\n",
            "Epoch 5/500\n",
            "14/14 [==============================] - 2s 133ms/step - loss: 0.0906 - acc: 0.9635 - val_loss: 0.1506 - val_acc: 0.9532\n",
            "Epoch 6/500\n",
            "14/14 [==============================] - 2s 132ms/step - loss: 0.0550 - acc: 0.9854 - val_loss: 0.1229 - val_acc: 0.9591\n",
            "Epoch 7/500\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.0369 - acc: 0.9927 - val_loss: 0.1280 - val_acc: 0.9591\n",
            "Epoch 8/500\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.0210 - acc: 0.9985 - val_loss: 0.1259 - val_acc: 0.9649\n",
            "Epoch 9/500\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.0136 - acc: 0.9985 - val_loss: 0.1271 - val_acc: 0.9649\n",
            "Epoch 10/500\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.0102 - acc: 0.9985 - val_loss: 0.1387 - val_acc: 0.9649\n",
            "Epoch 11/500\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.1592 - val_acc: 0.9708\n",
            "Epoch 12/500\n",
            "14/14 [==============================] - 2s 133ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.1567 - val_acc: 0.9649\n",
            "Epoch 13/500\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.1728 - val_acc: 0.9649\n",
            "Epoch 14/500\n",
            "14/14 [==============================] - 2s 134ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.1801 - val_acc: 0.9649\n",
            "Epoch 15/500\n",
            "14/14 [==============================] - 2s 134ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.1816 - val_acc: 0.9649\n",
            "Epoch 16/500\n",
            "14/14 [==============================] - 2s 134ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.1897 - val_acc: 0.9649\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f21382f1e50>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys, os, importlib, re, tensorflow.python.keras.engine\n",
        "from collections import defaultdict\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Dense, Input, Flatten, Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, Embedding, Layer, InputSpec\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras import initializers\n",
        "\n",
        "os.environ['KERAS_BACKEND']='theano'\n",
        "\n",
        "#Parameters\n",
        "MAX_SEQUENCE_LENGTH = 54\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "valid_lengths = []\n",
        "split_input = []\n",
        "\n",
        "for text, label in total_loader:\n",
        "  print(text)\n",
        "  # label, _, _ = items\n",
        "  texts.append(text[0])\n",
        "  labels.append(label[0])\n",
        "  valid_lengths.append(len(text[0].split(' ')))\n",
        "  split_input.append(text[0].split(' '))\n",
        "    \n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
        "split = pad_sequences(split_input, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post', dtype = object, value = '_PAD_')\n",
        "\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "\n",
        "valid_lengths = np.array(valid_lengths)\n",
        "split = np.array(split)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "valid_lens = valid_lengths[indices]\n",
        "split = split[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "\n",
        "texts_for_print = np.array(texts)\n",
        "texts_for_print = texts_for_print[indices]\n",
        "\n",
        "\n",
        "x_train = data[:-nb_validation_samples]\n",
        "y_train = labels[:-nb_validation_samples]\n",
        "x_val = data[-nb_validation_samples:]\n",
        "y_val = labels[-nb_validation_samples:]\n",
        "vallen_train = valid_lens[:-nb_validation_samples]\n",
        "vallen_val = valid_lens[-nb_validation_samples:]\n",
        "split_train = split[:-nb_validation_samples]\n",
        "split_val = split[-nb_validation_samples:]\n",
        "\n",
        "class AttLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.init = initializers.get('normal')\n",
        "        super(AttLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape)==3\n",
        "        self.W = self.init((input_shape[-1],))\n",
        "        self.trainable_W= [self.W]\n",
        "        self.sm = tf.keras.layers.Softmax(axis = -1)\n",
        "        super(AttLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None, splited_input = None, answer = None, batch_size = None):\n",
        "        eij = K.tanh(K.squeeze(K.dot(x, K.expand_dims(self.W)), axis=-1))\n",
        "        masking = np.array([range(x.shape[1])])<mask\n",
        "        ai = self.sm(eij, masking)\n",
        "        ai_result = tf.argsort(ai, direction = 'DESCENDING',)\n",
        "        weights = ai/tf.expand_dims(K.sum(ai, axis = 1), 1)\n",
        "        weighted_input = x*tf.expand_dims(weights,2)\n",
        "        return K.sum(weighted_input, axis = 1), ai_result[:, :3], ai\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])\n",
        "\n",
        "GLOVE_DIR = \"./\"\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Total %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "\n",
        "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        \n",
        "#Input Declartion\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "valid_len_input = Input(shape = (1, ), dtype = 'int32')\n",
        "\n",
        "#Model structure\n",
        "embedded_sequences = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)(sequence_input)\n",
        "lstm_out = Bidirectional(LSTM(100, return_sequences=True))(embedded_sequences)\n",
        "att_out, _, _ = AttLayer()(lstm_out, mask = valid_len_input)\n",
        "preds = Dense(2, activation='softmax')(att_out)\n",
        "#Early stopper\n",
        "callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=10,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        ")\n",
        "#Model Train\n",
        "model = Model([sequence_input, valid_len_input], preds)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "print(\"model fitting - attention Bi-LSTM network\")\n",
        "model.summary()\n",
        "model.fit([x_train, vallen_train], y_train, validation_data=([x_val, vallen_val], y_val),\n",
        "          batch_size = 50 , epochs = 500, callbacks=[callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('./model_save_check')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6ewjLYZ1EFo",
        "outputId": "984de3fd-7edc-4e41-e484-24b3341553d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as softmax_layer_call_fn, softmax_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ./model_save_check/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ./model_save_check/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f213a560c10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f213a53aa90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r model_save_check.zip model_save_check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9Gp8FTk1n50",
        "outputId": "77687377-71f8-411a-e89c-93460dee462e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: model_save_check/ (stored 0%)\n",
            "  adding: model_save_check/assets/ (stored 0%)\n",
            "  adding: model_save_check/saved_model.pb (deflated 91%)\n",
            "  adding: model_save_check/keras_metadata.pb (deflated 89%)\n",
            "  adding: model_save_check/variables/ (stored 0%)\n",
            "  adding: model_save_check/variables/variables.index (deflated 66%)\n",
            "  adding: model_save_check/variables/variables.data-00000-of-00001 (deflated 12%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Test_uniform_df = pd.read_csv(\"/content/CS376_Final_Project/datasets/2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.2.csv\",sep=',')\n",
        "Test_uniform_df.dropna(inplace=True)\n",
        "Test_uniform_df = Test_uniform_df[['text','label']]\n",
        "Test_uniform_df_hate = Test_uniform_df.loc[Test_uniform_df['label'] == 'hate',:]\n",
        "Test_uniform_df_nothate = Test_uniform_df.loc[Test_uniform_df['label'] == 'nothate',:]\n",
        "Test_uniform_df = pd.concat([Test_uniform_df_hate[:200], Test_uniform_df_nothate[:200]], axis=0).reset_index(drop=True)\n",
        "#Test_uniform_df = Test_uniform_df.sample(frac=1).reset_index(drop=True)\n",
        "Test_uniform_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in Test_uniform_df[\"label\"]]\n",
        "\n",
        "print(Test_uniform_df)\n",
        "\n",
        "texts_test = list(Test_uniform_df['text'])\n",
        "results_True = list(Test_uniform_df['label'])"
      ],
      "metadata": {
        "id": "Z0-sc0kPucSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d76a36b7-a197-4c42-f33e-8fa6f1e40807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  text  label\n",
            "0    Any of those gas chambers still available? I w...      0\n",
            "1    He will never propose to you, when you get old...      0\n",
            "2    there is no difference between white women and...      0\n",
            "3    The world would be so much better if we were d...      0\n",
            "4    comparing maternity leave with paternity leave...      0\n",
            "..                                                 ...    ...\n",
            "395  I read an article where black men said it can ...      1\n",
            "396  Honestly, I was pretty shocked to hear that sh...      1\n",
            "397  I still believe all of that is because most of...      1\n",
            "398  you can not come with us because you are weari...      1\n",
            "399                               some men are abusers      1\n",
            "\n",
            "[400 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# texts_test = [\n",
        "#               \"Blacks should suicide\",\n",
        "#               \"Chinese are ugly\",\n",
        "#               \"Fuck kill jew\",\n",
        "#               \"white trash go home\",\n",
        "#               \"Blacks are cockroaches\",\n",
        "#               \"hey what are you doing? are you fucking retarded? go back to your country. There is no place for stupid people like you\",\n",
        "#               \"People are dull and like simple things unconditionally.\",\n",
        "#               \"hey what are you doing? are you fucking retarded? go back to your country\",\n",
        "#               \"it tastes so fucking good\",\n",
        "#               \"Why can't you resolve this? you are fucking retarded, Get the fuck out\",\n",
        "#               \"I dont enjoy chinese food\", \n",
        "#               \"I fucking hate Korean food\",\n",
        "#               \"Chinese are cockroaches, and Japanese are monkey. All asians should be deleted\" ,\n",
        "#               \"all indians smell like curry\", \n",
        "#               \"Twatters gonna twat. Muslims gonna bomb. Fuck\", \n",
        "#               \"oh wow, fat black bitch short of breath raging about her special rights to spray her monkey looking graffiti\", \n",
        "#               \"The motherless idiot will suicide soon\",\n",
        "#                \"Hey what the fuck do you think you are doing? are you out of your fucking mind? Get the fuck out.\",\n",
        "#                \"Go fucking yourselves man you niggas smell so bad\",\n",
        "#               \"All white trashes are fucking retarded. They aren't capable of doing anything\",\n",
        "#               \"Go to hell you fucking pice of shit\",\n",
        "#               \"All fucking white trashes are retarded\",\n",
        "#               \"You guys are fucking lame\",\n",
        "#               \"You smell like fucking ass, go kill yourself\",\n",
        "#               \"Fuck I’m so done with this shit\",\n",
        "#               \"Hey whats fucking wrong with you?\",\n",
        "#               \"Chinese food tatse fucking bad\",\n",
        "#               \"Shut the fuck up please\",\n",
        "#               \"Why are you acting like a fucking pussy?\",\n",
        "#               \"Stop bitching about how your day was\",\n",
        "#               \"I fucking hate uneducated. They are so uncivilized\",\n",
        "#               \"Fuck exams. I’m done with them\",\n",
        "#               \"Fuck yourself. You are trash of this society\",\n",
        "#               \"Fucking Asians are more stupid than Europians\",\n",
        "#               \"I was on my way to the bar and some old fuck asked me for help haha\"\n",
        "# ]\n",
        "\n",
        "# texts_test = [\n",
        "#               \"Go to hell you fucking pice of shit\",\n",
        "#               \"All white trashes are fucking retarded\",\n",
        "#               \"You guys are fucking lame\",\n",
        "#               \"You smell like fucking ass, go kill yourself\",\n",
        "#               \"Fuck I’m so done with this shit\",\n",
        "#               \"Hey whats fucking wrong with you?\",\n",
        "#               \"Chinese food tatse fucking bad\",\n",
        "#               \"Shut the fuck up please\",\n",
        "#               \"Why are you acting like a fucking pussy?\",\n",
        "#               \"Stop bitching about how your day was\",\n",
        "#               \"I fucking hate chinks. They are so uncivilized\",\n",
        "#               \"Fuck exams. I’m done with them\",\n",
        "#               \"I was on my way to the bar and some old fuck asked me for help haha\"]\n",
        "\n",
        "# data = 'Blacks should suicide'\n",
        "# texts_test = [data]\n",
        "valid_lengths_test = []\n",
        "split_input_test = []\n",
        "for text in texts_test:\n",
        "  valid_lengths_test.append(len(text.split(' '))) \n",
        "  split_input_test.append(text.split(' '))\n",
        "sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
        "split_test = pad_sequences(split_input_test, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post', dtype = object, value = '_PAD_')\n",
        "\n",
        "indicess = np.arange(data_test.shape[0])\n",
        "# np.random.shuffle(indices)\n",
        "\n",
        "valid_lengths_test = np.array(valid_lengths_test)\n",
        "split_test = np.array(split_test)\n",
        "textss = np.array(texts_test)\n",
        "\n",
        "x_test = data_test[:]\n",
        "vallen_test = valid_lengths_test[:]\n"
      ],
      "metadata": {
        "id": "_75E0C4vp5Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_fil_tgt = K.function([model.layers[0].input, model.layers[3].input],\n",
        "                                  [model.layers[4].output, model.layers[5].output])\n",
        "x = [x_test, vallen_test]\n",
        "[_, fil_tgt, ai], preds = get_fil_tgt(x)\n",
        "result = preds.argmax(axis = -1)\n",
        "print(result)\n",
        "\n",
        "# for i in range(len(fil_tgt)):\n",
        "#   # print(fil_tgt[0][1][i])\n",
        "#   if result[i] == 0 and results_True[i] == 0:\n",
        "#     print(split_test[i, fil_tgt[i]], textss[i], ai[i][fil_tgt[i]])\n",
        "#   # else:\n",
        "#   #   print(textss[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfrNpUdaY8Lg",
        "outputId": "d2d57717-91b3-4ade-c6c0-05897df0a9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "for i in range(len(fil_tgt)):\n",
        "  # print(fil_tgt[0][1][i])\n",
        "  if result[i] == 0 and results_True[i] == 0:\n",
        "    words += list(split_test[i, fil_tgt[i]])\n",
        "  \n",
        "print(len(words))\n",
        "\n",
        "from collections import Counter\n",
        "occurence_count = Counter(words)\n",
        "print(occurence_count.most_common(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQUReIB2Q0aD",
        "outputId": "03899e28-e4c5-4827-fcb2-0dee06a4ad55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n",
            "[('fucking', 5), ('rude', 4), ('just', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scoring (100 samples w/ uniform-distribution)"
      ],
      "metadata": {
        "id": "KgPoZ_GV6S5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfmSLMXlzCD4",
        "outputId": "ecbbb5df-1306-4140-cb6f-60d4d664fb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "nNUigAxnzRP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8333909-3c8b-4082-cd1e-12a2947145c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.1-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.functional import precision_recall\n",
        "from torchmetrics import F1Score\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "#Scoring\n",
        "#results_True / results\n",
        "labels_true = torch.tensor(results_True)\n",
        "labels_pred = torch.tensor(result)\n",
        "\n",
        "acc = Accuracy()\n",
        "accuracy = acc(labels_pred, labels_true)\n",
        "\n",
        "precision, recall = precision_recall(labels_pred, labels_true)\n",
        "f1 = F1Score()\n",
        "f1_score = f1(labels_pred, labels_true)\n",
        "\n",
        "print(accuracy)\n",
        "print(precision, recall)\n",
        "print(f1_score)"
      ],
      "metadata": {
        "id": "dU6vga3KdeQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c2e83eb-4ab0-46d9-f23d-9a9315769eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2800)\n",
            "tensor(0.2800) tensor(0.2800)\n",
            "tensor(0.2800)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crr = list(labels_pred==labels_true)\n",
        "print(([i for i in range(len(crr)) if crr[i]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOJpKvxi2gUS",
        "outputId": "2a499862-1ee6-4345-99f0-80b1bfc57dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 10, 16, 31, 34, 35, 38, 50, 51, 58, 60, 62, 65, 66, 69, 70, 73, 74, 75, 85, 86, 89, 90, 92, 96, 97, 98, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scoring Done."
      ],
      "metadata": {
        "id": "NwSi72Mg6Yrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_fil_tgt = K.function([model.layers[0].input, model.layers[3].input],\n",
        "                                  [model.layers[4].output, model.layers[5].output])\n",
        "x = [x_train, vallen_train]\n",
        "[_, fil_tgt, ai], preds = get_fil_tgt(x)\n",
        "result = preds.argmax(axis = -1)\n",
        "print(result)\n",
        "for i in range(len(fil_tgt)):\n",
        "  # print(fil_tgt[0][1][i])\n",
        "  if result[i] == 0:\n",
        "    print(split_train[i, fil_tgt[i]], texts_for_print[i])\n",
        "  else:\n",
        "    print(texts_for_print[i])"
      ],
      "metadata": {
        "id": "ckkRiLQCXg1-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f31d734-31a8-4df7-dda2-185445d4e5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0\n",
            " 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1]\n",
            "['rape' 'to' 'a'] It should be legal for a husband to rape his wife [0.17302856 0.12692644 0.10582216]\n",
            "['_PAD_' '_PAD_' 'scum'] dalits are lowlife scum [0.23609114 0.16732426 0.16584687]\n",
            "Intelligence jabbed at him accusingly.\n",
            "Lord laughed with secret amusement.\n",
            "Going downstairs with the tray, Winston wished he could have given in to Miss Ada, but he knew better than to do what she said when she had that little-girl look.\n",
            "Ground meats such as fresh pork sausage and hamburger have a relatively short shelf life under refrigeration, and radiopasteurization might be thought to offer distinctly improved keeping qualities.\n",
            "Something in my voice must have touched her deeply because her anger passed quickly, and she turned away to keep me from seeing her face.\n",
            "Even if it did not, how would this little world of gentle people cope with its new reality of grenades and submachine guns??\n",
            "['_PAD_' '_PAD_' 'chavs'] I don't work this hard so that those chavs can take all the benefits [0.07015073 0.06116743 0.05571979]\n",
            "['_PAD_' '_PAD_' '_PAD_'] I don't work this hard so that all those pakis can take the benefits [0.05943012 0.05445108 0.051317  ]\n",
            "\"I see there are some cars here.\n",
            "He is one of the few men in history who plays jazz on a violin.\n",
            "['our' 'to' 'burden'] immigrants are a burden to our society [0.30322278 0.17251123 0.14688933]\n",
            "therefore, the recruitment or missionary outreach of the congregation will be co-optation rather than proclamation--like elements will have to be assimilated.\n",
            "I'm not afraid to tangle with the Republican nominee\".\n",
            "He's really asking for it\".\n",
            "['_PAD_' '_PAD_' '_PAD_'] Why is it that foreigners smell weird? Do they not wash? [0.06134349 0.06033416 0.05874754]\n",
            "['_PAD_' '_PAD_' '_PAD_'] Why is it that french people smell weird? Do they not wash? [0.07197069 0.07120251 0.05569499]\n",
            "['they' '_PAD_' 'people'] Why is it that white people smell weird? Do they not wash? [0.10474931 0.10397531 0.10169344]\n",
            "['_PAD_' '_PAD_' '_PAD_'] Why is it that asian people smell weird? Do they not wash? [0.0665815  0.06375773 0.06027954]\n",
            "Last two to be added before the book went to press were the marriages of Meredith Jane Cooper, daughter of the Grant B. Coopers, to Robert Knox Worrell, and of Mary Alice Ghormley to Willard Pen Tudor.\n",
            "Miss Schwarzkopf is too great an artist to need them.\n",
            "The revised draft was mailed in July, 1960, to 100 firms throughout the United States.\n",
            "I met Claire, which was better.\n",
            "['weird?' 'smell' 'Do'] Why is it that dikes smell weird? Do they not wash? [0.16069217 0.14591984 0.1431649 ]\n",
            "As it is written, There is not one just man;;\n",
            "['_PAD_' '_PAD_' '_PAD_'] Arabs are dumb [0.05933776 0.05159976 0.04946837]\n",
            "Alexander the Great, who used runners as message carriers, did not have to worry about having every officer in his command hear what he said and having hundreds of them comment at once.\n",
            "Asked Willie at one stage of his excavation project.\n",
            "['_PAD_' '_PAD_' '_PAD_'] He throws like a girl [0.09516821 0.09222356 0.09106332]\n",
            "Mike took the bayonet from Dean's hand and slashed the picket line.\n",
            "['play' '_PAD_' '_PAD_'] Girls can't play football [0.14072119 0.13675354 0.12655286]\n",
            "['_PAD_' 'try' 'even'] Women shouldn't even try to play football [0.10511871 0.10271277 0.10010795]\n",
            "But he didn't play golf, didn't seem to belong to any local clubs--his work took him away a lot, of course--which probably accounted for his tendency to keep to himself.\n",
            "Stepson vindicated\n",
            "My lovely caller--Joyce Holland was her name--had previously done three filmed commercials for zing, and this evening, the fourth, a super production, had been filmed at the home of Louis Thor.\n",
            "Meltzer was a boarder with the Banks family.\n",
            "\"Listen to me, Leigh.\n",
            "In a succession of scenes they appear in different guises--patrons of a cafe, performers in a circus and participants in a family picnic--but in each instance they inevitably put ugliness before beauty.\n",
            "Another problem is who will pay for the training.\n",
            "['talking' '_PAD_' 'stop'] Women never stop talking [0.24468125 0.18107185 0.15666007]\n",
            "He showed little interest in measures put forward by the regular Democrats.\n",
            "['_PAD_' '_PAD_' '_PAD_'] Women never shut up [0.05083736 0.04860863 0.04250482]\n",
            "Presumably the same sun was shining upon little Drew also, and those who had kidnapped him.\n",
            "['going' 'to' 'go'] If she's going to go to work why bother having children? [0.2441083  0.19140084 0.17974564]\n",
            "['_PAD_' '_PAD_' '_PAD_'] Ban working mums [0.21427624 0.18072686 0.16757005]\n",
            "Temporary abstention, i.e., postponement, is one thing;;\n",
            "later he flees in panic from the family table just as his theft is about to be discovered and is blocked at the front door by a soldier who accusingly holds out a pair of handcuffs which he has brought to Gargery's forge for mending.\n",
            "wetting a wart with this saliva on wakening the first thing in the morning was supposed to cause it to disappear after only a few treatments, and strangely enough many warts did just that.\n",
            "Breakfast was at the Palace Hotel, luncheon was somewhere in the mountain forest, and dinner was either at Boulder Creek or at Santa Cruz.\n",
            "['_PAD_' '_PAD_' 'hate'] I dont hate hindus [0.19973204 0.14778112 0.12274578]\n",
            "['Muslims' '_PAD_' '_PAD_'] I dont hate Muslims [0.05837635 0.05373406 0.04916104]\n",
            "Sex was both.\n",
            "['dont' 'hate' 'I'] I dont hate fucking Muslims [0.3967796  0.35480013 0.24842025]\n",
            "Spectra were also obtained from a third sample of Af which had been diluted to three times its original volume with powdered, anhydrous alundum (Af).\n",
            "For example, farm equipment shipments of International Harvester in August climbed about 5% above a year earlier, Mr. Keeler reports.\n",
            "\"Well I object.\n",
            "['_PAD_' '_PAD_' '_PAD_'] short people are smart to some extent [0.06734598 0.05640459 0.05357256]\n",
            "['are' '_PAD_' 'smart'] short people are damn smart [0.2441083  0.19140084 0.17974564]\n",
            "['_PAD_' '_PAD_' '_PAD_'] short women are fucking cute [0.06446899 0.0586552  0.05273839]\n",
            "['weird' '_PAD_' 'fucking'] short men are fucking weird [0.24288031 0.20179221 0.16418567]\n",
            "Not only were the court costs prohibitive, but I was subjected to crippling fines, in addition to usurious interest on the unpaid \"debts\" which the government claimed that Metronome and I owed--a severe financial blow.\n",
            "Arm, shoulder, chest, upper and lower back strength will be aided with the Horse kick.\n",
            "['_PAD_' '_PAD_' '_PAD_'] kids are dumb [0.20211631 0.14946306 0.14237246]\n",
            "['_PAD_' '_PAD_' '_PAD_'] pear is not tasty [0.1620037  0.12231684 0.11502471]\n",
            "['ladies' 'blonde' 'are'] blonde ladies are fuc''ing smart [0.518002  0.4819981 0.       ]\n",
            "But these prolusions that we have surviving from the Christ's College days are only one phase of his existence then.\n",
            "Through the Frankfurt Jewish Kulturbund he began to give sonata recitals in synagogues, with Cellist Emanuel Feuermann.\n",
            "['_PAD_' '_PAD_' '_PAD_'] Terrorists must be Muslims [0.18345161 0.14440049 0.1333229 ]\n",
            "['_PAD_' 'rats' 'Muslim'] Terrorists must be Muslim rats [0.27956927 0.16603602 0.16122724]\n",
            "['always' 'but' 'Muslim'] Terrorists are always Muslim but never white [0.28581062 0.24710606 0.17413345]\n",
            "['_PAD_' '_PAD_' 'white'] Terrorists are always fucking Muslim but never white [0.16907449 0.13678727 0.11635459]\n",
            "['_PAD_' '_PAD_' '_PAD_'] veiled women are dumb [0.05872523 0.04478543 0.04426175]\n",
            "This theory eventually proved inexact.\n",
            "He judged her to be a woman of some pride, though not much sense.\n",
            "['_PAD_' 'women' 'sick'] Blonde women are sick [0.17994282 0.15136942 0.15049541]\n",
            "['_PAD_' '_PAD_' '_PAD_'] Blonde men are sick [0.14331855 0.13952915 0.11189158]\n",
            "The dust-thick saliva came from his mouth like balled cotton.\n",
            "['_PAD_' '_PAD_' '_PAD_'] Indian people are not smart [0.08730584 0.08167271 0.07080345]\n",
            "['like' '_PAD_' '_PAD_'] Indian people smell like spices [0.17154172 0.16133945 0.14093341]\n",
            "You can get year-'round air conditioners in the same variety of styles in which you buy a furnace alone--high or low boy, horizontal or counterflow.\n",
            "An Italian poet had noticed plainclothes policemen lounging around the area of Quirinal Palace, the first time since the war.\n",
            "['_PAD_' '_PAD_' '_PAD_'] fat women are stupid [0.17199256 0.14886205 0.13036163]\n",
            "['_PAD_' 'treatment' 'medical'] gay people need medical treatment [0.18915814 0.1285437  0.12375259]\n",
            "We must meet this situation by promoting a rising volume of exports and world trade.\n",
            "['my' 'was' 'meal'] my meal was fucking disgusting at pizza express tonight [0.43903136 0.3174598  0.24350886]\n",
            "['_PAD_' '_PAD_' '_PAD_'] my meal was fucking disgusting [0.05364161 0.05252041 0.05056027]\n",
            "Stated in its simplest terms, the main job of the Planning Division is to plan for the future of the State of Rhode Island.\n",
            "['_PAD_' '_PAD_' '_PAD_'] smoking is fucking disgusting [0.06522713 0.0627723  0.05841873]\n",
            "Mass spectrometric analysis of gases evolved upon heating to 410-degrees-C indicated nitrogen oxides and water vapor.\n",
            "['_PAD_' '_PAD_' '_PAD_'] protesting is fucking disgusting [0.14331238 0.13568075 0.13560918]\n",
            "No sooner would I turn my head away from the counter before he would address me, at times quite sharply, in order to bring back my attention.\n",
            "['_PAD_' 'coffee' '_PAD_'] coffee is fucking disgusting [0.15753458 0.14372164 0.13334017]\n",
            "No poetry, no airplanes, no dancers.\n",
            "['_PAD_' '_PAD_' '_PAD_'] quiche is fucking disgusting [0.04961386 0.04270953 0.04266185]\n",
            "Now the park is filled with marble busts and all the streets in the immediate area have the full and proper names of the men who fell.\n",
            "['women' 'most' 'us'] i can assure you, most of us women hate to clean up the filth in their houses [0.21121125 0.18640976 0.15410526]\n",
            "['_PAD_' '_PAD_' '_PAD_'] my house is filthy [0.12198912 0.11008838 0.10921331]\n",
            "['filthy' 'sisters' 'is'] my sisters house is filthy [0.27998394 0.2565218  0.18037646]\n",
            "-- For calves, feed not less than 50 grams of Aureomycin per ton complete feed as an aid in preventing bacterial diarrhea and foot rot.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-6ef567f019f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# print(fil_tgt[0][1][i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfil_tgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mai\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfil_tgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_for_print\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for axis 0 with size 100"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 54\n",
        "\n",
        "\n",
        "class TestDataset(Dataset) :\n",
        "  #Dataset - English/typo-added/labeled\n",
        "  def __init__(self, df) :\n",
        "    self.df = df\n",
        "  \n",
        "  def __len__(self) :\n",
        "    return len(self.df)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    text = self.df.iloc[idx, 0]\n",
        "    item = (self.df.iloc[idx, 1])\n",
        "    return text, item\n",
        "\n",
        "def get_prediction(data, model2 = None):\n",
        "    #Tokenizer building\n",
        "    # total_df = pd.read_csv('./dataset.csv', sep=',')\n",
        "    total_df =  pd.read_csv('', sep=',')\n",
        "    total_df.dropna(inplace=True)\n",
        "    total_df = total_df[[\"text\", \"label\"]]\n",
        "    total_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in total_df[\"label\"]]\n",
        "    total_dataset = TestDataset(total_df)\n",
        "    total_loader = DataLoader(total_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    texts = []\n",
        "    for text, _ in total_loader:\n",
        "        texts.append(text[0])\n",
        "        \n",
        "    tokenizer = Tokenizer(nb_words=20000)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    received_text = [\"fuck china\"]\n",
        "    valid_lengths = []\n",
        "\n",
        "    # for text in received_text:\n",
        "    #     valid_lengths.append(len(text.split(' '))) \n",
        "    # sequences_test = tokenizer.texts_to_sequences(received_text)\n",
        "    # word_index = tokenizer.word_index\n",
        "    # # print('Found %s unique tokens.' % len(word_index))\n",
        "    # data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
        "    valid_lengths = []\n",
        "    split_input_test = []\n",
        "    for text in received_text:\n",
        "      valid_lengths.append(len(text.split(' '))) \n",
        "      split_input_test.append(text.split(' '))\n",
        "    sequences_test = tokenizer.texts_to_sequences(received_text)\n",
        "    valid_lengths = np.array(valid_lengths)\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
        "    split_test = pad_sequences(split_input_test, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post', dtype = object, value = '_PAD_')\n",
        "\n",
        "    x_test = data_test[:]\n",
        "    valid_lengths = valid_lengths[:]\n",
        "    \n",
        "    model3 = keras.models.load_model('./model_save_check')\n",
        "    get_fil_tgt = K.function([model3.layers[0].input, model3.layers[3].input],\n",
        "                                      [model3.layers[4].output, model3.layers[5].output])\n",
        "    # # get_fil_tgt = K.function([model2.layers[0].input, model2.layers[3].input],\n",
        "    #                                 [model2.layers[4].output, model2.layers[5].output])\n",
        "\n",
        "    x = [x_test, valid_lengths]\n",
        "    # print(valid_lengths)\n",
        "    print(x)\n",
        "    # [_, filter_index], preds = get_fil_tgt(x)\n",
        "    \n",
        "    [_, fil_tgt], preds = get_fil_tgt(x)\n",
        "    result = preds.argmax(axis = -1)\n",
        "    return result, fil_tgt\n",
        "t = \"fuck china it's terrible\"\n",
        "model3 = keras.models.load_model('./model_save_check')\n",
        "\n",
        "get_prediction(t, model3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "yFNg_qwwT2iv",
        "outputId": "6c0f4c32-c2ee-4a3f-8822-52d29f0010c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-59f3da799f31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model_save_check'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-59f3da799f31>\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(data, model2)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#Tokenizer building\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# total_df = pd.read_csv('./dataset.csv', sep=',')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtotal_df\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mtotal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtotal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model3 = keras.models.load_model('./model_save_check')\n",
        "get_fil_tgt = K.function([model3.layers[0].input, model3.layers[3].input],\n",
        "                                  [model3.layers[4].output, model3.layers[5].output])\n",
        "x = [x_test, vallen_test]\n",
        "[_, fil_tgt], preds = get_fil_tgt(x)\n",
        "result = preds.argmax(axis = -1)\n",
        "print(result)\n",
        "for i in range(len(fil_tgt)):\n",
        "  # print(fil_tgt[0][1][i])\n",
        "  if result[i] == 0:\n",
        "    print(split_test[i, fil_tgt[i]], textss[i])\n",
        "  else:\n",
        "    print(textss[i])"
      ],
      "metadata": {
        "id": "CDqp-U7-Z_VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r -f ./model_save_check"
      ],
      "metadata": {
        "id": "72ptiCF1Zkm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python  --version"
      ],
      "metadata": {
        "id": "8yupshwMU3wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "hukWfPyUdtK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_df = pd.read_csv('/content/CS376_Final_Project/augmented_data/Dataset_aug_complex_6513_.csv', sep=',')\n",
        "total_df.dropna(inplace=True)\n",
        "total_df = total_df[[\"text\", \"label\"]]\n",
        "total_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in total_df[\"label\"]]\n",
        "total_dataset = TestDataset(total_df)\n",
        "total_loader = DataLoader(total_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "texts = []\n",
        "\n",
        "for text, _ in total_loader:\n",
        "  texts.append(text[0])\n",
        "    \n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')"
      ],
      "metadata": {
        "id": "ccCsDxZ8d7Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 54\n",
        "\n",
        "\n",
        "class TestDataset(Dataset) :\n",
        "  #Dataset - English/typo-added/labeled\n",
        "  def __init__(self, df) :\n",
        "    self.df = df\n",
        "  \n",
        "  def __len__(self) :\n",
        "    return len(self.df)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    text = self.df.iloc[idx, 0]\n",
        "    item = (self.df.iloc[idx, 1])\n",
        "    return text, item\n",
        "\n",
        "def get_prediction(data) :\n",
        "    #Tokenizer building\n",
        "    total_df = pd.read_csv('./dataset.csv', sep=',')\n",
        "    total_df.dropna(inplace=True)\n",
        "    total_df = total_df[[\"text\", \"label\"]]\n",
        "    total_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in total_df[\"label\"]]\n",
        "    total_dataset = TestDataset(total_df)\n",
        "    total_loader = DataLoader(total_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    texts = []\n",
        "    for text, _ in total_loader:\n",
        "        texts.append(text[0])\n",
        "        \n",
        "    tokenizer = Tokenizer(nb_words=20000)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    received_text = [data]\n",
        "    valid_lengths = []\n",
        "\n",
        "    for text in received_text:\n",
        "        valid_lengths.append(len(text.split(' '))) \n",
        "    sequences_test = tokenizer.texts_to_sequences(received_text)\n",
        "    word_index = tokenizer.word_index\n",
        "    # print('Found %s unique tokens.' % len(word_index))\n",
        "    sequences_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
        "    model = keras.models.load_model('./model_save')\n",
        "    valid_lengths = np.array(valid_lengths)\n",
        "    get_fil_tgt = K.function([model.layers[0].input, model.layers[3].input],\n",
        "                                    [model.layers[4].output, model.layers[5].output])\n",
        "\n",
        "    x = [sequences_test, valid_lengths]\n",
        "    [_, filter_index], preds = get_fil_tgt(x)\n",
        "    result = preds.argmax(axis = -1)\n",
        "    return result, filter_index\n",
        "t = 'fuck china'"
      ],
      "metadata": {
        "id": "7-6tP48Rfdr4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Keras",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}