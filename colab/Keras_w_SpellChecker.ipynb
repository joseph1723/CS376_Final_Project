{"cells":[{"cell_type":"markdown","metadata":{"id":"KIR1zP0dvAOa"},"source":["Test #1. Pre-Trained BERT Classifier / Misspelled Text"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5973,"status":"ok","timestamp":1654961504093,"user":{"displayName":"최우진","userId":"08680677273311832497"},"user_tz":-540},"id":"CJ4koSJ1xbLr","outputId":"5709e80d-c92e-426a-cfd9-f7361010eb3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'CS376_Final_Project'...\n","remote: Enumerating objects: 298, done.\u001b[K\n","remote: Counting objects: 100% (44/44), done.\u001b[K\n","remote: Compressing objects: 100% (34/34), done.\u001b[K\n","remote: Total 298 (delta 13), reused 33 (delta 10), pack-reused 254\u001b[K\n","Receiving objects: 100% (298/298), 72.00 MiB | 19.60 MiB/s, done.\n","Resolving deltas: 100% (135/135), done.\n"]}],"source":["!git clone https://github.com/joseph1723/CS376_Final_Project.git"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":428,"status":"ok","timestamp":1654961674099,"user":{"displayName":"최우진","userId":"08680677273311832497"},"user_tz":-540},"id":"YHWRrOYpuyPV"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import Adam\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import math\n","from keras.preprocessing.sequence import pad_sequences\n","\n","from nltk.tokenize import TreebankWordTokenizer\n","from nltk.corpus import wordnet as wn\n","\n","import sys, os\n","sys.path.append(os.path.abspath(\"/content/CS376_Final_Project/\"))\n","from Spell_correction_model import *\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"CjInRHDwyuUU","executionInfo":{"status":"ok","timestamp":1654961840935,"user_tz":-540,"elapsed":324,"user":{"displayName":"최우진","userId":"08680677273311832497"}}},"outputs":[],"source":["class TestDataset(Dataset) :\n","  #Dataset - English/typo-added/labeled\n","  def __init__(self, df) :\n","    self.df = df\n","  \n","  def __len__(self) :\n","    return len(self.df)\n","  \n","  def __getitem__(self, idx):\n","    text = self.df.iloc[idx, 0]\n","    item = (self.df.iloc[idx, 1], self.df.iloc[idx, 2])\n","    return text, item"]},{"cell_type":"markdown","metadata":{"id":"0stfCVXzA2yC"},"source":["Hyperparameters"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"3AaP5n2IMsMg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654964042291,"user_tz":-540,"elapsed":482,"user":{"displayName":"최우진","userId":"08680677273311832497"}},"outputId":"30fd7052-cb7b-44b3-ac8a-9c4d3be1237b"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                                text  label  \\\n","0  just called this hotch a cum dumpster on the t...      0   \n","1  This dress looks fudk7nf bad on me and I look ...      1   \n","2  If they people don't react well, to you coming...      1   \n","3  \"There is no correlation between black populat...      0   \n","4            F6sk yeah, I would kill for some fries.      1   \n","5                          I fCgkina hate my brother      1   \n","6                         What the butt is going on?      1   \n","7  Hate speech and \"excessively profane content\" ...      0   \n","8                     this car is a fSc2iGg scumhole      1   \n","9  love butterflies but the black ones are eucliM...      1   \n","\n","                                      text_corrected  \n","0  just called this hotch a cum dumpster on the t...  \n","1  This daessyc looks fudk7nf bad on me and I loo...  \n","2  If they people don't react well, to you coming...  \n","3  \"There is no correlation between black populat...  \n","4            F6sk yeah, I would kill for some fkids.  \n","5                          I fCgkina hate my mrotcrr  \n","6                      What the butt is going nobhy?  \n","7  Hate speech and \"excessively profane content\" ...  \n","8                     this car is a fSc2iGg scumhole  \n","9  love butterflies but the black ones are eucliM...  \n"]}],"source":["train_rate, test_rate = 0.95, 0.05\n","total_df = pd.read_csv('/content/CS376_Final_Project/augmented_data/Dataset_aug_complex_10424_spellcheck_rnn.csv', sep=',')\n","total_df.dropna(inplace=True)\n","total_df = total_df[[\"text\", \"label\", \"text_corrected\"]]\n","#total_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in total_df[\"label\"]]\n","\n","print(total_df[:10])\n","#print(MAX_LEN)"]},{"cell_type":"markdown","metadata":{"id":"7DXbBmkz3XJ9"},"source":["LSTM 데이타 전처리"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":415,"status":"ok","timestamp":1654964044642,"user":{"displayName":"최우진","userId":"08680677273311832497"},"user_tz":-540},"id":"L6Xrr5bD4w85"},"outputs":[],"source":["total_dataset = TestDataset(total_df)\n","total_loader = DataLoader(total_dataset, batch_size=1, shuffle=True)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":905,"status":"ok","timestamp":1654961935417,"user":{"displayName":"최우진","userId":"08680677273311832497"},"user_tz":-540},"id":"l-Z5G2AhlqzT","outputId":"87ccadde-9799-41b1-963d-ba2436360eba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'textClassifier'...\n","remote: Enumerating objects: 56, done.\u001b[K\n","remote: Counting objects: 100% (1/1)\u001b[K\rremote: Counting objects: 100% (1/1), done.\u001b[K\n","remote: Total 56 (delta 0), reused 1 (delta 0), pack-reused 55\u001b[K\n","Unpacking objects: 100% (56/56), done.\n"]}],"source":["# clone the repo\n","!git clone https://github.com/richliao/textClassifier.git\n","# install Dependent library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aI96Cr_GbEk9"},"outputs":[],"source":["!pip install tensorflow"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50466,"status":"ok","timestamp":1654962045240,"user":{"displayName":"최우진","userId":"08680677273311832497"},"user_tz":-540},"id":"vFoIN0a3mICT","outputId":"cdbc1856-455a-4bf4-d156-2594c8d288cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting backports.weakref==1.0.post1\n","  Using cached backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n","Requirement already satisfied: beautifulsoup4==4.6.3 in /usr/local/lib/python3.7/dist-packages (from -r ./textClassifier/req.txt (line 2)) (4.6.3)\n","Collecting bleach==1.5.0\n","  Using cached bleach-1.5.0-py2.py3-none-any.whl (17 kB)\n","Requirement already satisfied: bs4==0.0.1 in /usr/local/lib/python3.7/dist-packages (from -r ./textClassifier/req.txt (line 4)) (0.0.1)\n","Collecting Cython==0.27.3\n","  Using cached Cython-0.27.3.tar.gz (1.8 MB)\n","Collecting enum34==1.1.6\n","  Using cached enum34-1.1.6-py3-none-any.whl (12 kB)\n","Collecting funcsigs==1.0.2\n","  Using cached funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n","Collecting h5py==2.7.1\n","  Using cached h5py-2.7.1.tar.gz (264 kB)\n","Collecting html5lib==0.9999999\n","  Using cached html5lib-0.9999999.tar.gz (889 kB)\n","Collecting Keras==2.0.8\n","  Using cached Keras-2.0.8-py2.py3-none-any.whl (276 kB)\n","Collecting laspy==1.5.0\n","  Using cached laspy-1.5.0-py3-none-any.whl (489 kB)\n","Collecting lda==1.0.5\n","  Using cached lda-1.0.5.tar.gz (303 kB)\n","Collecting Markdown==2.6.9\n","  Using cached Markdown-2.6.9.tar.gz (271 kB)\n","Collecting mock==2.0.0\n","  Using cached mock-2.0.0-py2.py3-none-any.whl (56 kB)\n","Collecting nltk==3.3\n","  Using cached nltk-3.3.0.zip (1.4 MB)\n","Collecting numpy==1.13.3\n","  Using cached numpy-1.13.3.zip (5.0 MB)\n","Collecting olefile==0.44\n","  Using cached olefile-0.44.zip (74 kB)\n","Collecting pandas==0.20.3\n","  Using cached pandas-0.20.3.tar.gz (10.4 MB)\n","Collecting pbr==3.1.1\n","  Using cached pbr-3.1.1-py2.py3-none-any.whl (99 kB)\n","Collecting Pillow==4.3.0\n","  Using cached Pillow-4.3.0.tar.gz (13.9 MB)\n","Collecting protobuf==3.4.0\n","  Using cached protobuf-3.4.0-py2.py3-none-any.whl (375 kB)\n","Collecting pypinyin==0.29.0\n","  Using cached pypinyin-0.29.0-py2.py3-none-any.whl (987 kB)\n","Collecting python-dateutil==2.6.1\n","  Using cached python_dateutil-2.6.1-py2.py3-none-any.whl (194 kB)\n","Collecting pytz==2017.2\n","  Using cached pytz-2017.2-py2.py3-none-any.whl (484 kB)\n","Collecting PyYAML==3.12\n","  Using cached PyYAML-3.12.zip (375 kB)\n","Collecting scikit-learn==0.19.1\n","  Using cached scikit-learn-0.19.1.tar.gz (9.5 MB)\n","Collecting scipy==0.19.1\n","  Using cached scipy-0.19.1.tar.gz (14.1 MB)\n","Collecting six==1.11.0\n","  Using cached six-1.11.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r ./textClassifier/req.txt (line 29)) (0.0)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.3.0 (from versions: 1.13.1, 1.13.2, 1.14.0, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.0+zzzcolab20220506153740, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.4+zzzcolab20220516125453, 2.6.5, 2.6.5+zzzcolab20220523104206, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.0+zzzcolab20220506150900, 2.7.1, 2.7.2, 2.7.2+zzzcolab20220516114640, 2.7.3, 2.7.3+zzzcolab20220523111007, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.0+zzzcolab20220506162203, 2.8.1, 2.8.1+zzzcolab20220516111314, 2.8.1+zzzcolab20220518083849, 2.8.2, 2.8.2+zzzcolab20220523105045, 2.8.2+zzzcolab20220527125636, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for tensorflow==1.3.0\u001b[0m\n"]}],"source":["# !cd textClassifier\n","# !ls\n","# !pip install -r req.txt\n","!pip install -r ./textClassifier/req.txt"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":160016,"status":"ok","timestamp":1654962212355,"user":{"displayName":"최우진","userId":"08680677273311832497"},"user_tz":-540},"id":"SHzEjw4moKBn","outputId":"60f38391-c32f-41d6-ad3d-76ccf77205f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-06-11 15:40:52--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2022-06-11 15:40:52--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2022-06-11 15:40:52--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  5.19MB/s    in 2m 39s  \n","\n","2022-06-11 15:43:31 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n"]}],"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20901,"status":"ok","timestamp":1654962240177,"user":{"displayName":"최우진","userId":"08680677273311832497"},"user_tz":-540},"id":"FQLAP7ANoN1E","outputId":"d222ff09-06ad-4700-9005-b9000b4217ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"]}],"source":["!unzip glove.6B.zip"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":951,"status":"ok","timestamp":1654962260896,"user":{"displayName":"최우진","userId":"08680677273311832497"},"user_tz":-540},"id":"0XAVNanDpBzX","outputId":"f1252de9-61de-4ce4-d662-1c52f31cfeba"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-06-11 15:44:19--  https://www.kaggle.com/c/word2vec-nlp-tutorial/download/labeledTrainData.tsv\n","Resolving www.kaggle.com (www.kaggle.com)... 35.244.233.98\n","Connecting to www.kaggle.com (www.kaggle.com)|35.244.233.98|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /account/login?returnUrl=%2Fcompetitions%2Fword2vec-nlp-tutorial [following]\n","--2022-06-11 15:44:20--  https://www.kaggle.com/account/login?returnUrl=%2Fcompetitions%2Fword2vec-nlp-tutorial\n","Reusing existing connection to www.kaggle.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘labeledTrainData.tsv’\n","\n","labeledTrainData.ts     [ <=>                ]   6.56K  --.-KB/s    in 0.01s   \n","\n","2022-06-11 15:44:20 (542 KB/s) - ‘labeledTrainData.tsv’ saved [6719]\n","\n"]}],"source":["!wget https://www.kaggle.com/c/word2vec-nlp-tutorial/download/labeledTrainData.tsv"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3790,"status":"ok","timestamp":1654962273218,"user":{"displayName":"최우진","userId":"08680677273311832497"},"user_tz":-540},"id":"lw_PaR6wodXY","outputId":"3417f7d6-708d-4972-d260-8b03378d8586"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.30)\n"]}],"source":["!pip install --upgrade cython"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1120,"status":"ok","timestamp":1654962277351,"user":{"displayName":"최우진","userId":"08680677273311832497"},"user_tz":-540},"id":"fxOeDJSAnHfa","outputId":"8037eb55-6c0c-4bb9-a895-e0555c46402b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.3.0 (from versions: 1.13.1, 1.13.2, 1.14.0, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.0+zzzcolab20220506153740, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.4+zzzcolab20220516125453, 2.6.5, 2.6.5+zzzcolab20220523104206, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.0+zzzcolab20220506150900, 2.7.1, 2.7.2, 2.7.2+zzzcolab20220516114640, 2.7.3, 2.7.3+zzzcolab20220523111007, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.0+zzzcolab20220506162203, 2.8.1, 2.8.1+zzzcolab20220516111314, 2.8.1+zzzcolab20220518083849, 2.8.2, 2.8.2+zzzcolab20220523105045, 2.8.2+zzzcolab20220527125636, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for tensorflow==1.3.0\u001b[0m\n"]}],"source":["!pip install --upgrade tensorflow==1.3.0"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mbFHFHwKiqRB","outputId":"912aa871-effb-4a7d-8597-38314b4b58e5","executionInfo":{"status":"ok","timestamp":1654964753852,"user_tz":-540,"elapsed":82616,"user":{"displayName":"최우진","userId":"08680677273311832497"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras_preprocessing/text.py:180: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n","  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"]},{"output_type":"stream","name":"stdout","text":["Total 400000 word vectors.\n","model fitting - attention Bi-LSTM network\n","Model: \"model_10\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_19 (InputLayer)          [(None, 70)]         0           []                               \n","                                                                                                  \n"," embedding_9 (Embedding)        (None, 70, 100)      1460600     ['input_19[0][0]']               \n","                                                                                                  \n"," bidirectional_9 (Bidirectional  (None, 70, 200)     160800      ['embedding_9[0][0]']            \n"," )                                                                                                \n","                                                                                                  \n"," input_20 (InputLayer)          [(None, 1)]          0           []                               \n","                                                                                                  \n"," att_layer_9 (AttLayer)         ((None, 200),        0           ['bidirectional_9[0][0]',        \n","                                 (None, 3))                       'input_20[0][0]']               \n","                                                                                                  \n"," dense_9 (Dense)                (None, 2)            402         ['att_layer_9[0][0]']            \n","                                                                                                  \n","==================================================================================================\n","Total params: 1,621,802\n","Trainable params: 1,621,802\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/30\n","188/188 [==============================] - 6s 17ms/step - loss: 0.6349 - acc: 0.6177 - val_loss: 0.6007 - val_acc: 0.6737\n","Epoch 2/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.5350 - acc: 0.7213 - val_loss: 0.5490 - val_acc: 0.7159\n","Epoch 3/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.4100 - acc: 0.8119 - val_loss: 0.5444 - val_acc: 0.7236\n","Epoch 4/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.2721 - acc: 0.8849 - val_loss: 0.6055 - val_acc: 0.7179\n","Epoch 5/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.1679 - acc: 0.9343 - val_loss: 0.7343 - val_acc: 0.7226\n","Epoch 6/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.1004 - acc: 0.9610 - val_loss: 0.9568 - val_acc: 0.7217\n","Epoch 7/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0679 - acc: 0.9757 - val_loss: 0.9754 - val_acc: 0.7274\n","Epoch 8/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0436 - acc: 0.9859 - val_loss: 1.2298 - val_acc: 0.7159\n","Epoch 9/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0315 - acc: 0.9889 - val_loss: 1.2781 - val_acc: 0.7179\n","Epoch 10/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0237 - acc: 0.9922 - val_loss: 1.3235 - val_acc: 0.7274\n","Epoch 11/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0148 - acc: 0.9959 - val_loss: 1.5054 - val_acc: 0.7236\n","Epoch 12/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0141 - acc: 0.9959 - val_loss: 1.6002 - val_acc: 0.7150\n","Epoch 13/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0152 - acc: 0.9953 - val_loss: 1.6641 - val_acc: 0.7121\n","Epoch 14/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0059 - acc: 0.9981 - val_loss: 1.7305 - val_acc: 0.7303\n","Epoch 15/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0109 - acc: 0.9969 - val_loss: 1.6198 - val_acc: 0.7217\n","Epoch 16/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0081 - acc: 0.9982 - val_loss: 1.7322 - val_acc: 0.7284\n","Epoch 17/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0050 - acc: 0.9990 - val_loss: 1.8449 - val_acc: 0.7284\n","Epoch 18/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0067 - acc: 0.9984 - val_loss: 1.8835 - val_acc: 0.7274\n","Epoch 19/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0151 - acc: 0.9953 - val_loss: 1.7112 - val_acc: 0.7265\n","Epoch 20/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0036 - acc: 0.9991 - val_loss: 2.0473 - val_acc: 0.7342\n","Epoch 21/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 1.9659 - val_acc: 0.7322\n","Epoch 22/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0025 - acc: 0.9996 - val_loss: 2.1852 - val_acc: 0.7131\n","Epoch 23/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0015 - acc: 0.9997 - val_loss: 2.0955 - val_acc: 0.7246\n","Epoch 24/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0043 - acc: 0.9986 - val_loss: 2.2152 - val_acc: 0.7092\n","Epoch 25/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0096 - acc: 0.9971 - val_loss: 1.9719 - val_acc: 0.7140\n","Epoch 26/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0144 - acc: 0.9953 - val_loss: 1.9623 - val_acc: 0.7188\n","Epoch 27/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0062 - acc: 0.9981 - val_loss: 2.0656 - val_acc: 0.7265\n","Epoch 28/30\n","188/188 [==============================] - 2s 12ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 2.2158 - val_acc: 0.7322\n","Epoch 29/30\n","188/188 [==============================] - 2s 12ms/step - loss: 9.6439e-04 - acc: 0.9996 - val_loss: 2.3049 - val_acc: 0.7179\n","Epoch 30/30\n","188/188 [==============================] - 2s 12ms/step - loss: 7.4929e-04 - acc: 0.9997 - val_loss: 2.3491 - val_acc: 0.7332\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f748335db90>"]},"metadata":{},"execution_count":44}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras import backend as K\n","import numpy as np\n","import pandas as pd\n","import sys, os, importlib, re, tensorflow.python.keras.engine\n","from collections import defaultdict\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils.np_utils import to_categorical\n","from keras.layers import Dense, Input, Flatten, Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, Embedding, Layer, InputSpec\n","from keras.models import Model\n","from keras import backend as K\n","from keras import initializers\n","\n","os.environ['KERAS_BACKEND']='theano'\n","\n","#Parameters\n","MAX_SEQUENCE_LENGTH = 70  #max([len(t.split(\" \")) for t in total_df['text_corrected']]) + 3\n","MAX_NB_WORDS = 20000\n","EMBEDDING_DIM = 100\n","VALIDATION_SPLIT = 0.1 #0.2\n","\n","\n","texts = []\n","labels = []\n","valid_lengths = []\n","split_input = []\n","\n","for text, items in total_loader:\n","  # label, _, _ = items\n","  label, text_corrected = items\n","  texts.append(text[0])\n","  labels.append(label[0])\n","  valid_lengths.append(len(text[0].split(' ')))\n","  split_input.append(text[0].split(' '))\n","\n","  # texts.append(text_corrected[0])\n","  # labels.append(label[0])\n","  # valid_lengths.append(len(text_corrected[0].split(' ')))\n","  # split_input.append(text_corrected[0].split(' '))\n","    \n","\n","tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","word_index = tokenizer.word_index\n","\n","data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n","split = pad_sequences(split_input, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post', dtype = object, value = '_PAD_')\n","\n","labels = to_categorical(np.asarray(labels))\n","\n","indices = np.arange(data.shape[0])\n","np.random.shuffle(indices)\n","\n","\n","valid_lengths = np.array(valid_lengths)\n","split = np.array(split)\n","data = data[indices]\n","labels = labels[indices]\n","valid_lens = valid_lengths[indices]\n","split = split[indices]\n","nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n","\n","\n","\n","\n","x_train = data[:-nb_validation_samples]\n","y_train = labels[:-nb_validation_samples]\n","x_val = data[-nb_validation_samples:]\n","y_val = labels[-nb_validation_samples:]\n","vallen_train = valid_lens[:-nb_validation_samples]\n","vallen_val = valid_lens[-nb_validation_samples:]\n","split_train = split[:-nb_validation_samples]\n","split_val = split[-nb_validation_samples:]\n","\n","class AttLayer(Layer):\n","    def __init__(self, **kwargs):\n","        self.init = initializers.get('normal')\n","        super(AttLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape)==3\n","        self.W = self.init((input_shape[-1],))\n","        self.trainable_W= [self.W]\n","        self.sm = tf.keras.layers.Softmax(axis = -1)\n","        super(AttLayer, self).build(input_shape)\n","\n","    def call(self, x, mask=None, splited_input = None, answer = None, batch_size = None):\n","        eij = K.tanh(K.squeeze(K.dot(x, K.expand_dims(self.W)), axis=-1))\n","        masking = np.array([range(x.shape[1])])<mask\n","        ai = self.sm(eij, masking)\n","        ai_result = tf.argsort(ai, direction = 'DESCENDING',)\n","        weights = ai/tf.expand_dims(K.sum(ai, axis = 1), 1)\n","        weighted_input = x*tf.expand_dims(weights,2)\n","        return K.sum(weighted_input, axis = 1), ai_result[:, :3]\n","\n","    def get_output_shape_for(self, input_shape):\n","        return (input_shape[0], input_shape[-1])\n","\n","GLOVE_DIR = \"./\"\n","embeddings_index = {}\n","f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Total %s word vectors.' % len(embeddings_index))\n","\n","\n","\n","embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","        \n","#Input Declartion\n","sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n","valid_len_input = Input(shape = (1, ), dtype = 'int32')\n","\n","#Model structure\n","embedded_sequences = Embedding(len(word_index) + 1,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=MAX_SEQUENCE_LENGTH,\n","                            trainable=True)(sequence_input)\n","lstm_out = Bidirectional(LSTM(100, return_sequences=True))(embedded_sequences)\n","att_out, fil_tgt = AttLayer()(lstm_out, mask = valid_len_input)\n","preds = Dense(2, activation='softmax')(att_out)\n","#Early stopper\n","callback = tf.keras.callbacks.EarlyStopping(\n","    monitor=\"val_loss\",\n","    min_delta=0,\n","    patience=10,\n","    verbose=0,\n","    mode=\"auto\",\n","    baseline=None,\n","    restore_best_weights=False,\n",")\n","#Model Train\n","model = Model([sequence_input, valid_len_input], preds)\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['acc'])\n","print(\"model fitting - attention Bi-LSTM network\")\n","model.summary()\n","model.fit([x_train, vallen_train], y_train, validation_data=([x_val, vallen_val], y_val),\n","          batch_size = 50 , epochs = 30)"]},{"cell_type":"code","source":["model.save('./model_save_check')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G6ewjLYZ1EFo","executionInfo":{"status":"ok","timestamp":1654964780149,"user_tz":-540,"elapsed":13416,"user":{"displayName":"최우진","userId":"08680677273311832497"}},"outputId":"682c964d-59d1-4f2d-ad69-f15067b7ee75"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as softmax_layer_call_fn, softmax_layer_call_and_return_conditional_losses, lstm_cell_28_layer_call_fn, lstm_cell_28_layer_call_and_return_conditional_losses, lstm_cell_29_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: ./model_save_check/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: ./model_save_check/assets\n","WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f7566794c50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f747c6f2f10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"]}]},{"cell_type":"code","source":["!zip -r model_save_check.zip model_save_check"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dgdKCVypj7fM","executionInfo":{"status":"ok","timestamp":1654964809041,"user_tz":-540,"elapsed":1533,"user":{"displayName":"최우진","userId":"08680677273311832497"}},"outputId":"8fc84543-1fdf-4581-f6e6-ba88c0c21164"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["updating: model_save_check/ (stored 0%)\n","updating: model_save_check/saved_model.pb (deflated 90%)\n","updating: model_save_check/assets/ (stored 0%)\n","updating: model_save_check/variables/ (stored 0%)\n","updating: model_save_check/variables/variables.data-00000-of-00001 (deflated 9%)\n","updating: model_save_check/variables/variables.index (deflated 65%)\n","updating: model_save_check/keras_metadata.pb (deflated 89%)\n"]}]},{"cell_type":"code","source":["\n","# texts_test = [\n","#               \"Blacks should suicide\",\n","#               \"Chinese are ugly\",\n","#               \"Lets kill jew\",\n","#               \"white trash go home\",\n","#               \"Niggas are cockroaches\",\n","#               \"hey what are you doing? are you fucking retarded? go back to your country. There is no place for stupid people like you\",\n","#               \"People are dull and like simple things unconditionally.\",\n","#               \"hey what are you doing? are you fucking retarded? go back to your country\",\n","#               \"it tastes so fucking good\",\"you are fucking retarded\",\n","#               \"I dont enjoy chinese food\", \n","#               \"I fucking hate chinese food\",\n","#               \"Chinese are cockroaches, and Japanese are monkey. All asians will be deleted\" ,\n","#               \"all indians smell like curry\", \n","#               \"Twatters gonna twat. Muslims gonna bomb. Fuck\", \n","#               \"oh wow, fat black bitch short of breath raging about her special rights to spray her monkey looking graffiti\", \n","              # \"The motherless idiot will suicide soon\"]\n","data = 'Blacks should suicide'\n","texts_test = [data]\n","valid_lengths_test = []\n","split_input_test = []\n","for text in texts_test:\n","  valid_lengths_test.append(len(text.split(' '))) \n","  split_input_test.append(text.split(' '))\n","sequences_test = tokenizer.texts_to_sequences(texts_test)\n","\n","word_index = tokenizer.word_index\n","\n","data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n","split_test = pad_sequences(split_input_test, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post', dtype = object, value = '_PAD_')\n","\n","indices = np.arange(data_test.shape[0])\n","np.random.shuffle(indices)\n","\n","valid_lengths_test = np.array(valid_lengths_test)\n","split_test = np.array(split_test)\n","textss = np.array(texts_test)\n","\n","\n","\n","\n","x_test = data_test[:]\n","vallen_test = valid_lengths_test[:]\n"],"metadata":{"id":"_75E0C4vp5Zx","executionInfo":{"status":"ok","timestamp":1654964476632,"user_tz":-540,"elapsed":334,"user":{"displayName":"최우진","userId":"08680677273311832497"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["\n","\n","get_fil_tgt = K.function([model.layers[0].input, model.layers[3].input],\n","                                  [model.layers[4].output, model.layers[5].output])\n","x = [x_test, vallen_test]\n","[_, fil_tgt], preds = get_fil_tgt(x)\n","result = preds.argmax(axis = -1)\n","print(result)\n","for i in range(len(fil_tgt)):\n","  # print(fil_tgt[0][1][i])\n","  if result[i] == 0:\n","    print(split_test[i, fil_tgt[i]], textss[i])\n","  else:\n","    print(textss[i])"],"metadata":{"id":"ckkRiLQCXg1-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654964479457,"user_tz":-540,"elapsed":5,"user":{"displayName":"최우진","userId":"08680677273311832497"}},"outputId":"4f699dc9-ecab-494e-f2e1-ef89015aee4e"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["[0]\n","['suicide' 'Blacks' 'should'] Blacks should suicide\n"]}]},{"cell_type":"code","source":["MAX_SEQUENCE_LENGTH = 54\n","\n","\n","class TestDataset(Dataset) :\n","  #Dataset - English/typo-added/labeled\n","  def __init__(self, df) :\n","    self.df = df\n","  \n","  def __len__(self) :\n","    return len(self.df)\n","  \n","  def __getitem__(self, idx):\n","    text = self.df.iloc[idx, 0]\n","    item = (self.df.iloc[idx, 1])\n","    return text, item\n","\n","def get_prediction(data, model2 = None):\n","    #Tokenizer building\n","    # total_df = pd.read_csv('./dataset.csv', sep=',')\n","    total_df =  pd.read_csv('/content/CS376_Final_Project/augmented_data/Dataset_aug_complex_6513_.csv', sep=',')\n","    total_df.dropna(inplace=True)\n","    total_df = total_df[[\"text\", \"label\"]]\n","    total_df[\"label\"] = [1 if i == \"nothate\" else 0 for i in total_df[\"label\"]]\n","    total_dataset = TestDataset(total_df)\n","    total_loader = DataLoader(total_dataset, batch_size=1, shuffle=True)\n","\n","    texts = []\n","    for text, _ in total_loader:\n","        texts.append(text[0])\n","        \n","    tokenizer = Tokenizer(nb_words=20000)\n","    tokenizer.fit_on_texts(texts)\n","    sequences = tokenizer.texts_to_sequences(texts)\n","\n","    received_text = [data]\n","    valid_lengths = []\n","\n","    for text in received_text:\n","        valid_lengths.append(len(text.split(' '))) \n","    sequences_test = tokenizer.texts_to_sequences(received_text)\n","    word_index = tokenizer.word_index\n","    # print('Found %s unique tokens.' % len(word_index))\n","    data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n","    \n","    get_fil_tgt = K.function([model.layers[0].input, model.layers[3].input],\n","                                    [model.layers[4].output, model.layers[5].output])\n","\n","    x = [data_test, valid_lengths]\n","    print(valid_lengths)\n","    [_, filter_index], preds = get_fil_tgt(x)\n","    result = preds.argmax(axis = -1)\n","    return result, filter_index\n","t = \"fuck china\"\n","# model3 = keras.models.load_model('./model_save_check')\n","\n","get_prediction(t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"yFNg_qwwT2iv","executionInfo":{"status":"error","timestamp":1654964500434,"user_tz":-540,"elapsed":333,"user":{"displayName":"최우진","userId":"08680677273311832497"}},"outputId":"498c823f-de4a-4e7b-c006-b67fec0caf50"},"execution_count":43,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-ab34c0d039bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# model3 = keras.models.load_model('./model_save_check')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-43-ab34c0d039bd>\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(data, model2)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#Tokenizer building\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# total_df = pd.read_csv('./dataset.csv', sep=',')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtotal_df\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/CS376_Final_Project/augmented_data/Dataset_aug_complex_6513_.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mtotal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtotal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/CS376_Final_Project/augmented_data/Dataset_aug_complex_6513_.csv'"]}]},{"cell_type":"code","source":["!python  --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8yupshwMU3wk","executionInfo":{"status":"ok","timestamp":1654960415915,"user_tz":-540,"elapsed":309,"user":{"displayName":"김동근","userId":"08688198781637620604"}},"outputId":"917aec09-7b04-4dd4-dfdb-9ddf5f5e0ced"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.7.13\n"]}]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Keras_w_SpellChecker.ipynb","provenance":[{"file_id":"1YFHo1VE__-xOpNy3JoRRRm-ijwkOT7bc","timestamp":1654857932748}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}